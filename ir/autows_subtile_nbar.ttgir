#blocked = #ttg.blocked<{sizePerThread = [1, 64], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 32], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 128], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 2, 64], threadsPerWarp = [32, 1, 1], warpsPerCTA = [4, 1, 1], order = [0, 2, 1]}>
#blocked6 = #ttg.blocked<{sizePerThread = [1, 64, 2], threadsPerWarp = [32, 1, 1], warpsPerCTA = [4, 1, 1], order = [0, 1, 2]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 2, 32], threadsPerWarp = [32, 1, 1], warpsPerCTA = [4, 1, 1], order = [0, 2, 1]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 32, 2], threadsPerWarp = [32, 1, 1], warpsPerCTA = [4, 1, 1], order = [0, 1, 2]}>
#blocked9 = #ttg.blocked<{sizePerThread = [1, 32, 2], threadsPerWarp = [32, 1, 1], warpsPerCTA = [4, 1, 1], order = [2, 0, 1]}>
#blocked10 = #ttg.blocked<{sizePerThread = [1, 2, 32], threadsPerWarp = [32, 1, 1], warpsPerCTA = [4, 1, 1], order = [1, 0, 2]}>
#linear = #ttg.linear<{register = [], lane = [[1], [2], [4], [8], [16]], warp = [[32], [64]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 64], [0, 32], [0, 1], [0, 2], [0, 4], [0, 8], [0, 16]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0]], warp = [[32, 0], [64, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 32], [0, 1], [0, 2], [0, 4], [0, 8], [0, 16]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0]], warp = [[32, 0], [64, 0]], block = []}>
#linear3 = #ttg.linear<{register = [[0, 0, 1], [0, 32, 0], [0, 1, 0], [0, 2, 0], [0, 4, 0], [0, 8, 0], [0, 16, 0]], lane = [[1, 0, 0], [2, 0, 0], [4, 0, 0], [8, 0, 0], [16, 0, 0]], warp = [[32, 0, 0], [64, 0, 0]], block = []}>
#linear4 = #ttg.linear<{register = [[0, 1, 0], [0, 0, 32], [0, 0, 1], [0, 0, 2], [0, 0, 4], [0, 0, 8], [0, 0, 16]], lane = [[1, 0, 0], [2, 0, 0], [4, 0, 0], [8, 0, 0], [16, 0, 0]], warp = [[32, 0, 0], [64, 0, 0]], block = []}>
#loc = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0)
#loc1 = loc(unknown)
#loc2 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":565:58)
#loc3 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":711:12)
#loc4 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":559:43)
#loc5 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":202:24)
#loc6 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":514:12)
#loc7 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":476:21)
#loc8 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":475:21)
#loc9 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":71:19)
#loc10 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":238:12)
#loc11 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":220:12)
#loc12 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":145:28)
#loc72 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":78:42)
#loc105 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":113:25)
#shared = #ttg.nvmma_shared<{swizzlingByteWidth = 128, transposed = false, elementBitWidth = 16}>
#shared1 = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0]}>
#shared2 = #ttg.nvmma_shared<{swizzlingByteWidth = 128, transposed = true, elementBitWidth = 16}>
#smem = #ttg.shared_memory
#tmem = #ttng.tensor_memory_encoding<blockM = 128, blockN = 128, unpacked = true>
#tmem1 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 64, unpacked = true>
#tmem2 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 32, unpacked = true>
#tmem3 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 1, unpacked = true>
#tmem4 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 128, unpacked = false>
#tmem5 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 64, unpacked = false>
#tmem6 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 32, unpacked = false>
#loc110 = loc("sm_scale"(#loc))
#loc111 = loc("M"(#loc))
#loc112 = loc("Z"(#loc))
#loc113 = loc("H"(#loc))
#loc114 = loc("desc_q"(#loc))
#loc115 = loc("desc_k"(#loc))
#loc116 = loc("desc_v"(#loc))
#loc117 = loc("desc_o"(#loc))
#loc118 = loc(callsite(#loc2 at #loc3))
#loc119 = loc(callsite(#loc4 at #loc3))
#loc120 = loc("v"(#loc5))
#loc121 = loc(callsite(#loc6 at #loc3))
#loc122 = loc("q1"(#loc7))
#loc123 = loc("q0"(#loc8))
#loc124 = loc("qk"(#loc9))
#loc125 = loc("acc"(#loc12))
#loc177 = loc("m_ij"(#loc72))
#loc199 = loc("l_ij"(#loc105))
#loc202 = loc(callsite(#loc120 at #loc121))
#loc203 = loc(callsite(#loc122 at #loc3))
#loc204 = loc(callsite(#loc123 at #loc3))
#loc205 = loc(callsite(#loc10 at #loc121))
#loc206 = loc(callsite(#loc11 at #loc121))
#loc241 = loc(callsite(#loc124 at #loc205))
#loc242 = loc(callsite(#loc124 at #loc206))
#loc243 = loc(callsite(#loc125 at #loc206))
#loc244 = loc(callsite(#loc125 at #loc205))
#loc256 = loc(callsite(#loc177 at #loc205))
#loc289 = loc(callsite(#loc199 at #loc205))
#loc292 = loc(callsite(#loc177 at #loc206))
#loc323 = loc(callsite(#loc199 at #loc206))
#loc332 = loc(callsite(#loc1 at #loc256))
#loc334 = loc(callsite(#loc1 at #loc289))
#loc336 = loc(callsite(#loc1 at #loc292))
#loc338 = loc(callsite(#loc1 at #loc323))
module attributes {ttg.max_reg_auto_ws = 192 : i32, ttg.min_reg_auto_ws = 24 : i32, "ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "cuda:100", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_v3_vec_exp_attn_fwd_persist(%sm_scale: f32 loc("sm_scale"(#loc)), %M: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("M"(#loc)), %Z: i32 loc("Z"(#loc)), %H: i32 {tt.divisibility = 16 : i32} loc("H"(#loc)), %desc_q: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_q"(#loc)), %desc_q_0: i32 loc("desc_q"(#loc)), %desc_q_1: i32 loc("desc_q"(#loc)), %desc_q_2: i64 loc("desc_q"(#loc)), %desc_q_3: i64 loc("desc_q"(#loc)), %desc_k: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_k"(#loc)), %desc_k_4: i32 loc("desc_k"(#loc)), %desc_k_5: i32 loc("desc_k"(#loc)), %desc_k_6: i64 loc("desc_k"(#loc)), %desc_k_7: i64 loc("desc_k"(#loc)), %desc_v: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_v"(#loc)), %desc_v_8: i32 loc("desc_v"(#loc)), %desc_v_9: i32 loc("desc_v"(#loc)), %desc_v_10: i64 loc("desc_v"(#loc)), %desc_v_11: i64 loc("desc_v"(#loc)), %desc_o: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_o"(#loc)), %desc_o_12: i32 loc("desc_o"(#loc)), %desc_o_13: i32 loc("desc_o"(#loc)), %desc_o_14: i64 loc("desc_o"(#loc)), %desc_o_15: i64 loc("desc_o"(#loc))) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc1)
    %c1_i32 = arith.constant 1 : i32 loc(#loc1)
    %c2_i32 = arith.constant 2 : i32 loc(#loc1)
    %true = arith.constant true loc(#loc1)
    %c32_i32 = arith.constant 32 : i32 loc(#loc1)
    %c8192_i32 = arith.constant 8192 : i32 loc(#loc1)
    %c256_i32 = arith.constant 256 : i32 loc(#loc1)
    %c128_i32 = arith.constant 128 : i32 loc(#loc1)
    %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #blocked> loc(#loc1)
    %c0_i64 = arith.constant 0 : i64 loc(#loc1)
    %c1_i64 = arith.constant 1 : i64 loc(#loc1)
    %c8064_i32 = arith.constant 8064 : i32 loc(#loc1)
    %0 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %1 = ttg.memdesc_index %0[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %1, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %2 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %3 = ttg.memdesc_index %2[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %3, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %4 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %5 = ttg.memdesc_index %4[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %5, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %6 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %7 = ttg.memdesc_index %6[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %7, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %8 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %9 = ttg.memdesc_index %8[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %9, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %10 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %11 = ttg.memdesc_index %10[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %11, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %12 = ttg.local_alloc : () -> !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc(#loc)
    %13 = ttg.memdesc_index %12[%c0_i32] : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %13, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %14 = ttg.memdesc_index %12[%c1_i32] : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %14, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %15 = ttg.memdesc_index %12[%c2_i32] : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %15, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %16 = ttg.local_alloc : () -> !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc(#loc)
    %17 = ttg.memdesc_index %16[%c0_i32] : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %17, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %18 = ttg.memdesc_index %16[%c1_i32] : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %18, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %19 = ttg.memdesc_index %16[%c2_i32] : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %19, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %20 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %21 = ttg.memdesc_index %20[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %21, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %22 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %23 = ttg.memdesc_index %22[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %23, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %24 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %25 = ttg.memdesc_index %24[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %25, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %26 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %27 = ttg.memdesc_index %26[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %27, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %28 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %29 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %30 = ttg.memdesc_index %28[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %30, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %31 = ttg.memdesc_index %29[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %31, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %32 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %33 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %34 = ttg.memdesc_index %32[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %34, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %35 = ttg.memdesc_index %33[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %35, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %36 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %37 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %38 = ttg.memdesc_index %36[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %38, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %39 = ttg.memdesc_index %37[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %39, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %40 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %41 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %42 = ttg.memdesc_index %40[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %42, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %43 = ttg.memdesc_index %41[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %43, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %44 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %45 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %46 = ttg.memdesc_index %44[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %46, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %47 = ttg.memdesc_index %45[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %47, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %48 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %49 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %50 = ttg.memdesc_index %48[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %50, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %51 = ttg.memdesc_index %49[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %51, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %52 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %53 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %54 = ttg.memdesc_index %52[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %54, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %55 = ttg.memdesc_index %53[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %55, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %56 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %57 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %58 = ttg.memdesc_index %56[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %58, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %59 = ttg.memdesc_index %57[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %59, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %60 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %61 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %62 = ttg.memdesc_index %60[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %62, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %63 = ttg.memdesc_index %61[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %63, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %64 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %65 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %66 = ttg.memdesc_index %64[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %66, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %67 = ttg.memdesc_index %65[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %67, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %68 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %69 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %70 = ttg.memdesc_index %68[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %70, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %71 = ttg.memdesc_index %69[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %71, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %72 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %73 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %74 = ttg.memdesc_index %72[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %74, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %75 = ttg.memdesc_index %73[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %75, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %76 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %77 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %78 = ttg.memdesc_index %76[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %78, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %79 = ttg.memdesc_index %77[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %79, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %80 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %81 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %82 = ttg.memdesc_index %80[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %82, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %83 = ttg.memdesc_index %81[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %83, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %84 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %85 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %86 = ttg.memdesc_index %84[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %86, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %87 = ttg.memdesc_index %85[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %87, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %88 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %89 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %90 = ttg.memdesc_index %88[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %90, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %91 = ttg.memdesc_index %89[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %91, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %92 = ttg.local_alloc {buffer.copy = 1 : i32, buffer.id = 0 : i32} : () -> !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(#loc118)
    %93 = ttg.local_alloc {buffer.copy = 1 : i32, buffer.id = 1 : i32} : () -> !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(#loc119)
    %v = ttg.local_alloc {allocation.shareGroup = 1 : i32, buffer.copy = 3 : i32, buffer.id = 2 : i32} : () -> !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> loc(#loc202)
    %q1 = ttg.local_alloc {buffer.copy = 1 : i32, buffer.id = 3 : i32} : () -> !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(#loc203)
    %q0 = ttg.local_alloc {buffer.copy = 1 : i32, buffer.id = 4 : i32} : () -> !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(#loc204)
    %qk = ttng.tmem_alloc {allocation.shareGroup = 4 : i32, buffer.copy = 1 : i32, buffer.id = 5 : i32} : () -> !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc241)
    %qk_16 = ttng.tmem_alloc {allocation.shareGroup = 2 : i32, buffer.copy = 1 : i32, buffer.id = 6 : i32} : () -> !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc242)
    %acc = ttng.tmem_alloc {allocation.shareGroup = 0 : i32, buffer.copy = 1 : i32, buffer.id = 7 : i32} : () -> !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc243)
    %acc_17 = ttng.tmem_alloc {allocation.shareGroup = 3 : i32, buffer.copy = 1 : i32, buffer.id = 8 : i32} : () -> !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc244)
    ttg.warp_specialize(%Z, %H, %4, %2, %v, %16, %qk_16, %q0, %20, %acc, %22, %10, %qk, %q1, %12, %24, %acc_17, %26, %8, %6, %0, %desc_q, %desc_k, %desc_v, %93, %desc_o, %92, %sm_scale, %28, %29, %32, %33, %36, %37, %40, %45, %48, %49, %52, %57, %60, %61, %64, %65, %68, %69, %72, %73, %76, %81, %84, %89) attributes {requestedRegisters = array<i32: 24, 24, 24, 192, 192>}
    default {
      %prog_id = tt.get_program_id x {async_task_id = array<i32: 0>} : i32 loc(#loc126)
      %num_progs = tt.get_num_programs x {async_task_id = array<i32: 0>} : i32 loc(#loc127)
      %total_tiles = arith.muli %Z, %c32_i32 {async_task_id = array<i32: 0>} : i32 loc(#loc128)
      %total_tiles_18 = arith.muli %total_tiles, %H {async_task_id = array<i32: 0>} : i32 loc(#loc129)
      %tiles_per_sm = arith.divsi %total_tiles_18, %num_progs {async_task_id = array<i32: 0>} : i32 loc(#loc207)
      %94 = arith.remsi %total_tiles_18, %num_progs {async_task_id = array<i32: 0>} : i32 loc(#loc18)
      %95 = arith.cmpi slt, %prog_id, %94 {async_task_id = array<i32: 0>} : i32 loc(#loc19)
      %96 = scf.if %95 -> (i32) {
        %tiles_per_sm_19 = arith.addi %tiles_per_sm, %c1_i32 {async_task_id = array<i32: 0>} : i32 loc(#loc208)
        scf.yield {async_task_id = array<i32: 0>} %tiles_per_sm_19 : i32 loc(#loc208)
      } else {
        scf.yield {async_task_id = array<i32: 0>} %tiles_per_sm : i32 loc(#loc1)
      } {async_task_id = array<i32: 0>} loc(#loc20)
      %offs_m0 = tt.make_range {async_task_id = array<i32: 0>, end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked1> loc(#loc209)
      %offs_m1 = tt.make_range {async_task_id = array<i32: 0>, end = 256 : i32, start = 128 : i32} : tensor<128xi32, #blocked1> loc(#loc210)
      %tile_idx:3 = scf.for %tile_idx_19 = %c0_i32 to %96 step %c1_i32 iter_args(%prog_id_20 = %prog_id, %arg26 = %c0_i64, %arg27 = %c0_i64) -> (i32, i64, i64)  : i32 {
        %pid = arith.remsi %prog_id_20, %c32_i32 {async_task_id = array<i32: 0>} : i32 loc(#loc135)
        %off_hz = arith.divsi %prog_id_20, %c32_i32 {async_task_id = array<i32: 0>} : i32 loc(#loc136)
        %qo_offset_y = arith.muli %pid, %c256_i32 {async_task_id = array<i32: 0>} : i32 loc(#loc211)
        %offs_m0_21 = tt.splat %qo_offset_y {async_task_id = array<i32: 0>} : i32 -> tensor<128xi32, #blocked1> loc(#loc212)
        %offs_m0_22 = arith.addi %offs_m0_21, %offs_m0 {async_task_id = array<i32: 0>} : tensor<128xi32, #blocked1> loc(#loc212)
        %offs_m1_23 = arith.addi %offs_m0_21, %offs_m1 {async_task_id = array<i32: 0>} : tensor<128xi32, #blocked1> loc(#loc213)
        %acc_24 = ttg.memdesc_index %acc_17[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc244)
        ttng.tmem_store %cst, %acc_24, %true {async_task_id = array<i32: 0>} : tensor<128x64xf32, #blocked> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc244)
        %acc_25 = ttg.memdesc_index %acc[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc243)
        ttng.tmem_store %cst, %acc_25, %true {async_task_id = array<i32: 0>} : tensor<128x64xf32, #blocked> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc243)
        %offsetkv_y = arith.andi %arg26, %c1_i64 {async_task_id = array<i32: 0>} : i64 loc(#loc348)
        %offsetkv_y_26 = arith.trunci %offsetkv_y {async_task_id = array<i32: 0>} : i64 to i1 loc(#loc348)
        %alpha = arith.andi %arg27, %c1_i64 {async_task_id = array<i32: 0>} : i64 loc(#loc246)
        %alpha_27 = arith.trunci %alpha {async_task_id = array<i32: 0>} : i64 to i1 loc(#loc246)
        %97 = ttg.memdesc_index %10[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_28 = arith.xori %alpha_27, %true {async_task_id = array<i32: 0>} : i1 loc(#loc243)
        %acc_29 = arith.extsi %acc_28 {async_task_id = array<i32: 0>} : i1 to i32 loc(#loc243)
        ttng.wait_barrier %97, %acc_29, %true {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc243)
        %acc_30 = ttng.tmem_subslice %acc_25 {N = 0 : i32} : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc243)
        %acc_31 = ttng.tmem_subslice %acc_25 {N = 32 : i32} : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc243)
        %qk_32 = ttng.tmem_subslice %qk_16 {N = 64 : i32, async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc242)
        %qk_33 = ttg.memdesc_reinterpret %qk_32 {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc242)
        %alpha_34 = ttg.memdesc_index %qk_33[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc246)
        %98 = ttg.memdesc_index %48[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc0 = arith.extui %alpha_27 : i1 to i32 loc(#loc247)
        ttng.wait_barrier %98, %acc0, %true {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc247)
        %99 = ttg.memdesc_index %49[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_35 = ttng.tmem_load %acc_30 : !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> -> tensor<128x32xf32, #blocked2> loc(#loc243)
        %acc_36 = ttng.tmem_load %acc_31 : !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> -> tensor<128x32xf32, #blocked2> loc(#loc243)
        %acc0_37 = ttng.tmem_load %alpha_34 {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc247)
        ttng.arrive_barrier %99, 1, %true {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc247)
        %acc0_38 = tt.reshape %acc0_37 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc247)
        %acc0_39 = ttg.convert_layout %acc0_38 : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc327)
        %acc0_40 = tt.expand_dims %acc0_39 {async_task_id = array<i32: 0>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<128x1xf32, #blocked2> loc(#loc247)
        %acc0_41 = tt.broadcast %acc0_40 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc327)
        %acc0_42 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {async_task_id = array<i32: 0>, constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %acc_35, %acc0_41 : tensor<128x32xf32, #blocked2>, tensor<128x32xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc327)
        %acc1 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {async_task_id = array<i32: 0>, constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %acc_36, %acc0_41 : tensor<128x32xf32, #blocked2>, tensor<128x32xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc328)
        ttng.tmem_store %acc0_42, %acc_30, %true : tensor<128x32xf32, #blocked2> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc243)
        ttng.tmem_store %acc1, %acc_31, %true : tensor<128x32xf32, #blocked2> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc243)
        %100 = ttg.memdesc_index %76[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %100, 1, %true {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc243)
        %offsetkv_y_43 = scf.for %offsetkv_y_103 = %c0_i32 to %c8064_i32 step %c128_i32 iter_args(%arg29 = %arg27) -> (i64)  : i32 {
          %alpha_104 = arith.andi %arg29, %c1_i64 {async_task_id = array<i32: 0>} : i64 loc(#loc250)
          %alpha_105 = arith.trunci %alpha_104 {async_task_id = array<i32: 0>} : i64 to i1 loc(#loc250)
          %128 = ttg.memdesc_index %8[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %acc_106 = arith.xori %alpha_105, %true {async_task_id = array<i32: 0>} : i1 loc(#loc244)
          %acc_107 = arith.extsi %acc_106 {async_task_id = array<i32: 0>} : i1 to i32 loc(#loc244)
          ttng.wait_barrier %128, %acc_107 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc244)
          %acc_108 = ttng.tmem_subslice %acc_24 {N = 0 : i32} : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc244)
          %acc_109 = ttng.tmem_subslice %acc_24 {N = 32 : i32} : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc244)
          %qk_110 = ttng.tmem_subslice %qk {N = 64 : i32, async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc241)
          %qk_111 = ttg.memdesc_reinterpret %qk_110 {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc241)
          %alpha_112 = ttg.memdesc_index %qk_111[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc250)
          %129 = ttg.memdesc_index %36[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %acc0_113 = arith.extui %alpha_105 : i1 to i32 loc(#loc251)
          ttng.wait_barrier %129, %acc0_113 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc251)
          %130 = ttg.memdesc_index %37[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %acc_114 = ttng.tmem_load %acc_108 : !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> -> tensor<128x32xf32, #blocked2> loc(#loc244)
          %acc_115 = ttng.tmem_load %acc_109 : !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> -> tensor<128x32xf32, #blocked2> loc(#loc244)
          %acc0_116 = ttng.tmem_load %alpha_112 {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc251)
          ttng.arrive_barrier %130, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc251)
          %acc0_117 = tt.reshape %acc0_116 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc251)
          %acc0_118 = ttg.convert_layout %acc0_117 : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc329)
          %acc0_119 = tt.expand_dims %acc0_118 {async_task_id = array<i32: 0>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<128x1xf32, #blocked2> loc(#loc251)
          %acc0_120 = tt.broadcast %acc0_119 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc329)
          %acc0_121 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {async_task_id = array<i32: 0>, constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %acc_114, %acc0_120 : tensor<128x32xf32, #blocked2>, tensor<128x32xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc329)
          %acc1_122 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {async_task_id = array<i32: 0>, constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %acc_115, %acc0_120 : tensor<128x32xf32, #blocked2>, tensor<128x32xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc330)
          ttng.tmem_store %acc0_121, %acc_108, %true : tensor<128x32xf32, #blocked2> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc244)
          ttng.tmem_store %acc1_122, %acc_109, %true : tensor<128x32xf32, #blocked2> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc244)
          %131 = ttg.memdesc_index %84[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.arrive_barrier %131, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc244)
          %offsetkv_y_123 = arith.addi %arg29, %c1_i64 {async_task_id = array<i32: 0>} : i64 loc(#loc348)
          %alpha_124 = arith.andi %offsetkv_y_123, %c1_i64 {async_task_id = array<i32: 0>} : i64 loc(#loc246)
          %alpha_125 = arith.trunci %alpha_124 {async_task_id = array<i32: 0>} : i64 to i1 loc(#loc246)
          %acc_126 = arith.xori %alpha_125, %true {async_task_id = array<i32: 0>} : i1 loc(#loc243)
          %acc_127 = arith.extsi %acc_126 {async_task_id = array<i32: 0>} : i1 to i32 loc(#loc243)
          ttng.wait_barrier %97, %acc_127, %true {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc243)
          %acc0_128 = arith.extui %alpha_125 : i1 to i32 loc(#loc247)
          ttng.wait_barrier %98, %acc0_128, %true {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc247)
          %acc_129 = ttng.tmem_load %acc_30 : !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> -> tensor<128x32xf32, #blocked2> loc(#loc243)
          %acc_130 = ttng.tmem_load %acc_31 : !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> -> tensor<128x32xf32, #blocked2> loc(#loc243)
          %acc0_131 = ttng.tmem_load %alpha_34 {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc247)
          ttng.arrive_barrier %99, 1, %true {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc247)
          %acc0_132 = tt.reshape %acc0_131 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc247)
          %acc0_133 = ttg.convert_layout %acc0_132 : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc327)
          %acc0_134 = tt.expand_dims %acc0_133 {async_task_id = array<i32: 0>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<128x1xf32, #blocked2> loc(#loc247)
          %acc0_135 = tt.broadcast %acc0_134 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc327)
          %acc0_136 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {async_task_id = array<i32: 0>, constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %acc_129, %acc0_135 : tensor<128x32xf32, #blocked2>, tensor<128x32xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc327)
          %acc1_137 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {async_task_id = array<i32: 0>, constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %acc_130, %acc0_135 : tensor<128x32xf32, #blocked2>, tensor<128x32xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc328)
          ttng.tmem_store %acc0_136, %acc_30, %true : tensor<128x32xf32, #blocked2> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc243)
          ttng.tmem_store %acc1_137, %acc_31, %true : tensor<128x32xf32, #blocked2> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc243)
          ttng.arrive_barrier %100, 1, %true {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc243)
          scf.yield %offsetkv_y_123 : i64 loc(#loc348)
        } {async_task_id = array<i32: 0>, tt.disallow_acc_multi_buffer, tt.warp_specialize} loc(#loc348)
        %alpha_44 = arith.andi %offsetkv_y_43, %c1_i64 {async_task_id = array<i32: 0>} : i64 loc(#loc250)
        %alpha_45 = arith.trunci %alpha_44 {async_task_id = array<i32: 0>} : i64 to i1 loc(#loc250)
        %101 = ttg.memdesc_index %8[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_46 = arith.xori %alpha_45, %true {async_task_id = array<i32: 0>} : i1 loc(#loc244)
        %acc_47 = arith.extsi %acc_46 {async_task_id = array<i32: 0>} : i1 to i32 loc(#loc244)
        ttng.wait_barrier %101, %acc_47 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc244)
        %acc_48 = ttng.tmem_subslice %acc_24 {N = 0 : i32} : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc244)
        %acc_49 = ttng.tmem_subslice %acc_24 {N = 32 : i32} : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc244)
        %qk_50 = ttng.tmem_subslice %qk {N = 64 : i32, async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc241)
        %qk_51 = ttg.memdesc_reinterpret %qk_50 {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc241)
        %alpha_52 = ttg.memdesc_index %qk_51[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc250)
        %102 = ttg.memdesc_index %36[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc0_53 = arith.extui %alpha_45 : i1 to i32 loc(#loc251)
        ttng.wait_barrier %102, %acc0_53 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc251)
        %103 = ttg.memdesc_index %37[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_54 = ttng.tmem_load %acc_48 : !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> -> tensor<128x32xf32, #blocked2> loc(#loc244)
        %acc_55 = ttng.tmem_load %acc_49 : !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> -> tensor<128x32xf32, #blocked2> loc(#loc244)
        %acc0_56 = ttng.tmem_load %alpha_52 {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc251)
        ttng.arrive_barrier %103, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc251)
        %acc0_57 = tt.reshape %acc0_56 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc251)
        %acc0_58 = ttg.convert_layout %acc0_57 : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc329)
        %acc0_59 = tt.expand_dims %acc0_58 {async_task_id = array<i32: 0>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<128x1xf32, #blocked2> loc(#loc251)
        %acc0_60 = tt.broadcast %acc0_59 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc329)
        %acc0_61 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {async_task_id = array<i32: 0>, constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %acc_54, %acc0_60 : tensor<128x32xf32, #blocked2>, tensor<128x32xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc329)
        %acc1_62 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {async_task_id = array<i32: 0>, constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %acc_55, %acc0_60 : tensor<128x32xf32, #blocked2>, tensor<128x32xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc330)
        ttng.tmem_store %acc0_61, %acc_48, %true : tensor<128x32xf32, #blocked2> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc244)
        ttng.tmem_store %acc1_62, %acc_49, %true : tensor<128x32xf32, #blocked2> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc244)
        %104 = ttg.memdesc_index %84[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %104, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc244)
        %offsetkv_y_63 = arith.addi %offsetkv_y_43, %c1_i64 {async_task_id = array<i32: 0>} : i64 loc(#loc348)
        %qk_64 = ttng.tmem_subslice %qk_16 {N = 66 : i32, async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc242)
        %qk_65 = ttg.memdesc_reinterpret %qk_64 {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc242)
        %offsetkv_y_66 = ttg.memdesc_index %qk_65[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc348)
        %105 = ttg.memdesc_index %72[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %m_i0 = arith.extui %offsetkv_y_26 : i1 to i32 loc(#loc215)
        ttng.wait_barrier %105, %m_i0 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc215)
        %106 = ttg.memdesc_index %73[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %m_i0_67 = ttng.tmem_load %offsetkv_y_66 {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc215)
        ttng.arrive_barrier %106, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc215)
        %m_i0_68 = tt.reshape %m_i0_67 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc215)
        %m_i0_69 = math.log2 %m_i0_68 {async_task_id = array<i32: 0>} : tensor<128xf32, #linear> loc(#loc215)
        %qk_70 = ttng.tmem_subslice %qk_16 {N = 65 : i32, async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc242)
        %qk_71 = ttg.memdesc_reinterpret %qk_70 {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc242)
        %offsetkv_y_72 = ttg.memdesc_index %qk_71[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc348)
        %107 = ttg.memdesc_index %64[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %107, %m_i0 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc216)
        %108 = ttg.memdesc_index %65[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %m_i0_73 = ttng.tmem_load %offsetkv_y_72 {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc216)
        ttng.arrive_barrier %108, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc216)
        %m_i0_74 = tt.reshape %m_i0_73 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc216)
        %m_i0_75 = arith.addf %m_i0_74, %m_i0_69 {async_task_id = array<i32: 0>} : tensor<128xf32, #linear> loc(#loc216)
        %109 = ttg.convert_layout %m_i0_75 : tensor<128xf32, #linear> -> tensor<128xf32, #blocked1> loc(#loc147)
        %m_ptrs0 = arith.muli %off_hz, %c8192_i32 {async_task_id = array<i32: 0>} : i32 loc(#loc217)
        %m_ptrs0_76 = tt.addptr %M, %m_ptrs0 {async_task_id = array<i32: 0>} : !tt.ptr<f32>, i32 loc(#loc218)
        %m_ptrs0_77 = tt.splat %m_ptrs0_76 {async_task_id = array<i32: 0>} : !tt.ptr<f32> -> tensor<128x!tt.ptr<f32>, #blocked1> loc(#loc219)
        %m_ptrs0_78 = tt.addptr %m_ptrs0_77, %offs_m0_22 {async_task_id = array<i32: 0>} : tensor<128x!tt.ptr<f32>, #blocked1>, tensor<128xi32, #blocked1> loc(#loc219)
        tt.store %m_ptrs0_78, %109 {async_task_id = array<i32: 0>} : tensor<128x!tt.ptr<f32>, #blocked1> loc(#loc147)
        %acc0_79 = ttg.convert_layout %m_i0_68 : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc220)
        %acc0_80 = tt.expand_dims %acc0_79 {async_task_id = array<i32: 0>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc221)
        %acc0_81 = tt.broadcast %acc0_80 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked> -> tensor<128x64xf32, #blocked> loc(#loc220)
        %110 = ttg.memdesc_index %6[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_82 = arith.extsi %offsetkv_y_26 {async_task_id = array<i32: 0>} : i1 to i32 loc(#loc243)
        ttng.wait_barrier %110, %acc_82 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc243)
        %111 = ttg.memdesc_index %81[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_83 = ttng.tmem_load %acc_25 {async_task_id = array<i32: 0>, tmem.end = array<i32: 16, 16>} : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> tensor<128x64xf32, #blocked> loc(#loc243)
        ttng.arrive_barrier %111, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc243)
        %acc0_84 = arith.divf %acc_83, %acc0_81 {async_task_id = array<i32: 0>} : tensor<128x64xf32, #blocked> loc(#loc220)
        %112 = arith.truncf %acc0_84 {async_task_id = array<i32: 0>} : tensor<128x64xf32, #blocked> to tensor<128x64xbf16, #blocked> loc(#loc119)
        %113 = ttg.memdesc_index %93[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc119)
        %114 = ttg.memdesc_index %33[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %115 = arith.xori %offsetkv_y_26, %true : i1 loc(#loc119)
        %116 = arith.extui %115 : i1 to i32 loc(#loc119)
        ttng.wait_barrier %114, %116 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc119)
        ttg.local_store %112, %113 {async_task_id = array<i32: 0>} : tensor<128x64xbf16, #blocked> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc119)
        %117 = ttg.memdesc_index %32[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %117, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc119)
        %qk_85 = ttng.tmem_subslice %qk {N = 66 : i32, async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc241)
        %qk_86 = ttg.memdesc_reinterpret %qk_85 {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc241)
        %offsetkv_y_87 = ttg.memdesc_index %qk_86[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc348)
        %118 = ttg.memdesc_index %68[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %118, %m_i0 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc222)
        %119 = ttg.memdesc_index %69[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %m_i1 = ttng.tmem_load %offsetkv_y_87 {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc222)
        ttng.arrive_barrier %119, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc222)
        %m_i1_88 = tt.reshape %m_i1 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc222)
        %m_i1_89 = math.log2 %m_i1_88 {async_task_id = array<i32: 0>} : tensor<128xf32, #linear> loc(#loc222)
        %qk_90 = ttng.tmem_subslice %qk {N = 65 : i32, async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc241)
        %qk_91 = ttg.memdesc_reinterpret %qk_90 {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc241)
        %offsetkv_y_92 = ttg.memdesc_index %qk_91[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc348)
        %120 = ttg.memdesc_index %60[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %120, %m_i0 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc223)
        %121 = ttg.memdesc_index %61[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %m_i1_93 = ttng.tmem_load %offsetkv_y_92 {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc223)
        ttng.arrive_barrier %121, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc223)
        %m_i1_94 = tt.reshape %m_i1_93 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc223)
        %m_i1_95 = arith.addf %m_i1_94, %m_i1_89 {async_task_id = array<i32: 0>} : tensor<128xf32, #linear> loc(#loc223)
        %122 = ttg.convert_layout %m_i1_95 : tensor<128xf32, #linear> -> tensor<128xf32, #blocked1> loc(#loc155)
        %m_ptrs1 = tt.addptr %m_ptrs0_77, %offs_m1_23 {async_task_id = array<i32: 0>} : tensor<128x!tt.ptr<f32>, #blocked1>, tensor<128xi32, #blocked1> loc(#loc224)
        tt.store %m_ptrs1, %122 {async_task_id = array<i32: 0>} : tensor<128x!tt.ptr<f32>, #blocked1> loc(#loc155)
        %acc1_96 = ttg.convert_layout %m_i1_88 : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc225)
        %acc1_97 = tt.expand_dims %acc1_96 {async_task_id = array<i32: 0>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc226)
        %acc1_98 = tt.broadcast %acc1_97 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked> -> tensor<128x64xf32, #blocked> loc(#loc225)
        %123 = ttg.memdesc_index %89[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_99 = ttng.tmem_load %acc_24 {async_task_id = array<i32: 0>, tmem.end = array<i32: 19, 19>} : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> tensor<128x64xf32, #blocked> loc(#loc244)
        ttng.arrive_barrier %123, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc244)
        %acc1_100 = arith.divf %acc_99, %acc1_98 {async_task_id = array<i32: 0>} : tensor<128x64xf32, #blocked> loc(#loc225)
        %124 = arith.truncf %acc1_100 {async_task_id = array<i32: 0>} : tensor<128x64xf32, #blocked> to tensor<128x64xbf16, #blocked> loc(#loc118)
        %125 = ttg.memdesc_index %92[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc118)
        %126 = ttg.memdesc_index %29[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %126, %116 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc118)
        ttg.local_store %124, %125 {async_task_id = array<i32: 0>} : tensor<128x64xbf16, #blocked> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc118)
        %127 = ttg.memdesc_index %28[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %127, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc118)
        %tile_idx_101 = arith.addi %prog_id_20, %num_progs {async_task_id = array<i32: 0>} : i32 loc(#loc159)
        %tile_idx_102 = arith.addi %arg26, %c1_i64 {async_task_id = array<i32: 0>} : i64 loc(#loc134)
        scf.yield %tile_idx_101, %tile_idx_102, %offsetkv_y_63 : i32, i64, i64 loc(#loc51)
      } {async_task_id = array<i32: 0>, tt.warp_specialize, ttg.partition.stages = [0 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], ttg.warp_specialize.tag = 0 : i32} loc(#loc134)
      ttg.warp_yield {async_task_id = array<i32: 0>} loc(#loc)
    }
    partition0(%Z_18: i32 loc("Z"(#loc)), %H_19: i32 loc("H"(#loc)), %arg26: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg27: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %v_20: !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc120 at #loc121)), %arg29: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %qk_21: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc124 at #loc206)), %q0_22: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc123 at #loc3)), %arg32: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %acc_23: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc125 at #loc206)), %arg34: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg35: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %qk_24: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc124 at #loc205)), %q1_25: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc122 at #loc3)), %arg38: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg39: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %acc_26: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc125 at #loc205)), %arg41: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg42: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg43: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg44: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %desc_q_27: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_q"(#loc)), %desc_k_28: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_k"(#loc)), %desc_v_29: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_v"(#loc)), %arg48: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc4 at #loc3)), %desc_o_30: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_o"(#loc)), %arg50: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc2 at #loc3)), %sm_scale_31: f32 loc("sm_scale"(#loc)), %arg52: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg53: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg54: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg55: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg56: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg57: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg58: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg59: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg60: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg61: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg62: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg63: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg64: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg65: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg66: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg67: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg68: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg69: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg70: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg71: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg72: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg73: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg74: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg75: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0)) num_warps(1) {
      %c8064_i32_32 = arith.constant 8064 : i32 loc(#loc1)
      %c3_i64 = arith.constant 3 : i64 loc(#loc1)
      %c2_i64 = arith.constant {async_task_id = array<i32: 1>} 2 : i64 loc(#loc1)
      %c1_i64_33 = arith.constant {async_task_id = array<i32: 1>} 1 : i64 loc(#loc1)
      %c0_i64_34 = arith.constant {async_task_id = array<i32: 1>} 0 : i64 loc(#loc1)
      %false = arith.constant {async_task_id = array<i32: 1>} false loc(#loc1)
      %true_35 = arith.constant {async_task_id = array<i32: 1>} true loc(#loc1)
      %n_tile_num = arith.constant {async_task_id = array<i32: 1>} 32 : i32 loc(#loc227)
      %c1_i32_36 = arith.constant {async_task_id = array<i32: 1>} 1 : i32 loc(#loc1)
      %c0_i32_37 = arith.constant {async_task_id = array<i32: 1>} 0 : i32 loc(#loc1)
      %c128_i32_38 = arith.constant {async_task_id = array<i32: 1>} 128 : i32 loc(#loc1)
      %prog_id = tt.get_program_id x {async_task_id = array<i32: 1>} : i32 loc(#loc126)
      %num_progs = tt.get_num_programs x {async_task_id = array<i32: 1>} : i32 loc(#loc127)
      %total_tiles = arith.muli %Z_18, %n_tile_num {async_task_id = array<i32: 1>} : i32 loc(#loc128)
      %total_tiles_39 = arith.muli %total_tiles, %H_19 {async_task_id = array<i32: 1>} : i32 loc(#loc129)
      %tiles_per_sm = arith.divsi %total_tiles_39, %num_progs {async_task_id = array<i32: 1>} : i32 loc(#loc207)
      %94 = arith.remsi %total_tiles_39, %num_progs {async_task_id = array<i32: 1>} : i32 loc(#loc18)
      %95 = arith.cmpi slt, %prog_id, %94 {async_task_id = array<i32: 1>} : i32 loc(#loc19)
      %96 = scf.if %95 -> (i32) {
        %tiles_per_sm_40 = arith.addi %tiles_per_sm, %c1_i32_36 {async_task_id = array<i32: 1>} : i32 loc(#loc208)
        scf.yield {async_task_id = array<i32: 1>} %tiles_per_sm_40 : i32 loc(#loc208)
      } else {
        scf.yield {async_task_id = array<i32: 1>} %tiles_per_sm : i32 loc(#loc1)
      } {async_task_id = array<i32: 1>} loc(#loc20)
      %tile_idx:3 = scf.for %tile_idx_40 = %c0_i32_37 to %96 step %c1_i32_36 iter_args(%tile_idx_41 = %c0_i64_34, %tile_idx_42 = %c0_i64_34, %tile_idx_43 = %c0_i64_34) -> (i64, i64, i64)  : i32 {
        %offsetkv_y = arith.andi %tile_idx_41, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc348)
        %offsetkv_y_44 = arith.trunci %offsetkv_y {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc348)
        %97 = ttg.memdesc_index %arg26[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %98 = arith.extsi %offsetkv_y_44 {async_task_id = array<i32: 1>} : i1 to i32 loc(#loc)
        ttng.wait_barrier %97, %98 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %99 = ttg.memdesc_index %arg27[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %99, %98 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %100 = ttg.memdesc_index %arg73[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_45 = arith.xori %offsetkv_y_44, %true_35 : i1 loc(#loc243)
        %acc_46 = arith.extui %acc_45 : i1 to i32 loc(#loc243)
        ttng.wait_barrier %100, %acc_46 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc243)
        %101 = ttg.memdesc_index %arg75[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %101, %acc_46 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc244)
        %k = arith.divui %tile_idx_43, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc228)
        %k_47 = arith.muli %k, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc228)
        %k_48 = arith.subi %tile_idx_43, %k_47 {async_task_id = array<i32: 1>} : i64 loc(#loc228)
        %k_49 = arith.trunci %k_48 {async_task_id = array<i32: 1>} : i64 to i32 loc(#loc228)
        %k_50 = arith.andi %k, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc228)
        %k_51 = arith.trunci %k_50 {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc228)
        %k_52 = ttg.memdesc_index %v_20[%k_49] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc228)
        %k_53 = ttg.memdesc_trans %k_52 {async_task_id = array<i32: 1>, order = array<i32: 1, 0>} : !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<64x128xbf16, #shared2, #smem, mutable> loc(#loc228)
        %102 = ttg.memdesc_index %arg29[%k_49] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %103 = arith.extsi %k_51 {async_task_id = array<i32: 1>} : i1 to i32 loc(#loc)
        ttng.wait_barrier %102, %103, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %qk_54 = ttg.memdesc_index %qk_21[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc242)
        %q0_55 = ttg.memdesc_index %q0_22[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc204)
        %qk_56 = arith.andi %tile_idx_42, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc242)
        %qk_57 = arith.trunci %qk_56 {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc242)
        %104 = ttg.memdesc_index %arg32[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %105 = ttg.memdesc_index %arg63[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %qk_58 = arith.xori %qk_57, %true_35 : i1 loc(#loc242)
        %qk_59 = arith.extui %qk_58 : i1 to i32 loc(#loc242)
        ttng.wait_barrier %105, %qk_59, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc242)
        ttng.tc_gen5_mma %q0_55, %k_53, %qk_54, %false, %true_35, %104[%true_35] {async_task_id = array<i32: 1>, is_async, tt.self_latency = 1 : i32} : !ttg.memdesc<128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<64x128xbf16, #shared2, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc242)
        %qk_60 = ttg.memdesc_index %qk_24[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc241)
        %q1_61 = ttg.memdesc_index %q1_25[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc203)
        %106 = ttg.memdesc_index %arg38[%k_49] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %107 = ttg.memdesc_index %arg39[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %108 = ttg.memdesc_index %arg59[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %108, %qk_59, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc241)
        ttng.tc_gen5_mma %q1_61, %k_53, %qk_60, %false, %true_35, %106[%true_35], %107[%true_35] {async_task_id = array<i32: 1>, is_async, tt.self_latency = 1 : i32} : !ttg.memdesc<128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<64x128xbf16, #shared2, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc241)
        %v_62 = arith.addi %tile_idx_43, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc202)
        %v_63 = arith.divui %v_62, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc202)
        %v_64 = arith.muli %v_63, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc202)
        %v_65 = arith.subi %v_62, %v_64 {async_task_id = array<i32: 1>} : i64 loc(#loc202)
        %v_66 = arith.trunci %v_65 {async_task_id = array<i32: 1>} : i64 to i32 loc(#loc202)
        %v_67 = arith.andi %v_63, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc202)
        %v_68 = arith.trunci %v_67 {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc202)
        %qk_69 = ttng.tmem_subslice %qk_21 {N = 0 : i32, async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc242)
        %qk_70 = ttg.memdesc_reinterpret %qk_69 {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc242)
        %acc_71 = ttg.memdesc_index %qk_70[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc243)
        %v_72 = ttg.memdesc_index %v_20[%v_66] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc202)
        %acc_73 = ttg.memdesc_index %acc_23[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc243)
        %109 = ttg.memdesc_index %arg29[%v_66] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %110 = arith.extsi %v_68 {async_task_id = array<i32: 1>} : i1 to i32 loc(#loc)
        ttng.wait_barrier %109, %110, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %111 = ttg.memdesc_index %arg34[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %112 = ttg.memdesc_index %arg62[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_74 = arith.extui %qk_57 : i1 to i32 loc(#loc243)
        ttng.wait_barrier %112, %acc_74, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc243)
        %113 = ttg.memdesc_index %arg35[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %114 = ttg.memdesc_index %arg72[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %114, %acc_74, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc243)
        ttng.tc_gen5_mma %acc_71, %v_72, %acc_73, %true_35, %true_35, %111[%true_35], %113[%true_35] {async_task_id = array<i32: 1>, is_async, tmem.end = array<i32: 15, 15>, tmem.start = array<i32: 14, 14, 16, 16>, tt.self_latency = 1 : i32} : !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc243)
        %offsetkv_y_75:3 = scf.for %offsetkv_y_92 = %c0_i32_37 to %c8064_i32_32 step %c128_i32_38 iter_args(%tile_idx_93 = %tile_idx_42, %tile_idx_94 = %tile_idx_43, %v_95 = %v_66) -> (i64, i64, i32)  : i32 {
          %offsetkv_y_96 = arith.addi %tile_idx_94, %c2_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc348)
          %offsetkv_y_97 = arith.addi %tile_idx_93, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc348)
          %k_98 = arith.divui %offsetkv_y_96, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc228)
          %k_99 = arith.muli %k_98, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc228)
          %k_100 = arith.subi %offsetkv_y_96, %k_99 {async_task_id = array<i32: 1>} : i64 loc(#loc228)
          %k_101 = arith.trunci %k_100 {async_task_id = array<i32: 1>} : i64 to i32 loc(#loc228)
          %k_102 = arith.andi %k_98, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc228)
          %k_103 = arith.trunci %k_102 {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc228)
          %k_104 = ttg.memdesc_index %v_20[%k_101] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc228)
          %k_105 = ttg.memdesc_trans %k_104 {async_task_id = array<i32: 1>, order = array<i32: 1, 0>} : !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<64x128xbf16, #shared2, #smem, mutable> loc(#loc228)
          %120 = ttg.memdesc_index %arg29[%k_101] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %121 = arith.extsi %k_103 {async_task_id = array<i32: 1>} : i1 to i32 loc(#loc)
          ttng.wait_barrier %120, %121, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %qk_106 = arith.andi %offsetkv_y_97, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc242)
          %qk_107 = arith.trunci %qk_106 {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc242)
          %qk_108 = arith.xori %qk_107, %true_35 : i1 loc(#loc242)
          %qk_109 = arith.extui %qk_108 : i1 to i32 loc(#loc242)
          ttng.wait_barrier %105, %qk_109, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc242)
          ttng.tc_gen5_mma %q0_55, %k_105, %qk_54, %false, %true_35, %104[%true_35] {async_task_id = array<i32: 1>, is_async, tt.self_latency = 1 : i32} : !ttg.memdesc<128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<64x128xbf16, #shared2, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc242)
          %acc_110 = arith.andi %tile_idx_93, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc244)
          %acc_111 = arith.trunci %acc_110 {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc244)
          %qk_112 = ttng.tmem_subslice %qk_24 {N = 0 : i32, async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc241)
          %qk_113 = ttg.memdesc_reinterpret %qk_112 {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc241)
          %acc_114 = ttg.memdesc_index %qk_113[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc244)
          %acc_115 = arith.addi %tile_idx_94, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc244)
          %acc_116 = arith.divui %acc_115, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc244)
          %acc_117 = arith.muli %acc_116, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc244)
          %acc_118 = arith.subi %acc_115, %acc_117 {async_task_id = array<i32: 1>} : i64 loc(#loc244)
          %acc_119 = arith.trunci %acc_118 {async_task_id = array<i32: 1>} : i64 to i32 loc(#loc244)
          %v_120 = ttg.memdesc_index %v_20[%acc_119] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc202)
          %acc_121 = ttg.memdesc_index %acc_26[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc244)
          %122 = ttg.memdesc_index %arg38[%v_95] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %123 = ttg.memdesc_index %arg41[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %124 = ttg.memdesc_index %arg58[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %acc_122 = arith.extui %acc_111 : i1 to i32 loc(#loc244)
          ttng.wait_barrier %124, %acc_122 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc244)
          %125 = ttg.memdesc_index %arg42[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %126 = ttg.memdesc_index %arg74[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.wait_barrier %126, %acc_122 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc244)
          ttng.tc_gen5_mma %acc_114, %v_120, %acc_121, %true_35, %true_35, %122[%true_35], %123[%true_35], %125[%true_35] {async_task_id = array<i32: 1>, is_async, tmem.end = array<i32: 18, 18>, tmem.start = array<i32: 17, 17, 19, 19>, tt.self_latency = 1 : i32} : !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc244)
          %127 = ttg.memdesc_index %arg38[%k_101] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.wait_barrier %108, %qk_109, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc241)
          ttng.tc_gen5_mma %q1_61, %k_105, %qk_60, %false, %true_35, %127[%true_35], %107[%true_35] {async_task_id = array<i32: 1>, is_async, tt.self_latency = 1 : i32} : !ttg.memdesc<128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<64x128xbf16, #shared2, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc241)
          %v_123 = arith.addi %tile_idx_94, %c3_i64 : i64 loc(#loc202)
          %v_124 = arith.divui %v_123, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc202)
          %v_125 = arith.muli %v_124, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc202)
          %v_126 = arith.subi %v_123, %v_125 {async_task_id = array<i32: 1>} : i64 loc(#loc202)
          %v_127 = arith.trunci %v_126 {async_task_id = array<i32: 1>} : i64 to i32 loc(#loc202)
          %v_128 = arith.andi %v_124, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc202)
          %v_129 = arith.trunci %v_128 {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc202)
          %v_130 = ttg.memdesc_index %v_20[%v_127] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc202)
          %128 = ttg.memdesc_index %arg29[%v_127] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %129 = arith.extsi %v_129 {async_task_id = array<i32: 1>} : i1 to i32 loc(#loc)
          ttng.wait_barrier %128, %129, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %acc_131 = arith.extui %qk_107 : i1 to i32 loc(#loc243)
          ttng.wait_barrier %112, %acc_131, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc243)
          ttng.wait_barrier %114, %acc_131, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc243)
          ttng.tc_gen5_mma %acc_71, %v_130, %acc_73, %true_35, %true_35, %111[%true_35], %113[%true_35] {async_task_id = array<i32: 1>, is_async, tmem.end = array<i32: 15, 15>, tmem.start = array<i32: 14, 14, 16, 16>, tt.self_latency = 1 : i32} : !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc243)
          scf.yield %offsetkv_y_97, %offsetkv_y_96, %v_127 : i64, i64, i32 loc(#loc348)
        } {async_task_id = array<i32: 1>, tt.disallow_acc_multi_buffer, tt.warp_specialize} loc(#loc348)
        %offsetkv_y_76 = arith.addi %offsetkv_y_75#1, %c2_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc348)
        %offsetkv_y_77 = arith.addi %offsetkv_y_75#0, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc348)
        %acc_78 = arith.andi %offsetkv_y_75#0, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc244)
        %acc_79 = arith.trunci %acc_78 {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc244)
        %qk_80 = ttng.tmem_subslice %qk_24 {N = 0 : i32, async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc241)
        %qk_81 = ttg.memdesc_reinterpret %qk_80 {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc241)
        %acc_82 = ttg.memdesc_index %qk_81[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc244)
        %acc_83 = arith.addi %offsetkv_y_75#1, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc244)
        %acc_84 = arith.divui %acc_83, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc244)
        %acc_85 = arith.muli %acc_84, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc244)
        %acc_86 = arith.subi %acc_83, %acc_85 {async_task_id = array<i32: 1>} : i64 loc(#loc244)
        %acc_87 = arith.trunci %acc_86 {async_task_id = array<i32: 1>} : i64 to i32 loc(#loc244)
        %v_88 = ttg.memdesc_index %v_20[%acc_87] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc202)
        %acc_89 = ttg.memdesc_index %acc_26[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc244)
        %115 = ttg.memdesc_index %arg38[%offsetkv_y_75#2] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %116 = ttg.memdesc_index %arg41[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %117 = ttg.memdesc_index %arg58[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_90 = arith.extui %acc_79 : i1 to i32 loc(#loc244)
        ttng.wait_barrier %117, %acc_90 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc244)
        %118 = ttg.memdesc_index %arg42[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %119 = ttg.memdesc_index %arg74[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %119, %acc_90 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc244)
        ttng.tc_gen5_mma %acc_82, %v_88, %acc_89, %true_35, %true_35, %115[%true_35], %116[%true_35], %118[%true_35] {async_task_id = array<i32: 1>, is_async, tmem.end = array<i32: 18, 18>, tmem.start = array<i32: 17, 17, 19, 19>, tt.self_latency = 1 : i32} : !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc244)
        ttng.tc_gen5_commit %arg43 {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc244)
        ttng.tc_gen5_commit %arg44 {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc242)
        %tile_idx_91 = arith.addi %tile_idx_41, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc134)
        scf.yield {async_task_id = array<i32: 1>} %tile_idx_91, %offsetkv_y_77, %offsetkv_y_76 : i64, i64, i64 loc(#loc51)
      } {async_task_id = array<i32: 1>, tt.warp_specialize, ttg.partition.stages = [0 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], ttg.warp_specialize.tag = 0 : i32} loc(#loc134)
      ttg.warp_return {async_task_id = array<i32: 1>} loc(#loc)
    }
    partition1(%Z_18: i32 loc("Z"(#loc)), %H_19: i32 loc("H"(#loc)), %arg26: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg27: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %v_20: !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc120 at #loc121)), %arg29: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %qk_21: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc124 at #loc206)), %q0_22: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc123 at #loc3)), %arg32: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %acc_23: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc125 at #loc206)), %arg34: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg35: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %qk_24: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc124 at #loc205)), %q1_25: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc122 at #loc3)), %arg38: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg39: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %acc_26: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc125 at #loc205)), %arg41: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg42: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg43: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg44: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %desc_q_27: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_q"(#loc)), %desc_k_28: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_k"(#loc)), %desc_v_29: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_v"(#loc)), %arg48: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc4 at #loc3)), %desc_o_30: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_o"(#loc)), %arg50: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc2 at #loc3)), %sm_scale_31: f32 loc("sm_scale"(#loc)), %arg52: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg53: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg54: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg55: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg56: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg57: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg58: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg59: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg60: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg61: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg62: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg63: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg64: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg65: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg66: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg67: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg68: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg69: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg70: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg71: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg72: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg73: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg74: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg75: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0)) num_warps(1) {
      %c3_i64 = arith.constant 3 : i64 loc(#loc1)
      %c2_i64 = arith.constant {async_task_id = array<i32: 2>} 2 : i64 loc(#loc1)
      %true_32 = arith.constant {async_task_id = array<i32: 2>} true loc(#loc1)
      %c1_i64_33 = arith.constant {async_task_id = array<i32: 2>} 1 : i64 loc(#loc1)
      %c0_i64_34 = arith.constant {async_task_id = array<i32: 2>} 0 : i64 loc(#loc1)
      %n_tile_num = arith.constant {async_task_id = array<i32: 2>} 32 : i32 loc(#loc227)
      %c1_i32_35 = arith.constant {async_task_id = array<i32: 2>} 1 : i32 loc(#loc1)
      %c8192_i32_36 = arith.constant {async_task_id = array<i32: 2>} 8192 : i32 loc(#loc1)
      %c0_i32_37 = arith.constant {async_task_id = array<i32: 2>} 0 : i32 loc(#loc1)
      %c256_i32_38 = arith.constant {async_task_id = array<i32: 2>} 256 : i32 loc(#loc1)
      %c128_i32_39 = arith.constant {async_task_id = array<i32: 2>} 128 : i32 loc(#loc1)
      %prog_id = tt.get_program_id x {async_task_id = array<i32: 2>} : i32 loc(#loc126)
      %num_progs = tt.get_num_programs x {async_task_id = array<i32: 2>} : i32 loc(#loc127)
      %total_tiles = arith.muli %Z_18, %n_tile_num {async_task_id = array<i32: 2>} : i32 loc(#loc128)
      %total_tiles_40 = arith.muli %total_tiles, %H_19 {async_task_id = array<i32: 2>} : i32 loc(#loc129)
      %tiles_per_sm = arith.divsi %total_tiles_40, %num_progs {async_task_id = array<i32: 2>} : i32 loc(#loc207)
      %94 = arith.remsi %total_tiles_40, %num_progs {async_task_id = array<i32: 2>} : i32 loc(#loc18)
      %95 = arith.cmpi slt, %prog_id, %94 {async_task_id = array<i32: 2>} : i32 loc(#loc19)
      %96 = scf.if %95 -> (i32) {
        %tiles_per_sm_41 = arith.addi %tiles_per_sm, %c1_i32_35 {async_task_id = array<i32: 2>} : i32 loc(#loc208)
        scf.yield {async_task_id = array<i32: 2>} %tiles_per_sm_41 : i32 loc(#loc208)
      } else {
        scf.yield {async_task_id = array<i32: 2>} %tiles_per_sm : i32 loc(#loc1)
      } {async_task_id = array<i32: 2>} loc(#loc20)
      %offset_y = arith.muli %H_19, %c8192_i32_36 {async_task_id = array<i32: 2>} : i32 loc(#loc229)
      %tile_idx:3 = scf.for %tile_idx_41 = %c0_i32_37 to %96 step %c1_i32_35 iter_args(%prog_id_42 = %prog_id, %arg78 = %c0_i64_34, %arg79 = %c0_i64_34) -> (i32, i64, i64)  : i32 {
        %pid = arith.remsi %prog_id_42, %n_tile_num {async_task_id = array<i32: 2>} : i32 loc(#loc135)
        %off_hz = arith.divsi %prog_id_42, %n_tile_num {async_task_id = array<i32: 2>} : i32 loc(#loc136)
        %off_z = arith.divsi %off_hz, %H_19 {async_task_id = array<i32: 2>} : i32 loc(#loc230)
        %off_h = arith.remsi %off_hz, %H_19 {async_task_id = array<i32: 2>} : i32 loc(#loc231)
        %offset_y_43 = arith.muli %off_z, %offset_y {async_task_id = array<i32: 2>} : i32 loc(#loc232)
        %offset_y_44 = arith.muli %off_h, %c8192_i32_36 {async_task_id = array<i32: 2>} : i32 loc(#loc233)
        %offset_y_45 = arith.addi %offset_y_43, %offset_y_44 {async_task_id = array<i32: 2>} : i32 loc(#loc234)
        %qo_offset_y = arith.muli %pid, %c256_i32_38 {async_task_id = array<i32: 2>} : i32 loc(#loc211)
        %qo_offset_y_46 = arith.addi %offset_y_45, %qo_offset_y {async_task_id = array<i32: 2>} : i32 loc(#loc235)
        %offsetkv_y = arith.andi %arg78, %c1_i64_33 {async_task_id = array<i32: 2>} : i64 loc(#loc348)
        %offsetkv_y_47 = arith.trunci %offsetkv_y {async_task_id = array<i32: 2>} : i64 to i1 loc(#loc348)
        %97 = ttg.memdesc_index %arg44[%c0_i32_37] {async_task_id = array<i32: 2>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %q0_48 = arith.xori %offsetkv_y_47, %true_32 {async_task_id = array<i32: 2>} : i1 loc(#loc204)
        %q0_49 = arith.extsi %q0_48 {async_task_id = array<i32: 2>} : i1 to i32 loc(#loc204)
        ttng.wait_barrier %97, %q0_49 {async_task_id = array<i32: 2>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc204)
        %98 = ttg.memdesc_index %arg27[%c0_i32_37] {async_task_id = array<i32: 2>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.barrier_expect %98, 16384 {async_task_id = array<i32: 2>}, %true_32 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %q0_50 = ttg.memdesc_index %q0_22[%c0_i32_37] {async_task_id = array<i32: 2>} : !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc204)
        ttng.async_tma_copy_global_to_local %desc_q_27[%qo_offset_y_46, %c0_i32_37] %q0_50, %98, %true_32 {async_task_id = array<i32: 2>} : !tt.tensordesc<tensor<128x64xbf16, #shared>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc)
        %q1_51 = arith.addi %qo_offset_y_46, %c128_i32_39 {async_task_id = array<i32: 2>} : i32 loc(#loc236)
        %99 = ttg.memdesc_index %arg26[%c0_i32_37] {async_task_id = array<i32: 2>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.barrier_expect %99, 16384 {async_task_id = array<i32: 2>}, %true_32 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %q1_52 = ttg.memdesc_index %q1_25[%c0_i32_37] {async_task_id = array<i32: 2>} : !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc203)
        ttng.async_tma_copy_global_to_local %desc_q_27[%q1_51, %c0_i32_37] %q1_52, %99, %true_32 {async_task_id = array<i32: 2>} : !tt.tensordesc<tensor<128x64xbf16, #shared>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc)
        %offsetkv_y_53:2 = scf.for %offsetkv_y_56 = %c0_i32_37 to %c8192_i32_36 step %c128_i32_39 iter_args(%offset_y_57 = %offset_y_45, %arg82 = %arg79) -> (i32, i64)  : i32 {
          %k = arith.divui %arg82, %c3_i64 {async_task_id = array<i32: 2>} : i64 loc(#loc228)
          %k_58 = arith.muli %k, %c3_i64 {async_task_id = array<i32: 2>} : i64 loc(#loc228)
          %k_59 = arith.subi %arg82, %k_58 {async_task_id = array<i32: 2>} : i64 loc(#loc228)
          %k_60 = arith.trunci %k_59 {async_task_id = array<i32: 2>} : i64 to i32 loc(#loc228)
          %k_61 = arith.andi %k, %c1_i64_33 {async_task_id = array<i32: 2>} : i64 loc(#loc228)
          %k_62 = arith.trunci %k_61 {async_task_id = array<i32: 2>} : i64 to i1 loc(#loc228)
          %100 = ttg.memdesc_index %arg38[%k_60] {async_task_id = array<i32: 2>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %k_63 = arith.xori %k_62, %true_32 {async_task_id = array<i32: 2>} : i1 loc(#loc237)
          %k_64 = arith.extsi %k_63 {async_task_id = array<i32: 2>} : i1 to i32 loc(#loc237)
          ttng.wait_barrier %100, %k_64 {async_task_id = array<i32: 2>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc237)
          %101 = ttg.memdesc_index %arg29[%k_60] {async_task_id = array<i32: 2>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.barrier_expect %101, 16384 {async_task_id = array<i32: 2>}, %true_32 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %k_65 = ttg.memdesc_index %v_20[%k_60] {async_task_id = array<i32: 2>} : !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc228)
          ttng.async_tma_copy_global_to_local %desc_k_28[%offset_y_57, %c0_i32_37] %k_65, %101, %true_32 {async_task_id = array<i32: 2>} : !tt.tensordesc<tensor<128x64xbf16, #shared>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc)
          %v_66 = arith.addi %arg82, %c1_i64_33 {async_task_id = array<i32: 2>} : i64 loc(#loc202)
          %v_67 = arith.divui %v_66, %c3_i64 {async_task_id = array<i32: 2>} : i64 loc(#loc202)
          %v_68 = arith.muli %v_67, %c3_i64 {async_task_id = array<i32: 2>} : i64 loc(#loc202)
          %v_69 = arith.subi %v_66, %v_68 {async_task_id = array<i32: 2>} : i64 loc(#loc202)
          %v_70 = arith.trunci %v_69 {async_task_id = array<i32: 2>} : i64 to i32 loc(#loc202)
          %v_71 = arith.andi %v_67, %c1_i64_33 {async_task_id = array<i32: 2>} : i64 loc(#loc202)
          %v_72 = arith.trunci %v_71 {async_task_id = array<i32: 2>} : i64 to i1 loc(#loc202)
          %102 = ttg.memdesc_index %arg38[%v_70] {async_task_id = array<i32: 2>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %v_73 = arith.xori %v_72, %true_32 {async_task_id = array<i32: 2>} : i1 loc(#loc202)
          %v_74 = arith.extsi %v_73 {async_task_id = array<i32: 2>} : i1 to i32 loc(#loc202)
          ttng.wait_barrier %102, %v_74 {async_task_id = array<i32: 2>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc202)
          %103 = ttg.memdesc_index %arg29[%v_70] {async_task_id = array<i32: 2>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.barrier_expect %103, 16384 {async_task_id = array<i32: 2>}, %true_32 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %v_75 = ttg.memdesc_index %v_20[%v_70] {async_task_id = array<i32: 2>} : !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc202)
          ttng.async_tma_copy_global_to_local %desc_v_29[%offset_y_57, %c0_i32_37] %v_75, %103, %true_32 {async_task_id = array<i32: 2>} : !tt.tensordesc<tensor<128x64xbf16, #shared>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc)
          %offsetkv_y_76 = arith.addi %arg82, %c2_i64 {async_task_id = array<i32: 2>} : i64 loc(#loc348)
          %offsetkv_y_77 = arith.addi %offset_y_57, %c128_i32_39 {async_task_id = array<i32: 2>} : i32 loc(#loc238)
          scf.yield %offsetkv_y_77, %offsetkv_y_76 : i32, i64 loc(#loc239)
        } {async_task_id = array<i32: 2>, tt.disallow_acc_multi_buffer, tt.warp_specialize} loc(#loc348)
        %tile_idx_54 = arith.addi %prog_id_42, %num_progs {async_task_id = array<i32: 2>} : i32 loc(#loc159)
        %tile_idx_55 = arith.addi %arg78, %c1_i64_33 {async_task_id = array<i32: 2>} : i64 loc(#loc134)
        scf.yield %tile_idx_54, %tile_idx_55, %offsetkv_y_53#1 : i32, i64, i64 loc(#loc51)
      } {async_task_id = array<i32: 2>, tt.warp_specialize, ttg.partition.stages = [0 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], ttg.warp_specialize.tag = 0 : i32} loc(#loc134)
      ttg.warp_return {async_task_id = array<i32: 2>} loc(#loc)
    }
    partition2(%Z_18: i32 loc("Z"(#loc)), %H_19: i32 loc("H"(#loc)), %arg26: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg27: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %v_20: !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc120 at #loc121)), %arg29: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %qk_21: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc124 at #loc206)), %q0_22: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc123 at #loc3)), %arg32: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %acc_23: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc125 at #loc206)), %arg34: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg35: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %qk_24: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc124 at #loc205)), %q1_25: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc122 at #loc3)), %arg38: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg39: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %acc_26: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc125 at #loc205)), %arg41: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg42: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg43: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg44: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %desc_q_27: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_q"(#loc)), %desc_k_28: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_k"(#loc)), %desc_v_29: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_v"(#loc)), %arg48: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc4 at #loc3)), %desc_o_30: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_o"(#loc)), %arg50: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc2 at #loc3)), %sm_scale_31: f32 loc("sm_scale"(#loc)), %arg52: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg53: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg54: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg55: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg56: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg57: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg58: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg59: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg60: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg61: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg62: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg63: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg64: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg65: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg66: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg67: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg68: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg69: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg70: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg71: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg72: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg73: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg74: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg75: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0)) num_warps(1) {
      %c1_i64_32 = arith.constant {async_task_id = array<i32: 3>} 1 : i64 loc(#loc1)
      %c0_i64_33 = arith.constant {async_task_id = array<i32: 3>} 0 : i64 loc(#loc1)
      %n_tile_num = arith.constant {async_task_id = array<i32: 3>} 32 : i32 loc(#loc227)
      %c1_i32_34 = arith.constant {async_task_id = array<i32: 3>} 1 : i32 loc(#loc1)
      %c8192_i32_35 = arith.constant {async_task_id = array<i32: 3>} 8192 : i32 loc(#loc1)
      %c0_i32_36 = arith.constant {async_task_id = array<i32: 3>} 0 : i32 loc(#loc1)
      %c256_i32_37 = arith.constant {async_task_id = array<i32: 3>} 256 : i32 loc(#loc1)
      %c128_i32_38 = arith.constant {async_task_id = array<i32: 3>} 128 : i32 loc(#loc1)
      %prog_id = tt.get_program_id x {async_task_id = array<i32: 3>} : i32 loc(#loc126)
      %num_progs = tt.get_num_programs x {async_task_id = array<i32: 3>} : i32 loc(#loc127)
      %total_tiles = arith.muli %Z_18, %n_tile_num {async_task_id = array<i32: 3>} : i32 loc(#loc128)
      %total_tiles_39 = arith.muli %total_tiles, %H_19 {async_task_id = array<i32: 3>} : i32 loc(#loc129)
      %tiles_per_sm = arith.divsi %total_tiles_39, %num_progs {async_task_id = array<i32: 3>} : i32 loc(#loc207)
      %94 = arith.remsi %total_tiles_39, %num_progs {async_task_id = array<i32: 3>} : i32 loc(#loc18)
      %95 = arith.cmpi slt, %prog_id, %94 {async_task_id = array<i32: 3>} : i32 loc(#loc19)
      %96 = scf.if %95 -> (i32) {
        %tiles_per_sm_40 = arith.addi %tiles_per_sm, %c1_i32_34 {async_task_id = array<i32: 3>} : i32 loc(#loc208)
        scf.yield {async_task_id = array<i32: 3>} %tiles_per_sm_40 : i32 loc(#loc208)
      } else {
        scf.yield {async_task_id = array<i32: 3>} %tiles_per_sm : i32 loc(#loc1)
      } {async_task_id = array<i32: 3>} loc(#loc20)
      %offset_y = arith.muli %H_19, %c8192_i32_35 {async_task_id = array<i32: 3>} : i32 loc(#loc229)
      %tile_idx:2 = scf.for %tile_idx_40 = %c0_i32_36 to %96 step %c1_i32_34 iter_args(%prog_id_41 = %prog_id, %tile_idx_42 = %c0_i64_33) -> (i32, i64)  : i32 {
        %pid = arith.remsi %prog_id_41, %n_tile_num {async_task_id = array<i32: 3>} : i32 loc(#loc135)
        %off_hz = arith.divsi %prog_id_41, %n_tile_num {async_task_id = array<i32: 3>} : i32 loc(#loc136)
        %off_z = arith.divsi %off_hz, %H_19 {async_task_id = array<i32: 3>} : i32 loc(#loc230)
        %off_h = arith.remsi %off_hz, %H_19 {async_task_id = array<i32: 3>} : i32 loc(#loc231)
        %offset_y_43 = arith.muli %off_z, %offset_y {async_task_id = array<i32: 3>} : i32 loc(#loc232)
        %offset_y_44 = arith.muli %off_h, %c8192_i32_35 {async_task_id = array<i32: 3>} : i32 loc(#loc233)
        %offset_y_45 = arith.addi %offset_y_43, %offset_y_44 {async_task_id = array<i32: 3>} : i32 loc(#loc234)
        %qo_offset_y = arith.muli %pid, %c256_i32_37 {async_task_id = array<i32: 3>} : i32 loc(#loc211)
        %qo_offset_y_46 = arith.addi %offset_y_45, %qo_offset_y {async_task_id = array<i32: 3>} : i32 loc(#loc235)
        %q1_47 = arith.addi %qo_offset_y_46, %c128_i32_38 {async_task_id = array<i32: 3>} : i32 loc(#loc236)
        %97 = arith.andi %tile_idx_42, %c1_i64_32 {async_task_id = array<i32: 3>} : i64 loc(#loc119)
        %98 = arith.trunci %97 {async_task_id = array<i32: 3>} : i64 to i1 loc(#loc119)
        %99 = ttg.memdesc_index %arg48[%c0_i32_36] {async_task_id = array<i32: 3>} : !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc119)
        %100 = ttg.memdesc_index %arg54[%c0_i32_36] {async_task_id = array<i32: 3>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %101 = arith.extui %98 : i1 to i32 loc(#loc119)
        ttng.wait_barrier %100, %101 {async_task_id = array<i32: 3>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc119)
        ttng.fence_async_shared {bCluster = false} loc(#loc172)
        ttng.async_tma_copy_local_to_global %desc_o_30[%qo_offset_y_46, %c0_i32_36] %99 : !tt.tensordesc<tensor<128x64xbf16, #shared>>, !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc172)
        ttng.async_tma_store_wait {pendings = 0 : i32} loc(#loc172)
        %102 = ttg.memdesc_index %arg55[%c0_i32_36] {async_task_id = array<i32: 3>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %102, 1 {async_task_id = array<i32: 3>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc172)
        %103 = ttg.memdesc_index %arg50[%c0_i32_36] {async_task_id = array<i32: 3>} : !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc118)
        %104 = ttg.memdesc_index %arg52[%c0_i32_36] {async_task_id = array<i32: 3>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %104, %101 {async_task_id = array<i32: 3>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc118)
        ttng.fence_async_shared {bCluster = false} loc(#loc173)
        ttng.async_tma_copy_local_to_global %desc_o_30[%q1_47, %c0_i32_36] %103 : !tt.tensordesc<tensor<128x64xbf16, #shared>>, !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc173)
        ttng.async_tma_store_wait {pendings = 0 : i32} loc(#loc173)
        %105 = ttg.memdesc_index %arg53[%c0_i32_36] {async_task_id = array<i32: 3>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %105, 1 {async_task_id = array<i32: 3>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc173)
        %tile_idx_48 = arith.addi %prog_id_41, %num_progs {async_task_id = array<i32: 3>} : i32 loc(#loc159)
        %tile_idx_49 = arith.addi %tile_idx_42, %c1_i64_32 {async_task_id = array<i32: 3>} : i64 loc(#loc134)
        scf.yield {async_task_id = array<i32: 3>} %tile_idx_48, %tile_idx_49 : i32, i64 loc(#loc51)
      } {async_task_id = array<i32: 3>, tt.warp_specialize, ttg.partition.stages = [0 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], ttg.warp_specialize.tag = 0 : i32} loc(#loc134)
      ttg.warp_return {async_task_id = array<i32: 3>} loc(#loc)
    }
    partition3(%Z_18: i32 loc("Z"(#loc)), %H_19: i32 loc("H"(#loc)), %arg26: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg27: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %v_20: !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc120 at #loc121)), %arg29: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %qk_21: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc124 at #loc206)), %q0_22: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc123 at #loc3)), %arg32: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %acc_23: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc125 at #loc206)), %arg34: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg35: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %qk_24: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc124 at #loc205)), %q1_25: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc122 at #loc3)), %arg38: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg39: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %acc_26: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc125 at #loc205)), %arg41: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg42: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg43: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg44: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %desc_q_27: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_q"(#loc)), %desc_k_28: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_k"(#loc)), %desc_v_29: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_v"(#loc)), %arg48: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc4 at #loc3)), %desc_o_30: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_o"(#loc)), %arg50: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc2 at #loc3)), %sm_scale_31: f32 loc("sm_scale"(#loc)), %arg52: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg53: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg54: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg55: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg56: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg57: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg58: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg59: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg60: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg61: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg62: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg63: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg64: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg65: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg66: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg67: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg68: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg69: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg70: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg71: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg72: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg73: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg74: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg75: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0)) num_warps(4) {
      %true_32 = arith.constant {async_task_id = array<i32: 4>} true loc(#loc1)
      %c1_i64_33 = arith.constant {async_task_id = array<i32: 4>} 1 : i64 loc(#loc1)
      %c0_i64_34 = arith.constant {async_task_id = array<i32: 4>} 0 : i64 loc(#loc1)
      %n_tile_num = arith.constant {async_task_id = array<i32: 4>} 32 : i32 loc(#loc227)
      %c1_i32_35 = arith.constant {async_task_id = array<i32: 4>} 1 : i32 loc(#loc1)
      %c8192_i32_36 = arith.constant {async_task_id = array<i32: 4>} 8192 : i32 loc(#loc1)
      %c0_i32_37 = arith.constant {async_task_id = array<i32: 4>} 0 : i32 loc(#loc1)
      %cst_38 = arith.constant {async_task_id = array<i32: 4>} 1.44269502 : f32 loc(#loc1)
      %c128_i32_39 = arith.constant {async_task_id = array<i32: 4>} 128 : i32 loc(#loc1)
      %cst_40 = arith.constant {async_task_id = array<i32: 4>} dense<0xFF800000> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc1)
      %cst_41 = arith.constant dense<1.000000e+00> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #linear1}>> loc(#loc1)
      %prog_id = tt.get_program_id x {async_task_id = array<i32: 4>} : i32 loc(#loc126)
      %num_progs = tt.get_num_programs x {async_task_id = array<i32: 4>} : i32 loc(#loc127)
      %total_tiles = arith.muli %Z_18, %n_tile_num {async_task_id = array<i32: 4>} : i32 loc(#loc128)
      %total_tiles_42 = arith.muli %total_tiles, %H_19 {async_task_id = array<i32: 4>} : i32 loc(#loc129)
      %tiles_per_sm = arith.divsi %total_tiles_42, %num_progs {async_task_id = array<i32: 4>} : i32 loc(#loc207)
      %94 = arith.remsi %total_tiles_42, %num_progs {async_task_id = array<i32: 4>} : i32 loc(#loc18)
      %95 = arith.cmpi slt, %prog_id, %94 {async_task_id = array<i32: 4>} : i32 loc(#loc19)
      %96 = scf.if %95 -> (i32) {
        %tiles_per_sm_44 = arith.addi %tiles_per_sm, %c1_i32_35 {async_task_id = array<i32: 4>} : i32 loc(#loc208)
        scf.yield {async_task_id = array<i32: 4>} %tiles_per_sm_44 : i32 loc(#loc208)
      } else {
        scf.yield {async_task_id = array<i32: 4>} %tiles_per_sm : i32 loc(#loc1)
      } {async_task_id = array<i32: 4>} loc(#loc20)
      %qk_scale = arith.mulf %sm_scale_31, %cst_38 {async_task_id = array<i32: 4>} : f32 loc(#loc240)
      %m_ij = tt.splat %qk_scale {async_task_id = array<i32: 4>} : f32 -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc254)
      %qk_43 = tt.splat %qk_scale {async_task_id = array<i32: 4>} : f32 -> tensor<128x128xf32, #blocked4> loc(#loc255)
      %tile_idx:2 = scf.for %tile_idx_44 = %c0_i32_37 to %96 step %c1_i32_35 iter_args(%arg77 = %c0_i64_34, %arg78 = %c0_i64_34) -> (i64, i64)  : i32 {
        %offsetkv_y:3 = scf.for %offsetkv_y_60 = %c0_i32_37 to %c8192_i32_36 step %c128_i32_39 iter_args(%arg80 = %cst_41, %arg81 = %cst_40, %arg82 = %arg78) -> (tensor<128xf32, #ttg.slice<{dim = 1, parent = #linear1}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>, i64)  : i32 {
          %qk_61 = arith.andi %arg82, %c1_i64_33 {async_task_id = array<i32: 4>} : i64 loc(#loc241)
          %qk_62 = arith.trunci %qk_61 {async_task_id = array<i32: 4>} : i64 to i1 loc(#loc241)
          %qk_63 = ttg.memdesc_index %qk_24[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc241)
          %101 = ttg.memdesc_index %arg39[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %qk_64 = arith.extsi %qk_62 {async_task_id = array<i32: 4>} : i1 to i32 loc(#loc241)
          ttng.wait_barrier %101, %qk_64 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc241)
          %102 = ttg.memdesc_index %arg59[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %qk_65 = ttng.tmem_load %qk_63 {async_task_id = array<i32: 4>} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> tensor<128x128xf32, #blocked4> loc(#loc241)
          ttng.arrive_barrier %102, 1 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc241)
          %m_ij_66 = "tt.reduce"(%qk_65) <{axis = 1 : i32}> ({
          ^bb0(%m_ij_105: f32 loc(callsite(#loc1 at #loc256)), %m_ij_106: f32 loc(callsite(#loc1 at #loc256))):
            %m_ij_107 = arith.maxnumf %m_ij_105, %m_ij_106 {async_task_id = array<i32: 4>} : f32 loc(#loc340)
            tt.reduce.return %m_ij_107 {async_task_id = array<i32: 4>} : f32 loc(#loc331)
          }) {async_task_id = array<i32: 4>} : (tensor<128x128xf32, #blocked4>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc331)
          %m_ij_67 = arith.mulf %m_ij_66, %m_ij {async_task_id = array<i32: 4>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc257)
          %m_ij_68 = arith.maxnumf %arg81, %m_ij_67 {async_task_id = array<i32: 4>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc258)
          %qk_69 = arith.mulf %qk_65, %qk_43 {async_task_id = array<i32: 4>} : tensor<128x128xf32, #blocked4> loc(#loc259)
          %qk_70 = tt.expand_dims %m_ij_68 {async_task_id = array<i32: 4>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<128x1xf32, #blocked4> loc(#loc260)
          %qk_71 = tt.broadcast %qk_70 {async_task_id = array<i32: 4>} : tensor<128x1xf32, #blocked4> -> tensor<128x128xf32, #blocked4> loc(#loc261)
          %qk_72 = arith.subf %qk_69, %qk_71 {async_task_id = array<i32: 4>} : tensor<128x128xf32, #blocked4> loc(#loc261)
          %103 = tt.reshape %qk_72 {async_task_id = array<i32: 4>} : tensor<128x128xf32, #blocked4> -> tensor<128x2x64xf32, #blocked5> loc(#loc262)
          %104 = tt.trans %103 {async_task_id = array<i32: 4>, order = array<i32: 0, 2, 1>} : tensor<128x2x64xf32, #blocked5> -> tensor<128x64x2xf32, #blocked6> loc(#loc263)
          %outLHS, %outRHS = tt.split %104 {async_task_id = array<i32: 4>} : tensor<128x64x2xf32, #blocked6> -> tensor<128x64xf32, #blocked> loc(#loc264)
          %105 = tt.reshape %outLHS {async_task_id = array<i32: 4>} : tensor<128x64xf32, #blocked> -> tensor<128x2x32xf32, #blocked7> loc(#loc265)
          %106 = tt.trans %105 {async_task_id = array<i32: 4>, order = array<i32: 0, 2, 1>} : tensor<128x2x32xf32, #blocked7> -> tensor<128x32x2xf32, #blocked8> loc(#loc266)
          %outLHS_73, %outRHS_74 = tt.split %106 {async_task_id = array<i32: 4>} : tensor<128x32x2xf32, #blocked8> -> tensor<128x32xf32, #blocked2> loc(#loc267)
          %107 = tt.reshape %outRHS {async_task_id = array<i32: 4>} : tensor<128x64xf32, #blocked> -> tensor<128x2x32xf32, #blocked7> loc(#loc268)
          %108 = tt.trans %107 {async_task_id = array<i32: 4>, order = array<i32: 0, 2, 1>} : tensor<128x2x32xf32, #blocked7> -> tensor<128x32x2xf32, #blocked8> loc(#loc269)
          %outLHS_75, %outRHS_76 = tt.split %108 {async_task_id = array<i32: 4>} : tensor<128x32x2xf32, #blocked8> -> tensor<128x32xf32, #blocked2> loc(#loc270)

          // --- barrier id 9 wait
          %c9_i32_p3 = arith.constant 9 : i32
          %c10_i32_p3 = arith.constant 10 : i32
          %c256_i32_p3 = arith.constant 256 : i32
          ttng.wait_barrier_named %c9_i32_p3, %c256_i32_p3 : i32, i32
          // --- barrier id 9 wait

          %p00 = math.exp2 %outLHS_73 {async_task_id = array<i32: 4>} : tensor<128x32xf32, #blocked2> loc(#loc271)
          %p00_bf16 = arith.truncf %p00 {async_task_id = array<i32: 4>} : tensor<128x32xf32, #blocked2> to tensor<128x32xbf16, #blocked2> loc(#loc272)
          %p01 = math.exp2 %outRHS_74 {async_task_id = array<i32: 4>} : tensor<128x32xf32, #blocked2> loc(#loc273)
          %p01_bf16 = arith.truncf %p01 {async_task_id = array<i32: 4>} : tensor<128x32xf32, #blocked2> to tensor<128x32xbf16, #blocked2> loc(#loc274)
          %p10 = math.exp2 %outLHS_75 {async_task_id = array<i32: 4>} : tensor<128x32xf32, #blocked2> loc(#loc275)
          %p10_bf16 = arith.truncf %p10 {async_task_id = array<i32: 4>} : tensor<128x32xf32, #blocked2> to tensor<128x32xbf16, #blocked2> loc(#loc276)
          %p11 = math.exp2 %outRHS_76 {async_task_id = array<i32: 4>} : tensor<128x32xf32, #blocked2> loc(#loc277)
          %p11_bf16 = arith.truncf %p11 {async_task_id = array<i32: 4>} : tensor<128x32xf32, #blocked2> to tensor<128x32xbf16, #blocked2> loc(#loc278)

          %p0 = tt.join %p00, %p01 {async_task_id = array<i32: 4>} : tensor<128x32xf32, #blocked2> -> tensor<128x32x2xf32, #blocked9> loc(#loc279)
          %p0_77 = tt.trans %p0 {async_task_id = array<i32: 4>, order = array<i32: 0, 2, 1>} : tensor<128x32x2xf32, #blocked9> -> tensor<128x2x32xf32, #blocked10> loc(#loc280)
          %p0_78 = tt.reshape %p0_77 {async_task_id = array<i32: 4>} : tensor<128x2x32xf32, #blocked10> -> tensor<128x64xf32, #linear2> loc(#loc281)
          %p1 = tt.join %p10, %p11 {async_task_id = array<i32: 4>} : tensor<128x32xf32, #blocked2> -> tensor<128x32x2xf32, #blocked9> loc(#loc282)
          %p1_79 = tt.trans %p1 {async_task_id = array<i32: 4>, order = array<i32: 0, 2, 1>} : tensor<128x32x2xf32, #blocked9> -> tensor<128x2x32xf32, #blocked10> loc(#loc283)
          %p1_80 = tt.reshape %p1_79 {async_task_id = array<i32: 4>} : tensor<128x2x32xf32, #blocked10> -> tensor<128x64xf32, #linear2> loc(#loc284)
          %p = tt.join %p0_78, %p1_80 {async_task_id = array<i32: 4>} : tensor<128x64xf32, #linear2> -> tensor<128x64x2xf32, #linear3> loc(#loc285)
          %p_81 = tt.trans %p {async_task_id = array<i32: 4>, order = array<i32: 0, 2, 1>} : tensor<128x64x2xf32, #linear3> -> tensor<128x2x64xf32, #linear4> loc(#loc286)
          %p_82 = tt.reshape %p_81 {async_task_id = array<i32: 4>} : tensor<128x2x64xf32, #linear4> -> tensor<128x128xf32, #linear1> loc(#loc287)

          %qk_83 = ttng.tmem_subslice %qk_24 {N = 0 : i32, async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc241)
          %qk_84 = ttg.memdesc_reinterpret %qk_83 {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc241)
          %acc_85 = ttg.memdesc_index %qk_84[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc244)

          %109 = ttg.memdesc_index %arg41[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %acc_86 = arith.xori %qk_62, %true_32 {async_task_id = array<i32: 4>} : i1 loc(#loc244)
          %acc_87 = arith.extsi %acc_86 {async_task_id = array<i32: 4>} : i1 to i32 loc(#loc244)
          ttng.wait_barrier %109, %acc_87 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc244)

          %acc_88 = ttng.tmem_subslice %acc_85 {N = 0 : i32} : !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xbf16, #tmem5, #ttng.tensor_memory, mutable, 128x128> loc(#loc244)
          %acc_89 = ttng.tmem_subslice %acc_88 {N = 0 : i32} : !ttg.memdesc<128x64xbf16, #tmem5, #ttng.tensor_memory, mutable, 128x128> -> !ttg.memdesc<128x32xbf16, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc244)
          ttng.tmem_store %p00_bf16, %acc_89, %true_32 : tensor<128x32xbf16, #blocked2> -> !ttg.memdesc<128x32xbf16, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc244)
          %acc_90 = ttng.tmem_subslice %acc_88 {N = 32 : i32} : !ttg.memdesc<128x64xbf16, #tmem5, #ttng.tensor_memory, mutable, 128x128> -> !ttg.memdesc<128x32xbf16, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc244)
          ttng.tmem_store %p01_bf16, %acc_90, %true_32 : tensor<128x32xbf16, #blocked2> -> !ttg.memdesc<128x32xbf16, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc244)
          %acc_91 = ttng.tmem_subslice %acc_85 {N = 64 : i32} : !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xbf16, #tmem5, #ttng.tensor_memory, mutable, 128x128> loc(#loc244)
          %acc_92 = ttng.tmem_subslice %acc_91 {N = 0 : i32} : !ttg.memdesc<128x64xbf16, #tmem5, #ttng.tensor_memory, mutable, 128x128> -> !ttg.memdesc<128x32xbf16, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc244)
          ttng.tmem_store %p10_bf16, %acc_92, %true_32 : tensor<128x32xbf16, #blocked2> -> !ttg.memdesc<128x32xbf16, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc244)
          %acc_93 = ttng.tmem_subslice %acc_91 {N = 32 : i32} : !ttg.memdesc<128x64xbf16, #tmem5, #ttng.tensor_memory, mutable, 128x128> -> !ttg.memdesc<128x32xbf16, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc244)
          ttng.tmem_store %p11_bf16, %acc_93, %true_32 : tensor<128x32xbf16, #blocked2> -> !ttg.memdesc<128x32xbf16, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc244)

          %110 = ttg.memdesc_index %arg58[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.arrive_barrier %110, 1 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc244)

          // --- barrier id 10 arrive
          ttng.arrive_barrier_named %c10_i32_p3, %c256_i32_p3 : i32, i32
          // --- barrier id 10 arrive

          %alpha = arith.subf %arg81, %m_ij_68 {async_task_id = array<i32: 4>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc288)
          %alpha_94 = math.exp2 %alpha {async_task_id = array<i32: 4>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc250)
          %alpha_95 = ttg.convert_layout %alpha_94 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #linear1}>> loc(#loc250)
          %alpha_96 = ttg.convert_layout %alpha_94 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc250)
          %alpha_97 = tt.expand_dims %alpha_96 {async_task_id = array<i32: 4>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc250)
          %qk_98 = ttng.tmem_subslice %qk_24 {N = 64 : i32, async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc241)
          %qk_99 = ttg.memdesc_reinterpret %qk_98 {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc241)
          %alpha_100 = ttg.memdesc_index %qk_99[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc250)
          %111 = ttg.memdesc_index %arg57[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %alpha_101 = arith.xori %qk_62, %true_32 : i1 loc(#loc250)
          %alpha_102 = arith.extui %alpha_101 : i1 to i32 loc(#loc250)
          ttng.wait_barrier %111, %alpha_102 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc250)
          ttng.tmem_store %alpha_97, %alpha_100, %true_32 {async_task_id = array<i32: 4>} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc250)
          %112 = ttg.memdesc_index %arg56[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.arrive_barrier %112, 1 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc250)
          %l_ij = "tt.reduce"(%p_82) <{axis = 1 : i32}> ({
          ^bb0(%l_ij_105: f32 loc(callsite(#loc1 at #loc289)), %l_ij_106: f32 loc(callsite(#loc1 at #loc289))):
            %l_ij_107 = arith.addf %l_ij_105, %l_ij_106 {async_task_id = array<i32: 4>} : f32 loc(#loc341)
            tt.reduce.return %l_ij_107 {async_task_id = array<i32: 4>} : f32 loc(#loc333)
          }) {async_task_id = array<i32: 4>} : (tensor<128x128xf32, #linear1>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #linear1}>> loc(#loc333)
          %l_i0 = arith.mulf %arg80, %alpha_95 {async_task_id = array<i32: 4>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #linear1}>> loc(#loc290)
          %l_i0_103 = arith.addf %l_i0, %l_ij {async_task_id = array<i32: 4>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #linear1}>> loc(#loc291)
          %offsetkv_y_104 = arith.addi %arg82, %c1_i64_33 {async_task_id = array<i32: 4>} : i64 loc(#loc348)
          scf.yield %l_i0_103, %m_ij_68, %offsetkv_y_104 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #linear1}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>, i64 loc(#loc239)
        } {async_task_id = array<i32: 4>, tt.disallow_acc_multi_buffer, tt.warp_specialize} loc(#loc348)
        %offsetkv_y_45 = ttg.convert_layout %offsetkv_y#1 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc348)
        %offsetkv_y_46 = tt.expand_dims %offsetkv_y_45 {async_task_id = array<i32: 4>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc348)
        %qk_47 = ttng.tmem_subslice %qk_24 {N = 65 : i32, async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc241)
        %qk_48 = ttg.memdesc_reinterpret %qk_47 {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc241)
        %offsetkv_y_49 = ttg.memdesc_index %qk_48[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc348)
        %offsetkv_y_50 = arith.andi %arg77, %c1_i64_33 {async_task_id = array<i32: 4>} : i64 loc(#loc348)
        %offsetkv_y_51 = arith.trunci %offsetkv_y_50 {async_task_id = array<i32: 4>} : i64 to i1 loc(#loc348)
        %97 = ttg.memdesc_index %arg65[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %offsetkv_y_52 = arith.xori %offsetkv_y_51, %true_32 : i1 loc(#loc348)
        %offsetkv_y_53 = arith.extui %offsetkv_y_52 : i1 to i32 loc(#loc348)
        ttng.wait_barrier %97, %offsetkv_y_53 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc348)
        ttng.tmem_store %offsetkv_y_46, %offsetkv_y_49, %true_32 {async_task_id = array<i32: 4>} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc348)
        %98 = ttg.memdesc_index %arg64[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %98, 1 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc348)
        %offsetkv_y_54 = ttg.convert_layout %offsetkv_y#0 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #linear1}>> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc348)
        %offsetkv_y_55 = tt.expand_dims %offsetkv_y_54 {async_task_id = array<i32: 4>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc348)
        %qk_56 = ttng.tmem_subslice %qk_24 {N = 66 : i32, async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc241)
        %qk_57 = ttg.memdesc_reinterpret %qk_56 {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc241)
        %offsetkv_y_58 = ttg.memdesc_index %qk_57[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc348)
        %99 = ttg.memdesc_index %arg69[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %99, %offsetkv_y_53 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc348)
        ttng.tmem_store %offsetkv_y_55, %offsetkv_y_58, %true_32 {async_task_id = array<i32: 4>} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc348)
        %100 = ttg.memdesc_index %arg68[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %100, 1 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc348)
        %tile_idx_59 = arith.addi %arg77, %c1_i64_33 {async_task_id = array<i32: 4>} : i64 loc(#loc134)
        scf.yield %tile_idx_59, %offsetkv_y#2 : i64, i64 loc(#loc51)
      } {async_task_id = array<i32: 4>, tt.warp_specialize, ttg.partition.stages = [0 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], ttg.warp_specialize.tag = 0 : i32} loc(#loc134)
      ttg.warp_return {async_task_id = array<i32: 4>} loc(#loc)
    }
    partition4(%Z_18: i32 loc("Z"(#loc)), %H_19: i32 loc("H"(#loc)), %arg26: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg27: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %v_20: !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc120 at #loc121)), %arg29: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %qk_21: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc124 at #loc206)), %q0_22: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc123 at #loc3)), %arg32: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %acc_23: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc125 at #loc206)), %arg34: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg35: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %qk_24: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc124 at #loc205)), %q1_25: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc122 at #loc3)), %arg38: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg39: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %acc_26: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc125 at #loc205)), %arg41: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg42: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg43: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg44: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %desc_q_27: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_q"(#loc)), %desc_k_28: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_k"(#loc)), %desc_v_29: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_v"(#loc)), %arg48: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc4 at #loc3)), %desc_o_30: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_o"(#loc)), %arg50: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc2 at #loc3)), %sm_scale_31: f32 loc("sm_scale"(#loc)), %arg52: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg53: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg54: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg55: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg56: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg57: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg58: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg59: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg60: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg61: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg62: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg63: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg64: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg65: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg66: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg67: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg68: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg69: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg70: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg71: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg72: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg73: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg74: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0), %arg75: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:0)) num_warps(4) {
      %true_32 = arith.constant {async_task_id = array<i32: 5>} true loc(#loc1)
      %c1_i64_33 = arith.constant {async_task_id = array<i32: 5>} 1 : i64 loc(#loc1)
      %c0_i64_34 = arith.constant {async_task_id = array<i32: 5>} 0 : i64 loc(#loc1)
      %n_tile_num = arith.constant {async_task_id = array<i32: 5>} 32 : i32 loc(#loc227)
      %c1_i32_35 = arith.constant {async_task_id = array<i32: 5>} 1 : i32 loc(#loc1)
      %c8192_i32_36 = arith.constant {async_task_id = array<i32: 5>} 8192 : i32 loc(#loc1)
      %c0_i32_37 = arith.constant {async_task_id = array<i32: 5>} 0 : i32 loc(#loc1)
      %cst_38 = arith.constant {async_task_id = array<i32: 5>} 1.44269502 : f32 loc(#loc1)
      %c128_i32_39 = arith.constant {async_task_id = array<i32: 5>} 128 : i32 loc(#loc1)
      %cst_40 = arith.constant {async_task_id = array<i32: 5>} dense<0xFF800000> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc1)
      %cst_41 = arith.constant dense<1.000000e+00> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #linear1}>> loc(#loc1)

      // --- barrier id 9 arrive
      %c256_i32_p4 = arith.constant 256 : i32
      %c9_i32_p4 = arith.constant 9 : i32
      %c10_i32_p4 = arith.constant 10 : i32
      ttng.arrive_barrier_named %c9_i32_p4, %c256_i32_p4 : i32, i32
      // --- barrier id 9 arrive

      %prog_id = tt.get_program_id x {async_task_id = array<i32: 5>} : i32 loc(#loc126)
      %num_progs = tt.get_num_programs x {async_task_id = array<i32: 5>} : i32 loc(#loc127)
      %total_tiles = arith.muli %Z_18, %n_tile_num {async_task_id = array<i32: 5>} : i32 loc(#loc128)
      %total_tiles_42 = arith.muli %total_tiles, %H_19 {async_task_id = array<i32: 5>} : i32 loc(#loc129)
      %tiles_per_sm = arith.divsi %total_tiles_42, %num_progs {async_task_id = array<i32: 5>} : i32 loc(#loc207)
      %94 = arith.remsi %total_tiles_42, %num_progs {async_task_id = array<i32: 5>} : i32 loc(#loc18)
      %95 = arith.cmpi slt, %prog_id, %94 {async_task_id = array<i32: 5>} : i32 loc(#loc19)
      %96 = scf.if %95 -> (i32) {
        %tiles_per_sm_44 = arith.addi %tiles_per_sm, %c1_i32_35 {async_task_id = array<i32: 5>} : i32 loc(#loc208)
        scf.yield {async_task_id = array<i32: 5>} %tiles_per_sm_44 : i32 loc(#loc208)
      } else {
        scf.yield {async_task_id = array<i32: 5>} %tiles_per_sm : i32 loc(#loc1)
      } {async_task_id = array<i32: 5>} loc(#loc20)
      %qk_scale = arith.mulf %sm_scale_31, %cst_38 {async_task_id = array<i32: 5>} : f32 loc(#loc240)
      %m_ij = tt.splat %qk_scale {async_task_id = array<i32: 5>} : f32 -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc254)
      %qk_43 = tt.splat %qk_scale {async_task_id = array<i32: 5>} : f32 -> tensor<128x128xf32, #blocked4> loc(#loc255)
      %tile_idx:2 = scf.for %tile_idx_44 = %c0_i32_37 to %96 step %c1_i32_35 iter_args(%arg77 = %c0_i64_34, %arg78 = %c0_i64_34) -> (i64, i64)  : i32 {
        %offsetkv_y:3 = scf.for %offsetkv_y_60 = %c0_i32_37 to %c8192_i32_36 step %c128_i32_39 iter_args(%arg80 = %cst_41, %arg81 = %cst_40, %arg82 = %arg78) -> (tensor<128xf32, #ttg.slice<{dim = 1, parent = #linear1}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>, i64)  : i32 {
          %qk_61 = arith.andi %arg82, %c1_i64_33 {async_task_id = array<i32: 5>} : i64 loc(#loc242)
          %qk_62 = arith.trunci %qk_61 {async_task_id = array<i32: 5>} : i64 to i1 loc(#loc242)
          %qk_63 = ttg.memdesc_index %qk_21[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc242)
          %101 = ttg.memdesc_index %arg32[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %qk_64 = arith.extsi %qk_62 {async_task_id = array<i32: 5>} : i1 to i32 loc(#loc242)
          ttng.wait_barrier %101, %qk_64 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc242)
          %102 = ttg.memdesc_index %arg63[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %qk_65 = ttng.tmem_load %qk_63 {async_task_id = array<i32: 5>} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> tensor<128x128xf32, #blocked4> loc(#loc242)
          ttng.arrive_barrier %102, 1 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc242)
          %m_ij_66 = "tt.reduce"(%qk_65) <{axis = 1 : i32}> ({
          ^bb0(%m_ij_105: f32 loc(callsite(#loc1 at #loc292)), %m_ij_106: f32 loc(callsite(#loc1 at #loc292))):
            %m_ij_107 = arith.maxnumf %m_ij_105, %m_ij_106 {async_task_id = array<i32: 5>} : f32 loc(#loc342)
            tt.reduce.return %m_ij_107 {async_task_id = array<i32: 5>} : f32 loc(#loc335)
          }) {async_task_id = array<i32: 5>} : (tensor<128x128xf32, #blocked4>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc335)
          %m_ij_67 = arith.mulf %m_ij_66, %m_ij {async_task_id = array<i32: 5>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc254)
          %m_ij_68 = arith.maxnumf %arg81, %m_ij_67 {async_task_id = array<i32: 5>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc293)
          %qk_69 = arith.mulf %qk_65, %qk_43 {async_task_id = array<i32: 5>} : tensor<128x128xf32, #blocked4> loc(#loc255)
          %qk_70 = tt.expand_dims %m_ij_68 {async_task_id = array<i32: 5>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<128x1xf32, #blocked4> loc(#loc294)
          %qk_71 = tt.broadcast %qk_70 {async_task_id = array<i32: 5>} : tensor<128x1xf32, #blocked4> -> tensor<128x128xf32, #blocked4> loc(#loc295)
          %qk_72 = arith.subf %qk_69, %qk_71 {async_task_id = array<i32: 5>} : tensor<128x128xf32, #blocked4> loc(#loc295)
          %103 = tt.reshape %qk_72 {async_task_id = array<i32: 5>} : tensor<128x128xf32, #blocked4> -> tensor<128x2x64xf32, #blocked5> loc(#loc296)
          %104 = tt.trans %103 {async_task_id = array<i32: 5>, order = array<i32: 0, 2, 1>} : tensor<128x2x64xf32, #blocked5> -> tensor<128x64x2xf32, #blocked6> loc(#loc297)
          %outLHS, %outRHS = tt.split %104 {async_task_id = array<i32: 5>} : tensor<128x64x2xf32, #blocked6> -> tensor<128x64xf32, #blocked> loc(#loc298)
          %105 = tt.reshape %outLHS {async_task_id = array<i32: 5>} : tensor<128x64xf32, #blocked> -> tensor<128x2x32xf32, #blocked7> loc(#loc299)
          %106 = tt.trans %105 {async_task_id = array<i32: 5>, order = array<i32: 0, 2, 1>} : tensor<128x2x32xf32, #blocked7> -> tensor<128x32x2xf32, #blocked8> loc(#loc300)
          %outLHS_73, %outRHS_74 = tt.split %106 {async_task_id = array<i32: 5>} : tensor<128x32x2xf32, #blocked8> -> tensor<128x32xf32, #blocked2> loc(#loc301)
          %107 = tt.reshape %outRHS {async_task_id = array<i32: 5>} : tensor<128x64xf32, #blocked> -> tensor<128x2x32xf32, #blocked7> loc(#loc302)
          %108 = tt.trans %107 {async_task_id = array<i32: 5>, order = array<i32: 0, 2, 1>} : tensor<128x2x32xf32, #blocked7> -> tensor<128x32x2xf32, #blocked8> loc(#loc303)
          %outLHS_75, %outRHS_76 = tt.split %108 {async_task_id = array<i32: 5>} : tensor<128x32x2xf32, #blocked8> -> tensor<128x32xf32, #blocked2> loc(#loc304)

          // --- barrier id 10 wait
          ttng.wait_barrier_named %c10_i32_p4, %c256_i32_p4 : i32, i32
          // --- barrier id 10 wait

          %p00 = math.exp2 %outLHS_73 {async_task_id = array<i32: 5>} : tensor<128x32xf32, #blocked2> loc(#loc305)
          %p00_bf16 = arith.truncf %p00 {async_task_id = array<i32: 5>} : tensor<128x32xf32, #blocked2> to tensor<128x32xbf16, #blocked2> loc(#loc306)
          %p01 = math.exp2 %outRHS_74 {async_task_id = array<i32: 5>} : tensor<128x32xf32, #blocked2> loc(#loc307)
          %p01_bf16 = arith.truncf %p01 {async_task_id = array<i32: 5>} : tensor<128x32xf32, #blocked2> to tensor<128x32xbf16, #blocked2> loc(#loc308)
          %p10 = math.exp2 %outLHS_75 {async_task_id = array<i32: 5>} : tensor<128x32xf32, #blocked2> loc(#loc309)
          %p10_bf16 = arith.truncf %p10 {async_task_id = array<i32: 5>} : tensor<128x32xf32, #blocked2> to tensor<128x32xbf16, #blocked2> loc(#loc310)
          %p11 = math.exp2 %outRHS_76 {async_task_id = array<i32: 5>} : tensor<128x32xf32, #blocked2> loc(#loc311)
          %p11_bf16 = arith.truncf %p11 {async_task_id = array<i32: 5>} : tensor<128x32xf32, #blocked2> to tensor<128x32xbf16, #blocked2> loc(#loc312)

          %p0 = tt.join %p00, %p01 {async_task_id = array<i32: 5>} : tensor<128x32xf32, #blocked2> -> tensor<128x32x2xf32, #blocked9> loc(#loc313)
          %p0_77 = tt.trans %p0 {async_task_id = array<i32: 5>, order = array<i32: 0, 2, 1>} : tensor<128x32x2xf32, #blocked9> -> tensor<128x2x32xf32, #blocked10> loc(#loc314)
          %p0_78 = tt.reshape %p0_77 {async_task_id = array<i32: 5>} : tensor<128x2x32xf32, #blocked10> -> tensor<128x64xf32, #linear2> loc(#loc315)
          %p1 = tt.join %p10, %p11 {async_task_id = array<i32: 5>} : tensor<128x32xf32, #blocked2> -> tensor<128x32x2xf32, #blocked9> loc(#loc316)
          %p1_79 = tt.trans %p1 {async_task_id = array<i32: 5>, order = array<i32: 0, 2, 1>} : tensor<128x32x2xf32, #blocked9> -> tensor<128x2x32xf32, #blocked10> loc(#loc317)
          %p1_80 = tt.reshape %p1_79 {async_task_id = array<i32: 5>} : tensor<128x2x32xf32, #blocked10> -> tensor<128x64xf32, #linear2> loc(#loc318)
          %p = tt.join %p0_78, %p1_80 {async_task_id = array<i32: 5>} : tensor<128x64xf32, #linear2> -> tensor<128x64x2xf32, #linear3> loc(#loc319)
          %p_81 = tt.trans %p {async_task_id = array<i32: 5>, order = array<i32: 0, 2, 1>} : tensor<128x64x2xf32, #linear3> -> tensor<128x2x64xf32, #linear4> loc(#loc320)
          %p_82 = tt.reshape %p_81 {async_task_id = array<i32: 5>} : tensor<128x2x64xf32, #linear4> -> tensor<128x128xf32, #linear1> loc(#loc321)

          %qk_83 = ttng.tmem_subslice %qk_21 {N = 0 : i32, async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc242)
          %qk_84 = ttg.memdesc_reinterpret %qk_83 {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc242)
          %acc_85 = ttg.memdesc_index %qk_84[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc243)

          %109 = ttg.memdesc_index %arg34[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %acc_86 = arith.xori %qk_62, %true_32 {async_task_id = array<i32: 5>} : i1 loc(#loc243)
          %acc_87 = arith.extsi %acc_86 {async_task_id = array<i32: 5>} : i1 to i32 loc(#loc243)
          ttng.wait_barrier %109, %acc_87 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc243)

          %acc_88 = ttng.tmem_subslice %acc_85 {N = 0 : i32} : !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xbf16, #tmem5, #ttng.tensor_memory, mutable, 128x128> loc(#loc243)
          %acc_89 = ttng.tmem_subslice %acc_88 {N = 0 : i32} : !ttg.memdesc<128x64xbf16, #tmem5, #ttng.tensor_memory, mutable, 128x128> -> !ttg.memdesc<128x32xbf16, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc243)
          ttng.tmem_store %p00_bf16, %acc_89, %true_32 : tensor<128x32xbf16, #blocked2> -> !ttg.memdesc<128x32xbf16, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc243)
          %acc_90 = ttng.tmem_subslice %acc_88 {N = 32 : i32} : !ttg.memdesc<128x64xbf16, #tmem5, #ttng.tensor_memory, mutable, 128x128> -> !ttg.memdesc<128x32xbf16, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc243)
          ttng.tmem_store %p01_bf16, %acc_90, %true_32 : tensor<128x32xbf16, #blocked2> -> !ttg.memdesc<128x32xbf16, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc243)
          %acc_91 = ttng.tmem_subslice %acc_85 {N = 64 : i32} : !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xbf16, #tmem5, #ttng.tensor_memory, mutable, 128x128> loc(#loc243)
          %acc_92 = ttng.tmem_subslice %acc_91 {N = 0 : i32} : !ttg.memdesc<128x64xbf16, #tmem5, #ttng.tensor_memory, mutable, 128x128> -> !ttg.memdesc<128x32xbf16, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc243)
          ttng.tmem_store %p10_bf16, %acc_92, %true_32 : tensor<128x32xbf16, #blocked2> -> !ttg.memdesc<128x32xbf16, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc243)
          %acc_93 = ttng.tmem_subslice %acc_91 {N = 32 : i32} : !ttg.memdesc<128x64xbf16, #tmem5, #ttng.tensor_memory, mutable, 128x128> -> !ttg.memdesc<128x32xbf16, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc243)
          ttng.tmem_store %p11_bf16, %acc_93, %true_32 : tensor<128x32xbf16, #blocked2> -> !ttg.memdesc<128x32xbf16, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc243)

          %110 = ttg.memdesc_index %arg62[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.arrive_barrier %110, 1 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc243)

          // --- barrier id 9 arrive
          ttng.arrive_barrier_named %c9_i32_p4, %c256_i32_p4 : i32, i32
          // --- barrier id 9 arrive

          %alpha = arith.subf %arg81, %m_ij_68 {async_task_id = array<i32: 5>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc322)
          %alpha_94 = math.exp2 %alpha {async_task_id = array<i32: 5>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc246)
          %alpha_95 = ttg.convert_layout %alpha_94 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #linear1}>> loc(#loc246)
          %alpha_96 = ttg.convert_layout %alpha_94 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc246)
          %alpha_97 = tt.expand_dims %alpha_96 {async_task_id = array<i32: 5>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc246)
          %qk_98 = ttng.tmem_subslice %qk_21 {N = 64 : i32, async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc242)
          %qk_99 = ttg.memdesc_reinterpret %qk_98 {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc242)
          %alpha_100 = ttg.memdesc_index %qk_99[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc246)
          %111 = ttg.memdesc_index %arg61[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %alpha_101 = arith.xori %qk_62, %true_32 : i1 loc(#loc246)
          %alpha_102 = arith.extui %alpha_101 : i1 to i32 loc(#loc246)
          ttng.wait_barrier %111, %alpha_102 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc246)
          ttng.tmem_store %alpha_97, %alpha_100, %true_32 {async_task_id = array<i32: 5>} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc246)
          %112 = ttg.memdesc_index %arg60[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.arrive_barrier %112, 1 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc246)
          %l_ij = "tt.reduce"(%p_82) <{axis = 1 : i32}> ({
          ^bb0(%l_ij_105: f32 loc(callsite(#loc1 at #loc323)), %l_ij_106: f32 loc(callsite(#loc1 at #loc323))):
            %l_ij_107 = arith.addf %l_ij_105, %l_ij_106 {async_task_id = array<i32: 5>} : f32 loc(#loc343)
            tt.reduce.return %l_ij_107 {async_task_id = array<i32: 5>} : f32 loc(#loc337)
          }) {async_task_id = array<i32: 5>} : (tensor<128x128xf32, #linear1>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #linear1}>> loc(#loc337)
          %l_i0 = arith.mulf %arg80, %alpha_95 {async_task_id = array<i32: 5>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #linear1}>> loc(#loc324)
          %l_i0_103 = arith.addf %l_i0, %l_ij {async_task_id = array<i32: 5>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #linear1}>> loc(#loc325)
          %offsetkv_y_104 = arith.addi %arg82, %c1_i64_33 {async_task_id = array<i32: 5>} : i64 loc(#loc348)
          scf.yield %l_i0_103, %m_ij_68, %offsetkv_y_104 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #linear1}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>, i64 loc(#loc239)
        } {async_task_id = array<i32: 5>, tt.disallow_acc_multi_buffer, tt.warp_specialize} loc(#loc348)
        %offsetkv_y_45 = ttg.convert_layout %offsetkv_y#1 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc348)
        %offsetkv_y_46 = tt.expand_dims %offsetkv_y_45 {async_task_id = array<i32: 5>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc348)
        %qk_47 = ttng.tmem_subslice %qk_21 {N = 65 : i32, async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc242)
        %qk_48 = ttg.memdesc_reinterpret %qk_47 {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc242)
        %offsetkv_y_49 = ttg.memdesc_index %qk_48[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc348)
        %offsetkv_y_50 = arith.andi %arg77, %c1_i64_33 {async_task_id = array<i32: 5>} : i64 loc(#loc348)
        %offsetkv_y_51 = arith.trunci %offsetkv_y_50 {async_task_id = array<i32: 5>} : i64 to i1 loc(#loc348)
        %97 = ttg.memdesc_index %arg67[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %offsetkv_y_52 = arith.xori %offsetkv_y_51, %true_32 : i1 loc(#loc348)
        %offsetkv_y_53 = arith.extui %offsetkv_y_52 : i1 to i32 loc(#loc348)
        ttng.wait_barrier %97, %offsetkv_y_53 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc348)
        ttng.tmem_store %offsetkv_y_46, %offsetkv_y_49, %true_32 {async_task_id = array<i32: 5>} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc348)
        %98 = ttg.memdesc_index %arg66[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %98, 1 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc348)
        %offsetkv_y_54 = ttg.convert_layout %offsetkv_y#0 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #linear1}>> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc348)
        %offsetkv_y_55 = tt.expand_dims %offsetkv_y_54 {async_task_id = array<i32: 5>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc348)
        %qk_56 = ttng.tmem_subslice %qk_21 {N = 66 : i32, async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc242)
        %qk_57 = ttg.memdesc_reinterpret %qk_56 {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc242)
        %offsetkv_y_58 = ttg.memdesc_index %qk_57[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc348)
        %99 = ttg.memdesc_index %arg71[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %99, %offsetkv_y_53 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc348)
        ttng.tmem_store %offsetkv_y_55, %offsetkv_y_58, %true_32 {async_task_id = array<i32: 5>} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc348)
        %100 = ttg.memdesc_index %arg70[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %100, 1 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc348)
        %tile_idx_59 = arith.addi %arg77, %c1_i64_33 {async_task_id = array<i32: 5>} : i64 loc(#loc134)
        scf.yield %tile_idx_59, %offsetkv_y#2 : i64, i64 loc(#loc51)
      } {async_task_id = array<i32: 5>, tt.warp_specialize, ttg.partition.stages = [0 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], ttg.warp_specialize.tag = 0 : i32} loc(#loc134)
      ttg.warp_return {async_task_id = array<i32: 5>} loc(#loc)
    } : (i32, i32, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<3x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<3x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !tt.tensordesc<tensor<128x64xbf16, #shared>>, !tt.tensordesc<tensor<128x64xbf16, #shared>>, !tt.tensordesc<tensor<128x64xbf16, #shared>>, !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable>, !tt.tensordesc<tensor<128x64xbf16, #shared>>, !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable>, f32, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) -> () loc(#loc1)
    tt.return loc(#loc109)
  } loc(#loc)
} loc(#loc)
#loc13 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":651:28)
#loc14 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":652:32)
#loc15 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":653:31)
#loc16 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":653:35)
#loc17 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":655:34)
#loc18 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":656:31)
#loc19 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":656:17)
#loc20 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":656:7)
#loc21 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":657:24)
#loc22 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":460:47)
#loc23 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":461:58)
#loc24 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":687:39)
#loc25 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":688:25)
#loc26 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":689:29)
#loc27 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":458:39)
#loc28 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":460:34)
#loc29 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":461:34)
#loc30 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":197:58)
#loc31 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":111:25)
#loc32 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":122:42)
#loc33 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":379:8)
#loc34 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":122:36)
#loc35 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":123:36)
#loc36 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":555:25)
#loc37 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":555:12)
#loc38 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":558:22)
#loc39 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":557:27)
#loc40 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":557:18)
#loc41 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":557:35)
#loc42 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":556:18)
#loc43 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":556:23)
#loc44 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":561:25)
#loc45 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":561:12)
#loc46 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":564:22)
#loc47 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":563:35)
#loc48 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":562:18)
#loc49 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":562:23)
#loc50 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":713:20)
#loc51 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":713:8)
#loc52 = loc("/home/jieeliu/workspace/fb-triton/python/triton/language/standard.py":41:11)
#loc53 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":650:32)
#loc54 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":201:12)
#loc55 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":457:32)
#loc56 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":454:22)
#loc57 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":455:21)
#loc58 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":457:24)
#loc59 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":457:45)
#loc60 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":457:37)
#loc61 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":458:29)
#loc62 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":476:36)
#loc63 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":201:24)
#loc64 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":241:22)
#loc65 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":241:8)
#loc66 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":559:35)
#loc67 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":565:50)
#loc68 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":473:16)
#loc69 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":78:47)
#loc70 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":82:22)
#loc71 = loc("/home/jieeliu/workspace/fb-triton/python/triton/language/standard.py":189:40)
#loc73 = loc("/home/jieeliu/workspace/fb-triton/python/triton/language/standard.py":168:27)
#loc74 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":78:31)
#loc75 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":82:38)
#loc76 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":82:33)
#loc77 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":87:26)
#loc78 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":87:58)
#loc79 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":87:15)
#loc80 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":92:29)
#loc81 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":92:67)
#loc82 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":92:17)
#loc83 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":93:29)
#loc84 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":93:67)
#loc85 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":93:17)
#loc86 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":94:23)
#loc87 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":95:22)
#loc88 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":96:23)
#loc89 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":97:22)
#loc90 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":98:23)
#loc91 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":99:22)
#loc92 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":100:23)
#loc93 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":101:22)
#loc94 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":102:22)
#loc95 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":102:41)
#loc96 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":102:52)
#loc97 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":103:22)
#loc98 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":103:41)
#loc99 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":103:52)
#loc100 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":104:20)
#loc101 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":104:38)
#loc102 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":104:49)
#loc103 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":111:31)
#loc104 = loc("/home/jieeliu/workspace/fb-triton/python/triton/language/standard.py":291:36)
#loc106 = loc("/home/jieeliu/workspace/fb-triton/python/triton/language/standard.py":261:15)
#loc107 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":147:22)
#loc108 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":147:30)
#loc109 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":687:4)
#loc126 = loc("prog_id"(#loc13))
#loc127 = loc("num_progs"(#loc14))
#loc128 = loc("total_tiles"(#loc15))
#loc129 = loc("total_tiles"(#loc16))
#loc130 = loc("tiles_per_sm"(#loc17))
#loc131 = loc("tiles_per_sm"(#loc21))
#loc132 = loc("offs_m0"(#loc22))
#loc133 = loc("offs_m1"(#loc23))
#loc134 = loc("tile_idx"(#loc24))
#loc135 = loc("pid"(#loc25))
#loc136 = loc("off_hz"(#loc26))
#loc137 = loc("qo_offset_y"(#loc27))
#loc138 = loc("offs_m0"(#loc28))
#loc139 = loc("offs_m1"(#loc29))
#loc140 = loc("acc0"(#loc30))
#loc141 = loc("alpha"(#loc31))
#loc142 = loc("acc0"(#loc32))
#loc143 = loc("acc0"(#loc34))
#loc144 = loc("acc1"(#loc35))
#loc145 = loc("m_i0"(#loc36))
#loc146 = loc("m_i0"(#loc37))
#loc147 = loc(callsite(#loc38 at #loc3))
#loc148 = loc("m_ptrs0"(#loc39))
#loc149 = loc("m_ptrs0"(#loc40))
#loc150 = loc("m_ptrs0"(#loc41))
#loc151 = loc("acc0"(#loc42))
#loc152 = loc("acc0"(#loc43))
#loc153 = loc("m_i1"(#loc44))
#loc154 = loc("m_i1"(#loc45))
#loc155 = loc(callsite(#loc46 at #loc3))
#loc156 = loc("m_ptrs1"(#loc47))
#loc157 = loc("acc1"(#loc48))
#loc158 = loc("acc1"(#loc49))
#loc159 = loc("tile_idx"(#loc50))
#loc160 = loc("n_tile_num"(#loc53))
#loc161 = loc("k"(#loc54))
#loc162 = loc("offset_y"(#loc55))
#loc163 = loc("off_z"(#loc56))
#loc164 = loc("off_h"(#loc57))
#loc165 = loc("offset_y"(#loc58))
#loc166 = loc("offset_y"(#loc59))
#loc167 = loc("offset_y"(#loc60))
#loc168 = loc("qo_offset_y"(#loc61))
#loc169 = loc("q1"(#loc62))
#loc170 = loc("k"(#loc63))
#loc171 = loc("offsetkv_y"(#loc64))
#loc172 = loc(callsite(#loc66 at #loc3))
#loc173 = loc(callsite(#loc67 at #loc3))
#loc174 = loc("qk_scale"(#loc68))
#loc175 = loc("m_ij"(#loc69))
#loc176 = loc("qk"(#loc70))
#loc178 = loc("m_ij"(#loc74))
#loc179 = loc("qk"(#loc75))
#loc180 = loc("qk"(#loc76))
#loc181 = loc("p00"(#loc86))
#loc182 = loc("p00_bf16"(#loc87))
#loc183 = loc("p01"(#loc88))
#loc184 = loc("p01_bf16"(#loc89))
#loc185 = loc("p10"(#loc90))
#loc186 = loc("p10_bf16"(#loc91))
#loc187 = loc("p11"(#loc92))
#loc188 = loc("p11_bf16"(#loc93))
#loc189 = loc("p0"(#loc94))
#loc190 = loc("p0"(#loc95))
#loc191 = loc("p0"(#loc96))
#loc192 = loc("p1"(#loc97))
#loc193 = loc("p1"(#loc98))
#loc194 = loc("p1"(#loc99))
#loc195 = loc("p"(#loc100))
#loc196 = loc("p"(#loc101))
#loc197 = loc("p"(#loc102))
#loc198 = loc("alpha"(#loc103))
#loc200 = loc("l_i0"(#loc107))
#loc201 = loc("l_i0"(#loc108))
#loc207 = loc("tiles_per_sm"(#loc130))
#loc208 = loc("tiles_per_sm"(#loc131))
#loc209 = loc(callsite(#loc132 at #loc3))
#loc210 = loc(callsite(#loc133 at #loc3))
#loc211 = loc(callsite(#loc137 at #loc3))
#loc212 = loc(callsite(#loc138 at #loc3))
#loc213 = loc(callsite(#loc139 at #loc3))
#loc214 = loc("acc1"(#loc140))
#loc215 = loc(callsite(#loc145 at #loc3))
#loc216 = loc(callsite(#loc146 at #loc3))
#loc217 = loc(callsite(#loc148 at #loc3))
#loc218 = loc(callsite(#loc149 at #loc3))
#loc219 = loc(callsite(#loc150 at #loc3))
#loc220 = loc(callsite(#loc151 at #loc3))
#loc221 = loc(callsite(#loc152 at #loc3))
#loc222 = loc(callsite(#loc153 at #loc3))
#loc223 = loc(callsite(#loc154 at #loc3))
#loc224 = loc(callsite(#loc156 at #loc3))
#loc225 = loc(callsite(#loc157 at #loc3))
#loc226 = loc(callsite(#loc158 at #loc3))
#loc227 = loc(callsite(#loc52 at #loc160))
#loc228 = loc(callsite(#loc161 at #loc121))
#loc229 = loc(callsite(#loc162 at #loc3))
#loc230 = loc(callsite(#loc163 at #loc3))
#loc231 = loc(callsite(#loc164 at #loc3))
#loc232 = loc(callsite(#loc165 at #loc3))
#loc233 = loc(callsite(#loc166 at #loc3))
#loc234 = loc(callsite(#loc167 at #loc3))
#loc235 = loc(callsite(#loc168 at #loc3))
#loc236 = loc(callsite(#loc169 at #loc3))
#loc237 = loc(callsite(#loc170 at #loc121))
#loc238 = loc(callsite(#loc171 at #loc121))
#loc239 = loc(callsite(#loc65 at #loc121))
#loc240 = loc(callsite(#loc174 at #loc3))
#loc245 = loc("l_i0"(#loc214))
#loc246 = loc(callsite(#loc141 at #loc206))
#loc247 = loc(callsite(#loc142 at #loc206))
#loc248 = loc(callsite(#loc143 at #loc206))
#loc249 = loc(callsite(#loc144 at #loc206))
#loc250 = loc(callsite(#loc141 at #loc205))
#loc251 = loc(callsite(#loc142 at #loc205))
#loc252 = loc(callsite(#loc143 at #loc205))
#loc253 = loc(callsite(#loc144 at #loc205))
#loc254 = loc(callsite(#loc175 at #loc206))
#loc255 = loc(callsite(#loc176 at #loc206))
#loc257 = loc(callsite(#loc175 at #loc205))
#loc258 = loc(callsite(#loc178 at #loc205))
#loc259 = loc(callsite(#loc176 at #loc205))
#loc260 = loc(callsite(#loc179 at #loc205))
#loc261 = loc(callsite(#loc180 at #loc205))
#loc262 = loc(callsite(#loc77 at #loc205))
#loc263 = loc(callsite(#loc78 at #loc205))
#loc264 = loc(callsite(#loc79 at #loc205))
#loc265 = loc(callsite(#loc80 at #loc205))
#loc266 = loc(callsite(#loc81 at #loc205))
#loc267 = loc(callsite(#loc82 at #loc205))
#loc268 = loc(callsite(#loc83 at #loc205))
#loc269 = loc(callsite(#loc84 at #loc205))
#loc270 = loc(callsite(#loc85 at #loc205))
#loc271 = loc(callsite(#loc181 at #loc205))
#loc272 = loc(callsite(#loc182 at #loc205))
#loc273 = loc(callsite(#loc183 at #loc205))
#loc274 = loc(callsite(#loc184 at #loc205))
#loc275 = loc(callsite(#loc185 at #loc205))
#loc276 = loc(callsite(#loc186 at #loc205))
#loc277 = loc(callsite(#loc187 at #loc205))
#loc278 = loc(callsite(#loc188 at #loc205))
#loc279 = loc(callsite(#loc189 at #loc205))
#loc280 = loc(callsite(#loc190 at #loc205))
#loc281 = loc(callsite(#loc191 at #loc205))
#loc282 = loc(callsite(#loc192 at #loc205))
#loc283 = loc(callsite(#loc193 at #loc205))
#loc284 = loc(callsite(#loc194 at #loc205))
#loc285 = loc(callsite(#loc195 at #loc205))
#loc286 = loc(callsite(#loc196 at #loc205))
#loc287 = loc(callsite(#loc197 at #loc205))
#loc288 = loc(callsite(#loc198 at #loc205))
#loc290 = loc(callsite(#loc200 at #loc205))
#loc291 = loc(callsite(#loc201 at #loc205))
#loc293 = loc(callsite(#loc178 at #loc206))
#loc294 = loc(callsite(#loc179 at #loc206))
#loc295 = loc(callsite(#loc180 at #loc206))
#loc296 = loc(callsite(#loc77 at #loc206))
#loc297 = loc(callsite(#loc78 at #loc206))
#loc298 = loc(callsite(#loc79 at #loc206))
#loc299 = loc(callsite(#loc80 at #loc206))
#loc300 = loc(callsite(#loc81 at #loc206))
#loc301 = loc(callsite(#loc82 at #loc206))
#loc302 = loc(callsite(#loc83 at #loc206))
#loc303 = loc(callsite(#loc84 at #loc206))
#loc304 = loc(callsite(#loc85 at #loc206))
#loc305 = loc(callsite(#loc181 at #loc206))
#loc306 = loc(callsite(#loc182 at #loc206))
#loc307 = loc(callsite(#loc183 at #loc206))
#loc308 = loc(callsite(#loc184 at #loc206))
#loc309 = loc(callsite(#loc185 at #loc206))
#loc310 = loc(callsite(#loc186 at #loc206))
#loc311 = loc(callsite(#loc187 at #loc206))
#loc312 = loc(callsite(#loc188 at #loc206))
#loc313 = loc(callsite(#loc189 at #loc206))
#loc314 = loc(callsite(#loc190 at #loc206))
#loc315 = loc(callsite(#loc191 at #loc206))
#loc316 = loc(callsite(#loc192 at #loc206))
#loc317 = loc(callsite(#loc193 at #loc206))
#loc318 = loc(callsite(#loc194 at #loc206))
#loc319 = loc(callsite(#loc195 at #loc206))
#loc320 = loc(callsite(#loc196 at #loc206))
#loc321 = loc(callsite(#loc197 at #loc206))
#loc322 = loc(callsite(#loc198 at #loc206))
#loc324 = loc(callsite(#loc200 at #loc206))
#loc325 = loc(callsite(#loc201 at #loc206))
#loc326 = loc("l_i0_1"(#loc245))
#loc327 = loc(callsite(#loc33 at #loc248))
#loc328 = loc(callsite(#loc33 at #loc249))
#loc329 = loc(callsite(#loc33 at #loc252))
#loc330 = loc(callsite(#loc33 at #loc253))
#loc331 = loc(callsite(#loc71 at #loc256))
#loc333 = loc(callsite(#loc104 at #loc289))
#loc335 = loc(callsite(#loc71 at #loc292))
#loc337 = loc(callsite(#loc104 at #loc323))
#loc339 = loc("l_i1"(#loc326))
#loc340 = loc(callsite(#loc73 at #loc331))
#loc341 = loc(callsite(#loc106 at #loc333))
#loc342 = loc(callsite(#loc73 at #loc335))
#loc343 = loc(callsite(#loc106 at #loc337))
#loc344 = loc("l_i1_1"(#loc339))
#loc345 = loc("m_i0"(#loc344))
#loc346 = loc("m_i1"(#loc345))
#loc347 = loc("offsetkv_y"(#loc346))
#loc348 = loc(callsite(#loc347 at #loc121))
