#blocked = #ttg.blocked<{sizePerThread = [1, 64], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 32], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 128], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#linear = #ttg.linear<{register = [], lane = [[1], [2], [4], [8], [16]], warp = [[32], [64]], block = []}>
#loc = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0)
#loc1 = loc(unknown)
#loc2 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":540:58)
#loc3 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":686:12)
#loc4 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":534:43)
#loc5 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":177:24)
#loc6 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":489:12)
#loc7 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":451:21)
#loc8 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":450:21)
#loc9 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":71:19)
#loc10 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":213:12)
#loc11 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":195:12)
#loc12 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":120:23)
#loc72 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":78:42)
#loc80 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":87:25)
#shared = #ttg.nvmma_shared<{swizzlingByteWidth = 128, transposed = false, elementBitWidth = 16}>
#shared1 = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0]}>
#shared2 = #ttg.nvmma_shared<{swizzlingByteWidth = 128, transposed = true, elementBitWidth = 16}>
#smem = #ttg.shared_memory
#tmem = #ttng.tensor_memory_encoding<blockM = 128, blockN = 128, unpacked = true>
#tmem1 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 64, unpacked = true>
#tmem2 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 32, unpacked = true>
#tmem3 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 1, unpacked = true>
#tmem4 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 128, unpacked = false>
#loc86 = loc("sm_scale"(#loc))
#loc87 = loc("M"(#loc))
#loc88 = loc("Z"(#loc))
#loc89 = loc("H"(#loc))
#loc90 = loc("desc_q"(#loc))
#loc91 = loc("desc_k"(#loc))
#loc92 = loc("desc_v"(#loc))
#loc93 = loc("desc_o"(#loc))
#loc94 = loc(callsite(#loc2 at #loc3))
#loc95 = loc(callsite(#loc4 at #loc3))
#loc96 = loc("v"(#loc5))
#loc97 = loc(callsite(#loc6 at #loc3))
#loc98 = loc("q1"(#loc7))
#loc99 = loc("q0"(#loc8))
#loc100 = loc("qk"(#loc9))
#loc101 = loc("acc"(#loc12))
#loc153 = loc("m_ij"(#loc72))
#loc159 = loc("l_ij"(#loc80))
#loc163 = loc(callsite(#loc96 at #loc97))
#loc164 = loc(callsite(#loc98 at #loc3))
#loc165 = loc(callsite(#loc99 at #loc3))
#loc166 = loc(callsite(#loc10 at #loc97))
#loc167 = loc(callsite(#loc11 at #loc97))
#loc202 = loc(callsite(#loc100 at #loc166))
#loc203 = loc(callsite(#loc100 at #loc167))
#loc204 = loc(callsite(#loc101 at #loc167))
#loc205 = loc(callsite(#loc101 at #loc166))
#loc217 = loc(callsite(#loc153 at #loc166))
#loc225 = loc(callsite(#loc159 at #loc166))
#loc229 = loc(callsite(#loc153 at #loc167))
#loc235 = loc(callsite(#loc159 at #loc167))
#loc245 = loc(callsite(#loc1 at #loc217))
#loc247 = loc(callsite(#loc1 at #loc225))
#loc249 = loc(callsite(#loc1 at #loc229))
#loc251 = loc(callsite(#loc1 at #loc235))
module attributes {ttg.max_reg_auto_ws = 192 : i32, ttg.min_reg_auto_ws = 24 : i32, "ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "cuda:100", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_attn_fwd_persist(%sm_scale: f32 loc("sm_scale"(#loc)), %M: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("M"(#loc)), %Z: i32 loc("Z"(#loc)), %H: i32 {tt.divisibility = 16 : i32} loc("H"(#loc)), %desc_q: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_q"(#loc)), %desc_q_0: i32 loc("desc_q"(#loc)), %desc_q_1: i32 loc("desc_q"(#loc)), %desc_q_2: i64 loc("desc_q"(#loc)), %desc_q_3: i64 loc("desc_q"(#loc)), %desc_k: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_k"(#loc)), %desc_k_4: i32 loc("desc_k"(#loc)), %desc_k_5: i32 loc("desc_k"(#loc)), %desc_k_6: i64 loc("desc_k"(#loc)), %desc_k_7: i64 loc("desc_k"(#loc)), %desc_v: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_v"(#loc)), %desc_v_8: i32 loc("desc_v"(#loc)), %desc_v_9: i32 loc("desc_v"(#loc)), %desc_v_10: i64 loc("desc_v"(#loc)), %desc_v_11: i64 loc("desc_v"(#loc)), %desc_o: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_o"(#loc)), %desc_o_12: i32 loc("desc_o"(#loc)), %desc_o_13: i32 loc("desc_o"(#loc)), %desc_o_14: i64 loc("desc_o"(#loc)), %desc_o_15: i64 loc("desc_o"(#loc))) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc1)
    %c1_i32 = arith.constant 1 : i32 loc(#loc1)
    %c2_i32 = arith.constant 2 : i32 loc(#loc1)
    %true = arith.constant true loc(#loc1)
    %c32_i32 = arith.constant 32 : i32 loc(#loc1)
    %c8192_i32 = arith.constant 8192 : i32 loc(#loc1)
    %c256_i32 = arith.constant 256 : i32 loc(#loc1)
    %c128_i32 = arith.constant 128 : i32 loc(#loc1)
    %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #blocked> loc(#loc1)
    %c0_i64 = arith.constant 0 : i64 loc(#loc1)
    %c1_i64 = arith.constant 1 : i64 loc(#loc1)
    %c8064_i32 = arith.constant 8064 : i32 loc(#loc1)
    %0 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %1 = ttg.memdesc_index %0[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %1, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %2 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %3 = ttg.memdesc_index %2[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %3, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %4 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %5 = ttg.memdesc_index %4[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %5, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %6 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %7 = ttg.memdesc_index %6[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %7, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %8 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %9 = ttg.memdesc_index %8[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %9, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %10 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %11 = ttg.memdesc_index %10[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %11, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %12 = ttg.local_alloc : () -> !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc(#loc)
    %13 = ttg.memdesc_index %12[%c0_i32] : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %13, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %14 = ttg.memdesc_index %12[%c1_i32] : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %14, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %15 = ttg.memdesc_index %12[%c2_i32] : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %15, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %16 = ttg.local_alloc : () -> !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc(#loc)
    %17 = ttg.memdesc_index %16[%c0_i32] : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %17, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %18 = ttg.memdesc_index %16[%c1_i32] : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %18, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %19 = ttg.memdesc_index %16[%c2_i32] : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %19, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %20 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %21 = ttg.memdesc_index %20[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %21, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %22 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %23 = ttg.memdesc_index %22[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %23, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %24 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %25 = ttg.memdesc_index %24[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %25, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %26 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %27 = ttg.memdesc_index %26[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %27, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %28 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %29 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %30 = ttg.memdesc_index %28[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %30, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %31 = ttg.memdesc_index %29[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %31, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %32 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %33 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %34 = ttg.memdesc_index %32[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %34, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %35 = ttg.memdesc_index %33[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %35, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %36 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %37 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %38 = ttg.memdesc_index %36[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %38, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %39 = ttg.memdesc_index %37[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %39, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %40 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %41 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %42 = ttg.memdesc_index %40[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %42, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %43 = ttg.memdesc_index %41[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %43, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %44 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %45 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %46 = ttg.memdesc_index %44[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %46, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %47 = ttg.memdesc_index %45[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %47, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %48 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %49 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %50 = ttg.memdesc_index %48[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %50, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %51 = ttg.memdesc_index %49[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %51, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %52 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %53 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %54 = ttg.memdesc_index %52[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %54, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %55 = ttg.memdesc_index %53[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %55, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %56 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %57 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %58 = ttg.memdesc_index %56[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %58, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %59 = ttg.memdesc_index %57[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %59, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %60 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %61 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %62 = ttg.memdesc_index %60[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %62, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %63 = ttg.memdesc_index %61[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %63, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %64 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %65 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %66 = ttg.memdesc_index %64[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %66, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %67 = ttg.memdesc_index %65[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %67, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %68 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %69 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %70 = ttg.memdesc_index %68[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %70, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %71 = ttg.memdesc_index %69[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %71, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %72 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %73 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %74 = ttg.memdesc_index %72[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %74, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %75 = ttg.memdesc_index %73[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %75, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %76 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %77 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %78 = ttg.memdesc_index %76[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %78, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %79 = ttg.memdesc_index %77[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %79, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %80 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %81 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %82 = ttg.memdesc_index %80[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %82, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %83 = ttg.memdesc_index %81[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %83, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %84 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %85 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %86 = ttg.memdesc_index %84[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %86, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %87 = ttg.memdesc_index %85[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %87, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %88 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %89 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc)
    %90 = ttg.memdesc_index %88[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %90, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    %91 = ttg.memdesc_index %89[%c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    ttng.init_barrier %91, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %92 = ttg.local_alloc {buffer.copy = 1 : i32, buffer.id = 0 : i32} : () -> !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(#loc94)
    %93 = ttg.local_alloc {buffer.copy = 1 : i32, buffer.id = 1 : i32} : () -> !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(#loc95)
    %v = ttg.local_alloc {allocation.shareGroup = 1 : i32, buffer.copy = 3 : i32, buffer.id = 2 : i32} : () -> !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> loc(#loc163)
    %q1 = ttg.local_alloc {buffer.copy = 1 : i32, buffer.id = 3 : i32} : () -> !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(#loc164)
    %q0 = ttg.local_alloc {buffer.copy = 1 : i32, buffer.id = 4 : i32} : () -> !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(#loc165)
    %qk = ttng.tmem_alloc {allocation.shareGroup = 4 : i32, buffer.copy = 1 : i32, buffer.id = 5 : i32} : () -> !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc202)
    %qk_16 = ttng.tmem_alloc {allocation.shareGroup = 2 : i32, buffer.copy = 1 : i32, buffer.id = 6 : i32} : () -> !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc203)
    %acc = ttng.tmem_alloc {allocation.shareGroup = 0 : i32, buffer.copy = 1 : i32, buffer.id = 7 : i32} : () -> !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc204)
    %acc_17 = ttng.tmem_alloc {allocation.shareGroup = 3 : i32, buffer.copy = 1 : i32, buffer.id = 8 : i32} : () -> !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc205)
    ttg.warp_specialize(%Z, %H, %4, %2, %v, %16, %qk_16, %q0, %20, %acc, %22, %10, %qk, %q1, %12, %24, %acc_17, %26, %8, %6, %0, %desc_q, %desc_k, %desc_v, %93, %desc_o, %92, %sm_scale, %28, %29, %32, %33, %36, %40, %41, %45, %48, %52, %53, %57, %60, %61, %64, %65, %68, %69, %72, %73, %76, %81, %84, %89) attributes {requestedRegisters = array<i32: 24, 24, 24, 192, 192>}
    default {
      %prog_id = tt.get_program_id x {async_task_id = array<i32: 0>} : i32 loc(#loc102)
      %num_progs = tt.get_num_programs x {async_task_id = array<i32: 0>} : i32 loc(#loc103)
      %total_tiles = arith.muli %Z, %c32_i32 {async_task_id = array<i32: 0>} : i32 loc(#loc104)
      %total_tiles_18 = arith.muli %total_tiles, %H {async_task_id = array<i32: 0>} : i32 loc(#loc105)
      %tiles_per_sm = arith.divsi %total_tiles_18, %num_progs {async_task_id = array<i32: 0>} : i32 loc(#loc168)
      %94 = arith.remsi %total_tiles_18, %num_progs {async_task_id = array<i32: 0>} : i32 loc(#loc18)
      %95 = arith.cmpi slt, %prog_id, %94 {async_task_id = array<i32: 0>} : i32 loc(#loc19)
      %96 = scf.if %95 -> (i32) {
        %tiles_per_sm_19 = arith.addi %tiles_per_sm, %c1_i32 {async_task_id = array<i32: 0>} : i32 loc(#loc169)
        scf.yield {async_task_id = array<i32: 0>} %tiles_per_sm_19 : i32 loc(#loc169)
      } else {
        scf.yield {async_task_id = array<i32: 0>} %tiles_per_sm : i32 loc(#loc1)
      } {async_task_id = array<i32: 0>} loc(#loc20)
      %offs_m0 = tt.make_range {async_task_id = array<i32: 0>, end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked1> loc(#loc170)
      %offs_m1 = tt.make_range {async_task_id = array<i32: 0>, end = 256 : i32, start = 128 : i32} : tensor<128xi32, #blocked1> loc(#loc171)
      %tile_idx:3 = scf.for %tile_idx_19 = %c0_i32 to %96 step %c1_i32 iter_args(%prog_id_20 = %prog_id, %arg26 = %c0_i64, %arg27 = %c0_i64) -> (i32, i64, i64)  : i32 {
        %pid = arith.remsi %prog_id_20, %c32_i32 {async_task_id = array<i32: 0>} : i32 loc(#loc111)
        %off_hz = arith.divsi %prog_id_20, %c32_i32 {async_task_id = array<i32: 0>} : i32 loc(#loc112)
        %qo_offset_y = arith.muli %pid, %c256_i32 {async_task_id = array<i32: 0>} : i32 loc(#loc172)
        %offs_m0_21 = tt.splat %qo_offset_y {async_task_id = array<i32: 0>} : i32 -> tensor<128xi32, #blocked1> loc(#loc173)
        %offs_m0_22 = arith.addi %offs_m0_21, %offs_m0 {async_task_id = array<i32: 0>} : tensor<128xi32, #blocked1> loc(#loc173)
        %offs_m1_23 = arith.addi %offs_m0_21, %offs_m1 {async_task_id = array<i32: 0>} : tensor<128xi32, #blocked1> loc(#loc174)
        %acc_24 = ttg.memdesc_index %acc_17[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc205)
        ttng.tmem_store %cst, %acc_24, %true {async_task_id = array<i32: 0>} : tensor<128x64xf32, #blocked> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc205)
        %acc_25 = ttg.memdesc_index %acc[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc204)
        ttng.tmem_store %cst, %acc_25, %true {async_task_id = array<i32: 0>} : tensor<128x64xf32, #blocked> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc204)
        %offsetkv_y = arith.andi %arg26, %c1_i64 {async_task_id = array<i32: 0>} : i64 loc(#loc261)
        %offsetkv_y_26 = arith.trunci %offsetkv_y {async_task_id = array<i32: 0>} : i64 to i1 loc(#loc261)
        %alpha = arith.andi %arg27, %c1_i64 {async_task_id = array<i32: 0>} : i64 loc(#loc207)
        %alpha_27 = arith.trunci %alpha {async_task_id = array<i32: 0>} : i64 to i1 loc(#loc207)
        %97 = ttg.memdesc_index %10[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_28 = arith.xori %alpha_27, %true {async_task_id = array<i32: 0>} : i1 loc(#loc204)
        %acc_29 = arith.extsi %acc_28 {async_task_id = array<i32: 0>} : i1 to i32 loc(#loc204)
        ttng.wait_barrier %97, %acc_29, %true {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc204)
        %acc_30 = ttng.tmem_subslice %acc_25 {N = 0 : i32} : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc204)
        %acc_31 = ttng.tmem_subslice %acc_25 {N = 32 : i32} : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc204)
        %qk_32 = ttng.tmem_subslice %qk_16 {N = 64 : i32, async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc203)
        %qk_33 = ttg.memdesc_reinterpret %qk_32 {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc203)
        %alpha_34 = ttg.memdesc_index %qk_33[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc207)
        %98 = ttg.memdesc_index %52[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc0 = arith.extui %alpha_27 : i1 to i32 loc(#loc208)
        ttng.wait_barrier %98, %acc0, %true {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc208)
        %99 = ttg.memdesc_index %53[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_35 = ttng.tmem_load %acc_30 : !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> -> tensor<128x32xf32, #blocked2> loc(#loc204)
        %acc_36 = ttng.tmem_load %acc_31 : !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> -> tensor<128x32xf32, #blocked2> loc(#loc204)
        %acc0_37 = ttng.tmem_load %alpha_34 {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc208)
        ttng.arrive_barrier %99, 1, %true {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc208)
        %acc0_38 = tt.reshape %acc0_37 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc208)
        %acc0_39 = ttg.convert_layout %acc0_38 : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc240)
        %acc0_40 = tt.expand_dims %acc0_39 {async_task_id = array<i32: 0>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<128x1xf32, #blocked2> loc(#loc208)
        %acc0_41 = tt.broadcast %acc0_40 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc240)
        %acc0_42 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {async_task_id = array<i32: 0>, constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %acc_35, %acc0_41 : tensor<128x32xf32, #blocked2>, tensor<128x32xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc240)
        %acc1 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {async_task_id = array<i32: 0>, constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %acc_36, %acc0_41 : tensor<128x32xf32, #blocked2>, tensor<128x32xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc241)
        ttng.tmem_store %acc0_42, %acc_30, %true : tensor<128x32xf32, #blocked2> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc204)
        ttng.tmem_store %acc1, %acc_31, %true : tensor<128x32xf32, #blocked2> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc204)
        %100 = ttg.memdesc_index %76[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %100, 1, %true {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc204)
        %offsetkv_y_43 = scf.for %offsetkv_y_103 = %c0_i32 to %c8064_i32 step %c128_i32 iter_args(%arg29 = %arg27) -> (i64)  : i32 {
          %alpha_104 = arith.andi %arg29, %c1_i64 {async_task_id = array<i32: 0>} : i64 loc(#loc211)
          %alpha_105 = arith.trunci %alpha_104 {async_task_id = array<i32: 0>} : i64 to i1 loc(#loc211)
          %128 = ttg.memdesc_index %8[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %acc_106 = arith.xori %alpha_105, %true {async_task_id = array<i32: 0>} : i1 loc(#loc205)
          %acc_107 = arith.extsi %acc_106 {async_task_id = array<i32: 0>} : i1 to i32 loc(#loc205)
          ttng.wait_barrier %128, %acc_107 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc205)
          %acc_108 = ttng.tmem_subslice %acc_24 {N = 0 : i32} : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc205)
          %acc_109 = ttng.tmem_subslice %acc_24 {N = 32 : i32} : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc205)
          %qk_110 = ttng.tmem_subslice %qk {N = 64 : i32, async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc202)
          %qk_111 = ttg.memdesc_reinterpret %qk_110 {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc202)
          %alpha_112 = ttg.memdesc_index %qk_111[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc211)
          %129 = ttg.memdesc_index %40[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %acc0_113 = arith.extui %alpha_105 : i1 to i32 loc(#loc212)
          ttng.wait_barrier %129, %acc0_113 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc212)
          %130 = ttg.memdesc_index %41[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %acc_114 = ttng.tmem_load %acc_108 : !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> -> tensor<128x32xf32, #blocked2> loc(#loc205)
          %acc_115 = ttng.tmem_load %acc_109 : !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> -> tensor<128x32xf32, #blocked2> loc(#loc205)
          %acc0_116 = ttng.tmem_load %alpha_112 {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc212)
          ttng.arrive_barrier %130, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc212)
          %acc0_117 = tt.reshape %acc0_116 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc212)
          %acc0_118 = ttg.convert_layout %acc0_117 : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc242)
          %acc0_119 = tt.expand_dims %acc0_118 {async_task_id = array<i32: 0>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<128x1xf32, #blocked2> loc(#loc212)
          %acc0_120 = tt.broadcast %acc0_119 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc242)
          %acc0_121 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {async_task_id = array<i32: 0>, constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %acc_114, %acc0_120 : tensor<128x32xf32, #blocked2>, tensor<128x32xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc242)
          %acc1_122 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {async_task_id = array<i32: 0>, constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %acc_115, %acc0_120 : tensor<128x32xf32, #blocked2>, tensor<128x32xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc243)
          ttng.tmem_store %acc0_121, %acc_108, %true : tensor<128x32xf32, #blocked2> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc205)
          ttng.tmem_store %acc1_122, %acc_109, %true : tensor<128x32xf32, #blocked2> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc205)
          %131 = ttg.memdesc_index %84[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.arrive_barrier %131, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc205)
          %offsetkv_y_123 = arith.addi %arg29, %c1_i64 {async_task_id = array<i32: 0>} : i64 loc(#loc261)
          %alpha_124 = arith.andi %offsetkv_y_123, %c1_i64 {async_task_id = array<i32: 0>} : i64 loc(#loc207)
          %alpha_125 = arith.trunci %alpha_124 {async_task_id = array<i32: 0>} : i64 to i1 loc(#loc207)
          %acc_126 = arith.xori %alpha_125, %true {async_task_id = array<i32: 0>} : i1 loc(#loc204)
          %acc_127 = arith.extsi %acc_126 {async_task_id = array<i32: 0>} : i1 to i32 loc(#loc204)
          ttng.wait_barrier %97, %acc_127, %true {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc204)
          %acc0_128 = arith.extui %alpha_125 : i1 to i32 loc(#loc208)
          ttng.wait_barrier %98, %acc0_128, %true {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc208)
          %acc_129 = ttng.tmem_load %acc_30 : !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> -> tensor<128x32xf32, #blocked2> loc(#loc204)
          %acc_130 = ttng.tmem_load %acc_31 : !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> -> tensor<128x32xf32, #blocked2> loc(#loc204)
          %acc0_131 = ttng.tmem_load %alpha_34 {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc208)
          ttng.arrive_barrier %99, 1, %true {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc208)
          %acc0_132 = tt.reshape %acc0_131 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc208)
          %acc0_133 = ttg.convert_layout %acc0_132 : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc240)
          %acc0_134 = tt.expand_dims %acc0_133 {async_task_id = array<i32: 0>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<128x1xf32, #blocked2> loc(#loc208)
          %acc0_135 = tt.broadcast %acc0_134 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc240)
          %acc0_136 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {async_task_id = array<i32: 0>, constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %acc_129, %acc0_135 : tensor<128x32xf32, #blocked2>, tensor<128x32xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc240)
          %acc1_137 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {async_task_id = array<i32: 0>, constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %acc_130, %acc0_135 : tensor<128x32xf32, #blocked2>, tensor<128x32xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc241)
          ttng.tmem_store %acc0_136, %acc_30, %true : tensor<128x32xf32, #blocked2> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc204)
          ttng.tmem_store %acc1_137, %acc_31, %true : tensor<128x32xf32, #blocked2> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc204)
          ttng.arrive_barrier %100, 1, %true {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc204)
          scf.yield %offsetkv_y_123 : i64 loc(#loc261)
        } {async_task_id = array<i32: 0>, tt.disallow_acc_multi_buffer, tt.warp_specialize} loc(#loc261)
        %alpha_44 = arith.andi %offsetkv_y_43, %c1_i64 {async_task_id = array<i32: 0>} : i64 loc(#loc211)
        %alpha_45 = arith.trunci %alpha_44 {async_task_id = array<i32: 0>} : i64 to i1 loc(#loc211)
        %101 = ttg.memdesc_index %8[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_46 = arith.xori %alpha_45, %true {async_task_id = array<i32: 0>} : i1 loc(#loc205)
        %acc_47 = arith.extsi %acc_46 {async_task_id = array<i32: 0>} : i1 to i32 loc(#loc205)
        ttng.wait_barrier %101, %acc_47 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc205)
        %acc_48 = ttng.tmem_subslice %acc_24 {N = 0 : i32} : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc205)
        %acc_49 = ttng.tmem_subslice %acc_24 {N = 32 : i32} : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc205)
        %qk_50 = ttng.tmem_subslice %qk {N = 64 : i32, async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc202)
        %qk_51 = ttg.memdesc_reinterpret %qk_50 {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc202)
        %alpha_52 = ttg.memdesc_index %qk_51[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc211)
        %102 = ttg.memdesc_index %40[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc0_53 = arith.extui %alpha_45 : i1 to i32 loc(#loc212)
        ttng.wait_barrier %102, %acc0_53 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc212)
        %103 = ttg.memdesc_index %41[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_54 = ttng.tmem_load %acc_48 : !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> -> tensor<128x32xf32, #blocked2> loc(#loc205)
        %acc_55 = ttng.tmem_load %acc_49 : !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> -> tensor<128x32xf32, #blocked2> loc(#loc205)
        %acc0_56 = ttng.tmem_load %alpha_52 {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc212)
        ttng.arrive_barrier %103, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc212)
        %acc0_57 = tt.reshape %acc0_56 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc212)
        %acc0_58 = ttg.convert_layout %acc0_57 : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc242)
        %acc0_59 = tt.expand_dims %acc0_58 {async_task_id = array<i32: 0>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<128x1xf32, #blocked2> loc(#loc212)
        %acc0_60 = tt.broadcast %acc0_59 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc242)
        %acc0_61 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {async_task_id = array<i32: 0>, constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %acc_54, %acc0_60 : tensor<128x32xf32, #blocked2>, tensor<128x32xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc242)
        %acc1_62 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {async_task_id = array<i32: 0>, constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %acc_55, %acc0_60 : tensor<128x32xf32, #blocked2>, tensor<128x32xf32, #blocked2> -> tensor<128x32xf32, #blocked2> loc(#loc243)
        ttng.tmem_store %acc0_61, %acc_48, %true : tensor<128x32xf32, #blocked2> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc205)
        ttng.tmem_store %acc1_62, %acc_49, %true : tensor<128x32xf32, #blocked2> -> !ttg.memdesc<128x32xf32, #tmem2, #ttng.tensor_memory, mutable, 128x64> loc(#loc205)
        %104 = ttg.memdesc_index %84[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %104, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc205)
        %offsetkv_y_63 = arith.addi %offsetkv_y_43, %c1_i64 {async_task_id = array<i32: 0>} : i64 loc(#loc261)
        %qk_64 = ttng.tmem_subslice %qk_16 {N = 66 : i32, async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc203)
        %qk_65 = ttg.memdesc_reinterpret %qk_64 {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc203)
        %offsetkv_y_66 = ttg.memdesc_index %qk_65[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc261)
        %105 = ttg.memdesc_index %72[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %m_i0 = arith.extui %offsetkv_y_26 : i1 to i32 loc(#loc176)
        ttng.wait_barrier %105, %m_i0 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc176)
        %106 = ttg.memdesc_index %73[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %m_i0_67 = ttng.tmem_load %offsetkv_y_66 {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc176)
        ttng.arrive_barrier %106, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc176)
        %m_i0_68 = tt.reshape %m_i0_67 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc176)
        %m_i0_69 = math.log2 %m_i0_68 {async_task_id = array<i32: 0>} : tensor<128xf32, #linear> loc(#loc176)
        %qk_70 = ttng.tmem_subslice %qk_16 {N = 65 : i32, async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc203)
        %qk_71 = ttg.memdesc_reinterpret %qk_70 {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc203)
        %offsetkv_y_72 = ttg.memdesc_index %qk_71[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc261)
        %107 = ttg.memdesc_index %64[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %107, %m_i0 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc177)
        %108 = ttg.memdesc_index %65[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %m_i0_73 = ttng.tmem_load %offsetkv_y_72 {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc177)
        ttng.arrive_barrier %108, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc177)
        %m_i0_74 = tt.reshape %m_i0_73 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc177)
        %m_i0_75 = arith.addf %m_i0_74, %m_i0_69 {async_task_id = array<i32: 0>} : tensor<128xf32, #linear> loc(#loc177)
        %109 = ttg.convert_layout %m_i0_75 : tensor<128xf32, #linear> -> tensor<128xf32, #blocked1> loc(#loc123)
        %m_ptrs0 = arith.muli %off_hz, %c8192_i32 {async_task_id = array<i32: 0>} : i32 loc(#loc178)
        %m_ptrs0_76 = tt.addptr %M, %m_ptrs0 {async_task_id = array<i32: 0>} : !tt.ptr<f32>, i32 loc(#loc179)
        %m_ptrs0_77 = tt.splat %m_ptrs0_76 {async_task_id = array<i32: 0>} : !tt.ptr<f32> -> tensor<128x!tt.ptr<f32>, #blocked1> loc(#loc180)
        %m_ptrs0_78 = tt.addptr %m_ptrs0_77, %offs_m0_22 {async_task_id = array<i32: 0>} : tensor<128x!tt.ptr<f32>, #blocked1>, tensor<128xi32, #blocked1> loc(#loc180)
        tt.store %m_ptrs0_78, %109 {async_task_id = array<i32: 0>} : tensor<128x!tt.ptr<f32>, #blocked1> loc(#loc123)
        %acc0_79 = ttg.convert_layout %m_i0_68 : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc181)
        %acc0_80 = tt.expand_dims %acc0_79 {async_task_id = array<i32: 0>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc182)
        %acc0_81 = tt.broadcast %acc0_80 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked> -> tensor<128x64xf32, #blocked> loc(#loc181)
        %110 = ttg.memdesc_index %6[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_82 = arith.extsi %offsetkv_y_26 {async_task_id = array<i32: 0>} : i1 to i32 loc(#loc204)
        ttng.wait_barrier %110, %acc_82 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc204)
        %111 = ttg.memdesc_index %81[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_83 = ttng.tmem_load %acc_25 {async_task_id = array<i32: 0>, tmem.end = array<i32: 16, 16>} : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> tensor<128x64xf32, #blocked> loc(#loc204)
        ttng.arrive_barrier %111, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc204)
        %acc0_84 = arith.divf %acc_83, %acc0_81 {async_task_id = array<i32: 0>} : tensor<128x64xf32, #blocked> loc(#loc181)
        %112 = arith.truncf %acc0_84 {async_task_id = array<i32: 0>} : tensor<128x64xf32, #blocked> to tensor<128x64xbf16, #blocked> loc(#loc95)
        %113 = ttg.memdesc_index %93[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc95)
        %114 = ttg.memdesc_index %33[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %115 = arith.xori %offsetkv_y_26, %true : i1 loc(#loc95)
        %116 = arith.extui %115 : i1 to i32 loc(#loc95)
        ttng.wait_barrier %114, %116 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc95)
        ttg.local_store %112, %113 {async_task_id = array<i32: 0>} : tensor<128x64xbf16, #blocked> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc95)
        %117 = ttg.memdesc_index %32[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %117, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc95)
        %qk_85 = ttng.tmem_subslice %qk {N = 66 : i32, async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc202)
        %qk_86 = ttg.memdesc_reinterpret %qk_85 {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc202)
        %offsetkv_y_87 = ttg.memdesc_index %qk_86[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc261)
        %118 = ttg.memdesc_index %68[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %118, %m_i0 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc183)
        %119 = ttg.memdesc_index %69[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %m_i1 = ttng.tmem_load %offsetkv_y_87 {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc183)
        ttng.arrive_barrier %119, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc183)
        %m_i1_88 = tt.reshape %m_i1 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc183)
        %m_i1_89 = math.log2 %m_i1_88 {async_task_id = array<i32: 0>} : tensor<128xf32, #linear> loc(#loc183)
        %qk_90 = ttng.tmem_subslice %qk {N = 65 : i32, async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc202)
        %qk_91 = ttg.memdesc_reinterpret %qk_90 {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc202)
        %offsetkv_y_92 = ttg.memdesc_index %qk_91[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc261)
        %120 = ttg.memdesc_index %60[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %120, %m_i0 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc184)
        %121 = ttg.memdesc_index %61[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %m_i1_93 = ttng.tmem_load %offsetkv_y_92 {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc184)
        ttng.arrive_barrier %121, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc184)
        %m_i1_94 = tt.reshape %m_i1_93 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc184)
        %m_i1_95 = arith.addf %m_i1_94, %m_i1_89 {async_task_id = array<i32: 0>} : tensor<128xf32, #linear> loc(#loc184)
        %122 = ttg.convert_layout %m_i1_95 : tensor<128xf32, #linear> -> tensor<128xf32, #blocked1> loc(#loc131)
        %m_ptrs1 = tt.addptr %m_ptrs0_77, %offs_m1_23 {async_task_id = array<i32: 0>} : tensor<128x!tt.ptr<f32>, #blocked1>, tensor<128xi32, #blocked1> loc(#loc185)
        tt.store %m_ptrs1, %122 {async_task_id = array<i32: 0>} : tensor<128x!tt.ptr<f32>, #blocked1> loc(#loc131)
        %acc1_96 = ttg.convert_layout %m_i1_88 : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc186)
        %acc1_97 = tt.expand_dims %acc1_96 {async_task_id = array<i32: 0>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc187)
        %acc1_98 = tt.broadcast %acc1_97 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked> -> tensor<128x64xf32, #blocked> loc(#loc186)
        %123 = ttg.memdesc_index %89[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_99 = ttng.tmem_load %acc_24 {async_task_id = array<i32: 0>, tmem.end = array<i32: 19, 19>} : !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> tensor<128x64xf32, #blocked> loc(#loc205)
        ttng.arrive_barrier %123, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc205)
        %acc1_100 = arith.divf %acc_99, %acc1_98 {async_task_id = array<i32: 0>} : tensor<128x64xf32, #blocked> loc(#loc186)
        %124 = arith.truncf %acc1_100 {async_task_id = array<i32: 0>} : tensor<128x64xf32, #blocked> to tensor<128x64xbf16, #blocked> loc(#loc94)
        %125 = ttg.memdesc_index %92[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc94)
        %126 = ttg.memdesc_index %29[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %126, %116 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc94)
        ttg.local_store %124, %125 {async_task_id = array<i32: 0>} : tensor<128x64xbf16, #blocked> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc94)
        %127 = ttg.memdesc_index %28[%c0_i32] {async_task_id = array<i32: 0>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %127, 1 {async_task_id = array<i32: 0>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc94)
        %tile_idx_101 = arith.addi %prog_id_20, %num_progs {async_task_id = array<i32: 0>} : i32 loc(#loc135)
        %tile_idx_102 = arith.addi %arg26, %c1_i64 {async_task_id = array<i32: 0>} : i64 loc(#loc110)
        scf.yield %tile_idx_101, %tile_idx_102, %offsetkv_y_63 : i32, i64, i64 loc(#loc51)
      } {async_task_id = array<i32: 0>, tt.warp_specialize, ttg.partition.stages = [0 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], ttg.warp_specialize.tag = 0 : i32} loc(#loc110)
      ttg.warp_yield {async_task_id = array<i32: 0>} loc(#loc)
    }
    partition0(%Z_18: i32 loc("Z"(#loc)), %H_19: i32 loc("H"(#loc)), %arg26: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg27: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %v_20: !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc96 at #loc97)), %arg29: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %qk_21: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc100 at #loc167)), %q0_22: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc99 at #loc3)), %arg32: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %acc_23: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc101 at #loc167)), %arg34: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg35: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %qk_24: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc100 at #loc166)), %q1_25: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc98 at #loc3)), %arg38: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg39: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %acc_26: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc101 at #loc166)), %arg41: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg42: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg43: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg44: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %desc_q_27: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_q"(#loc)), %desc_k_28: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_k"(#loc)), %desc_v_29: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_v"(#loc)), %arg48: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc4 at #loc3)), %desc_o_30: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_o"(#loc)), %arg50: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc2 at #loc3)), %sm_scale_31: f32 loc("sm_scale"(#loc)), %arg52: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg53: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg54: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg55: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg56: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg57: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg58: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg59: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg60: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg61: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg62: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg63: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg64: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg65: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg66: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg67: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg68: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg69: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg70: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg71: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg72: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg73: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg74: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg75: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0)) num_warps(1) {
      %c8064_i32_32 = arith.constant 8064 : i32 loc(#loc1)
      %c3_i64 = arith.constant 3 : i64 loc(#loc1)
      %c2_i64 = arith.constant {async_task_id = array<i32: 1>} 2 : i64 loc(#loc1)
      %c1_i64_33 = arith.constant {async_task_id = array<i32: 1>} 1 : i64 loc(#loc1)
      %c0_i64_34 = arith.constant {async_task_id = array<i32: 1>} 0 : i64 loc(#loc1)
      %false = arith.constant {async_task_id = array<i32: 1>} false loc(#loc1)
      %true_35 = arith.constant {async_task_id = array<i32: 1>} true loc(#loc1)
      %n_tile_num = arith.constant {async_task_id = array<i32: 1>} 32 : i32 loc(#loc188)
      %c1_i32_36 = arith.constant {async_task_id = array<i32: 1>} 1 : i32 loc(#loc1)
      %c0_i32_37 = arith.constant {async_task_id = array<i32: 1>} 0 : i32 loc(#loc1)
      %c128_i32_38 = arith.constant {async_task_id = array<i32: 1>} 128 : i32 loc(#loc1)
      %prog_id = tt.get_program_id x {async_task_id = array<i32: 1>} : i32 loc(#loc102)
      %num_progs = tt.get_num_programs x {async_task_id = array<i32: 1>} : i32 loc(#loc103)
      %total_tiles = arith.muli %Z_18, %n_tile_num {async_task_id = array<i32: 1>} : i32 loc(#loc104)
      %total_tiles_39 = arith.muli %total_tiles, %H_19 {async_task_id = array<i32: 1>} : i32 loc(#loc105)
      %tiles_per_sm = arith.divsi %total_tiles_39, %num_progs {async_task_id = array<i32: 1>} : i32 loc(#loc168)
      %94 = arith.remsi %total_tiles_39, %num_progs {async_task_id = array<i32: 1>} : i32 loc(#loc18)
      %95 = arith.cmpi slt, %prog_id, %94 {async_task_id = array<i32: 1>} : i32 loc(#loc19)
      %96 = scf.if %95 -> (i32) {
        %tiles_per_sm_40 = arith.addi %tiles_per_sm, %c1_i32_36 {async_task_id = array<i32: 1>} : i32 loc(#loc169)
        scf.yield {async_task_id = array<i32: 1>} %tiles_per_sm_40 : i32 loc(#loc169)
      } else {
        scf.yield {async_task_id = array<i32: 1>} %tiles_per_sm : i32 loc(#loc1)
      } {async_task_id = array<i32: 1>} loc(#loc20)
      %tile_idx:3 = scf.for %tile_idx_40 = %c0_i32_37 to %96 step %c1_i32_36 iter_args(%tile_idx_41 = %c0_i64_34, %tile_idx_42 = %c0_i64_34, %tile_idx_43 = %c0_i64_34) -> (i64, i64, i64)  : i32 {
        %offsetkv_y = arith.andi %tile_idx_41, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc261)
        %offsetkv_y_44 = arith.trunci %offsetkv_y {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc261)
        %97 = ttg.memdesc_index %arg26[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %98 = arith.extsi %offsetkv_y_44 {async_task_id = array<i32: 1>} : i1 to i32 loc(#loc)
        ttng.wait_barrier %97, %98 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %99 = ttg.memdesc_index %arg27[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %99, %98 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %100 = ttg.memdesc_index %arg73[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_45 = arith.xori %offsetkv_y_44, %true_35 : i1 loc(#loc204)
        %acc_46 = arith.extui %acc_45 : i1 to i32 loc(#loc204)
        ttng.wait_barrier %100, %acc_46 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc204)
        %101 = ttg.memdesc_index %arg75[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %101, %acc_46 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc205)
        %k = arith.divui %tile_idx_43, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc189)
        %k_47 = arith.muli %k, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc189)
        %k_48 = arith.subi %tile_idx_43, %k_47 {async_task_id = array<i32: 1>} : i64 loc(#loc189)
        %k_49 = arith.trunci %k_48 {async_task_id = array<i32: 1>} : i64 to i32 loc(#loc189)
        %k_50 = arith.andi %k, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc189)
        %k_51 = arith.trunci %k_50 {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc189)
        %k_52 = ttg.memdesc_index %v_20[%k_49] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc189)
        %k_53 = ttg.memdesc_trans %k_52 {async_task_id = array<i32: 1>, order = array<i32: 1, 0>} : !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<64x128xbf16, #shared2, #smem, mutable> loc(#loc189)
        %102 = ttg.memdesc_index %arg29[%k_49] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %103 = arith.extsi %k_51 {async_task_id = array<i32: 1>} : i1 to i32 loc(#loc)
        ttng.wait_barrier %102, %103, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %qk_54 = ttg.memdesc_index %qk_21[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc203)
        %q0_55 = ttg.memdesc_index %q0_22[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc165)
        %qk_56 = arith.andi %tile_idx_42, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc203)
        %qk_57 = arith.trunci %qk_56 {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc203)
        %104 = ttg.memdesc_index %arg32[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %105 = ttg.memdesc_index %arg63[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %qk_58 = arith.xori %qk_57, %true_35 : i1 loc(#loc203)
        %qk_59 = arith.extui %qk_58 : i1 to i32 loc(#loc203)
        ttng.wait_barrier %105, %qk_59, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc203)
        ttng.tc_gen5_mma %q0_55, %k_53, %qk_54, %false, %true_35, %104[%true_35] {async_task_id = array<i32: 1>, is_async, tt.self_latency = 1 : i32} : !ttg.memdesc<128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<64x128xbf16, #shared2, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc203)
        %qk_60 = ttg.memdesc_index %qk_24[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc202)
        %q1_61 = ttg.memdesc_index %q1_25[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc164)
        %106 = ttg.memdesc_index %arg38[%k_49] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %107 = ttg.memdesc_index %arg39[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %108 = ttg.memdesc_index %arg59[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %108, %qk_59, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc202)
        ttng.tc_gen5_mma %q1_61, %k_53, %qk_60, %false, %true_35, %106[%true_35], %107[%true_35] {async_task_id = array<i32: 1>, is_async, tt.self_latency = 1 : i32} : !ttg.memdesc<128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<64x128xbf16, #shared2, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc202)
        %v_62 = arith.addi %tile_idx_43, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc163)
        %v_63 = arith.divui %v_62, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc163)
        %v_64 = arith.muli %v_63, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc163)
        %v_65 = arith.subi %v_62, %v_64 {async_task_id = array<i32: 1>} : i64 loc(#loc163)
        %v_66 = arith.trunci %v_65 {async_task_id = array<i32: 1>} : i64 to i32 loc(#loc163)
        %v_67 = arith.andi %v_63, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc163)
        %v_68 = arith.trunci %v_67 {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc163)
        %qk_69 = ttng.tmem_subslice %qk_21 {N = 0 : i32, async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc203)
        %qk_70 = ttg.memdesc_reinterpret %qk_69 {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc203)
        %acc_71 = ttg.memdesc_index %qk_70[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc204)
        %v_72 = ttg.memdesc_index %v_20[%v_66] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc163)
        %acc_73 = ttg.memdesc_index %acc_23[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc204)
        %109 = ttg.memdesc_index %arg29[%v_66] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %110 = arith.extsi %v_68 {async_task_id = array<i32: 1>} : i1 to i32 loc(#loc)
        ttng.wait_barrier %109, %110, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %111 = ttg.memdesc_index %arg34[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %112 = ttg.memdesc_index %arg60[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_74 = arith.extui %qk_57 : i1 to i32 loc(#loc204)
        ttng.wait_barrier %112, %acc_74, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc204)
        %113 = ttg.memdesc_index %arg35[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %114 = ttg.memdesc_index %arg72[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %114, %acc_74, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc204)
        ttng.tc_gen5_mma %acc_71, %v_72, %acc_73, %true_35, %true_35, %111[%true_35], %113[%true_35] {async_task_id = array<i32: 1>, is_async, tmem.end = array<i32: 15, 15>, tmem.start = array<i32: 14, 14, 16, 16>, tt.self_latency = 1 : i32} : !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc204)
        %offsetkv_y_75:3 = scf.for %offsetkv_y_92 = %c0_i32_37 to %c8064_i32_32 step %c128_i32_38 iter_args(%tile_idx_93 = %tile_idx_42, %tile_idx_94 = %tile_idx_43, %v_95 = %v_66) -> (i64, i64, i32)  : i32 {
          %offsetkv_y_96 = arith.addi %tile_idx_94, %c2_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc261)
          %offsetkv_y_97 = arith.addi %tile_idx_93, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc261)
          %k_98 = arith.divui %offsetkv_y_96, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc189)
          %k_99 = arith.muli %k_98, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc189)
          %k_100 = arith.subi %offsetkv_y_96, %k_99 {async_task_id = array<i32: 1>} : i64 loc(#loc189)
          %k_101 = arith.trunci %k_100 {async_task_id = array<i32: 1>} : i64 to i32 loc(#loc189)
          %k_102 = arith.andi %k_98, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc189)
          %k_103 = arith.trunci %k_102 {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc189)
          %k_104 = ttg.memdesc_index %v_20[%k_101] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc189)
          %k_105 = ttg.memdesc_trans %k_104 {async_task_id = array<i32: 1>, order = array<i32: 1, 0>} : !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<64x128xbf16, #shared2, #smem, mutable> loc(#loc189)
          %120 = ttg.memdesc_index %arg29[%k_101] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %121 = arith.extsi %k_103 {async_task_id = array<i32: 1>} : i1 to i32 loc(#loc)
          ttng.wait_barrier %120, %121, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %qk_106 = arith.andi %offsetkv_y_97, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc203)
          %qk_107 = arith.trunci %qk_106 {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc203)
          %qk_108 = arith.xori %qk_107, %true_35 : i1 loc(#loc203)
          %qk_109 = arith.extui %qk_108 : i1 to i32 loc(#loc203)
          ttng.wait_barrier %105, %qk_109, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc203)
          ttng.tc_gen5_mma %q0_55, %k_105, %qk_54, %false, %true_35, %104[%true_35] {async_task_id = array<i32: 1>, is_async, tt.self_latency = 1 : i32} : !ttg.memdesc<128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<64x128xbf16, #shared2, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc203)
          %acc_110 = arith.andi %tile_idx_93, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc205)
          %acc_111 = arith.trunci %acc_110 {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc205)
          %qk_112 = ttng.tmem_subslice %qk_24 {N = 0 : i32, async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc202)
          %qk_113 = ttg.memdesc_reinterpret %qk_112 {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc202)
          %acc_114 = ttg.memdesc_index %qk_113[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc205)
          %acc_115 = arith.addi %tile_idx_94, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc205)
          %acc_116 = arith.divui %acc_115, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc205)
          %acc_117 = arith.muli %acc_116, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc205)
          %acc_118 = arith.subi %acc_115, %acc_117 {async_task_id = array<i32: 1>} : i64 loc(#loc205)
          %acc_119 = arith.trunci %acc_118 {async_task_id = array<i32: 1>} : i64 to i32 loc(#loc205)
          %v_120 = ttg.memdesc_index %v_20[%acc_119] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc163)
          %acc_121 = ttg.memdesc_index %acc_26[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc205)
          %122 = ttg.memdesc_index %arg38[%v_95] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %123 = ttg.memdesc_index %arg41[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %124 = ttg.memdesc_index %arg56[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %acc_122 = arith.extui %acc_111 : i1 to i32 loc(#loc205)
          ttng.wait_barrier %124, %acc_122 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc205)
          %125 = ttg.memdesc_index %arg42[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %126 = ttg.memdesc_index %arg74[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.wait_barrier %126, %acc_122 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc205)
          ttng.tc_gen5_mma %acc_114, %v_120, %acc_121, %true_35, %true_35, %122[%true_35], %123[%true_35], %125[%true_35] {async_task_id = array<i32: 1>, is_async, tmem.end = array<i32: 18, 18>, tmem.start = array<i32: 17, 17, 19, 19>, tt.self_latency = 1 : i32} : !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc205)
          %127 = ttg.memdesc_index %arg38[%k_101] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.wait_barrier %108, %qk_109, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc202)
          ttng.tc_gen5_mma %q1_61, %k_105, %qk_60, %false, %true_35, %127[%true_35], %107[%true_35] {async_task_id = array<i32: 1>, is_async, tt.self_latency = 1 : i32} : !ttg.memdesc<128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<64x128xbf16, #shared2, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc202)
          %v_123 = arith.addi %tile_idx_94, %c3_i64 : i64 loc(#loc163)
          %v_124 = arith.divui %v_123, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc163)
          %v_125 = arith.muli %v_124, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc163)
          %v_126 = arith.subi %v_123, %v_125 {async_task_id = array<i32: 1>} : i64 loc(#loc163)
          %v_127 = arith.trunci %v_126 {async_task_id = array<i32: 1>} : i64 to i32 loc(#loc163)
          %v_128 = arith.andi %v_124, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc163)
          %v_129 = arith.trunci %v_128 {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc163)
          %v_130 = ttg.memdesc_index %v_20[%v_127] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc163)
          %128 = ttg.memdesc_index %arg29[%v_127] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %129 = arith.extsi %v_129 {async_task_id = array<i32: 1>} : i1 to i32 loc(#loc)
          ttng.wait_barrier %128, %129, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %acc_131 = arith.extui %qk_107 : i1 to i32 loc(#loc204)
          ttng.wait_barrier %112, %acc_131, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc204)
          ttng.wait_barrier %114, %acc_131, %true_35 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc204)
          ttng.tc_gen5_mma %acc_71, %v_130, %acc_73, %true_35, %true_35, %111[%true_35], %113[%true_35] {async_task_id = array<i32: 1>, is_async, tmem.end = array<i32: 15, 15>, tmem.start = array<i32: 14, 14, 16, 16>, tt.self_latency = 1 : i32} : !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc204)
          scf.yield %offsetkv_y_97, %offsetkv_y_96, %v_127 : i64, i64, i32 loc(#loc261)
        } {async_task_id = array<i32: 1>, tt.disallow_acc_multi_buffer, tt.warp_specialize} loc(#loc261)
        %offsetkv_y_76 = arith.addi %offsetkv_y_75#1, %c2_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc261)
        %offsetkv_y_77 = arith.addi %offsetkv_y_75#0, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc261)
        %acc_78 = arith.andi %offsetkv_y_75#0, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc205)
        %acc_79 = arith.trunci %acc_78 {async_task_id = array<i32: 1>} : i64 to i1 loc(#loc205)
        %qk_80 = ttng.tmem_subslice %qk_24 {N = 0 : i32, async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc202)
        %qk_81 = ttg.memdesc_reinterpret %qk_80 {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc202)
        %acc_82 = ttg.memdesc_index %qk_81[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc205)
        %acc_83 = arith.addi %offsetkv_y_75#1, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc205)
        %acc_84 = arith.divui %acc_83, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc205)
        %acc_85 = arith.muli %acc_84, %c3_i64 {async_task_id = array<i32: 1>} : i64 loc(#loc205)
        %acc_86 = arith.subi %acc_83, %acc_85 {async_task_id = array<i32: 1>} : i64 loc(#loc205)
        %acc_87 = arith.trunci %acc_86 {async_task_id = array<i32: 1>} : i64 to i32 loc(#loc205)
        %v_88 = ttg.memdesc_index %v_20[%acc_87] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc163)
        %acc_89 = ttg.memdesc_index %acc_26[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc205)
        %115 = ttg.memdesc_index %arg38[%offsetkv_y_75#2] {async_task_id = array<i32: 1>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %116 = ttg.memdesc_index %arg41[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %117 = ttg.memdesc_index %arg56[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %acc_90 = arith.extui %acc_79 : i1 to i32 loc(#loc205)
        ttng.wait_barrier %117, %acc_90 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc205)
        %118 = ttg.memdesc_index %arg42[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %119 = ttg.memdesc_index %arg74[%c0_i32_37] {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %119, %acc_90 {async_task_id = array<i32: 1>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc205)
        ttng.tc_gen5_mma %acc_82, %v_88, %acc_89, %true_35, %true_35, %115[%true_35], %116[%true_35], %118[%true_35] {async_task_id = array<i32: 1>, is_async, tmem.end = array<i32: 18, 18>, tmem.start = array<i32: 17, 17, 19, 19>, tt.self_latency = 1 : i32} : !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x64xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc205)
        ttng.tc_gen5_commit %arg43 {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc205)
        ttng.tc_gen5_commit %arg44 {async_task_id = array<i32: 1>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc203)
        %tile_idx_91 = arith.addi %tile_idx_41, %c1_i64_33 {async_task_id = array<i32: 1>} : i64 loc(#loc110)
        scf.yield {async_task_id = array<i32: 1>} %tile_idx_91, %offsetkv_y_77, %offsetkv_y_76 : i64, i64, i64 loc(#loc51)
      } {async_task_id = array<i32: 1>, tt.warp_specialize, ttg.partition.stages = [0 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], ttg.warp_specialize.tag = 0 : i32} loc(#loc110)
      ttg.warp_return {async_task_id = array<i32: 1>} loc(#loc)
    }
    partition1(%Z_18: i32 loc("Z"(#loc)), %H_19: i32 loc("H"(#loc)), %arg26: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg27: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %v_20: !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc96 at #loc97)), %arg29: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %qk_21: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc100 at #loc167)), %q0_22: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc99 at #loc3)), %arg32: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %acc_23: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc101 at #loc167)), %arg34: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg35: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %qk_24: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc100 at #loc166)), %q1_25: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc98 at #loc3)), %arg38: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg39: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %acc_26: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc101 at #loc166)), %arg41: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg42: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg43: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg44: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %desc_q_27: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_q"(#loc)), %desc_k_28: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_k"(#loc)), %desc_v_29: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_v"(#loc)), %arg48: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc4 at #loc3)), %desc_o_30: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_o"(#loc)), %arg50: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc2 at #loc3)), %sm_scale_31: f32 loc("sm_scale"(#loc)), %arg52: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg53: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg54: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg55: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg56: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg57: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg58: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg59: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg60: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg61: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg62: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg63: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg64: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg65: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg66: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg67: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg68: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg69: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg70: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg71: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg72: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg73: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg74: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg75: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0)) num_warps(1) {
      %c3_i64 = arith.constant 3 : i64 loc(#loc1)
      %c2_i64 = arith.constant {async_task_id = array<i32: 2>} 2 : i64 loc(#loc1)
      %true_32 = arith.constant {async_task_id = array<i32: 2>} true loc(#loc1)
      %c1_i64_33 = arith.constant {async_task_id = array<i32: 2>} 1 : i64 loc(#loc1)
      %c0_i64_34 = arith.constant {async_task_id = array<i32: 2>} 0 : i64 loc(#loc1)
      %n_tile_num = arith.constant {async_task_id = array<i32: 2>} 32 : i32 loc(#loc188)
      %c1_i32_35 = arith.constant {async_task_id = array<i32: 2>} 1 : i32 loc(#loc1)
      %c8192_i32_36 = arith.constant {async_task_id = array<i32: 2>} 8192 : i32 loc(#loc1)
      %c0_i32_37 = arith.constant {async_task_id = array<i32: 2>} 0 : i32 loc(#loc1)
      %c256_i32_38 = arith.constant {async_task_id = array<i32: 2>} 256 : i32 loc(#loc1)
      %c128_i32_39 = arith.constant {async_task_id = array<i32: 2>} 128 : i32 loc(#loc1)
      %prog_id = tt.get_program_id x {async_task_id = array<i32: 2>} : i32 loc(#loc102)
      %num_progs = tt.get_num_programs x {async_task_id = array<i32: 2>} : i32 loc(#loc103)
      %total_tiles = arith.muli %Z_18, %n_tile_num {async_task_id = array<i32: 2>} : i32 loc(#loc104)
      %total_tiles_40 = arith.muli %total_tiles, %H_19 {async_task_id = array<i32: 2>} : i32 loc(#loc105)
      %tiles_per_sm = arith.divsi %total_tiles_40, %num_progs {async_task_id = array<i32: 2>} : i32 loc(#loc168)
      %94 = arith.remsi %total_tiles_40, %num_progs {async_task_id = array<i32: 2>} : i32 loc(#loc18)
      %95 = arith.cmpi slt, %prog_id, %94 {async_task_id = array<i32: 2>} : i32 loc(#loc19)
      %96 = scf.if %95 -> (i32) {
        %tiles_per_sm_41 = arith.addi %tiles_per_sm, %c1_i32_35 {async_task_id = array<i32: 2>} : i32 loc(#loc169)
        scf.yield {async_task_id = array<i32: 2>} %tiles_per_sm_41 : i32 loc(#loc169)
      } else {
        scf.yield {async_task_id = array<i32: 2>} %tiles_per_sm : i32 loc(#loc1)
      } {async_task_id = array<i32: 2>} loc(#loc20)
      %offset_y = arith.muli %H_19, %c8192_i32_36 {async_task_id = array<i32: 2>} : i32 loc(#loc190)
      %tile_idx:3 = scf.for %tile_idx_41 = %c0_i32_37 to %96 step %c1_i32_35 iter_args(%prog_id_42 = %prog_id, %arg78 = %c0_i64_34, %arg79 = %c0_i64_34) -> (i32, i64, i64)  : i32 {
        %pid = arith.remsi %prog_id_42, %n_tile_num {async_task_id = array<i32: 2>} : i32 loc(#loc111)
        %off_hz = arith.divsi %prog_id_42, %n_tile_num {async_task_id = array<i32: 2>} : i32 loc(#loc112)
        %off_z = arith.divsi %off_hz, %H_19 {async_task_id = array<i32: 2>} : i32 loc(#loc191)
        %off_h = arith.remsi %off_hz, %H_19 {async_task_id = array<i32: 2>} : i32 loc(#loc192)
        %offset_y_43 = arith.muli %off_z, %offset_y {async_task_id = array<i32: 2>} : i32 loc(#loc193)
        %offset_y_44 = arith.muli %off_h, %c8192_i32_36 {async_task_id = array<i32: 2>} : i32 loc(#loc194)
        %offset_y_45 = arith.addi %offset_y_43, %offset_y_44 {async_task_id = array<i32: 2>} : i32 loc(#loc195)
        %qo_offset_y = arith.muli %pid, %c256_i32_38 {async_task_id = array<i32: 2>} : i32 loc(#loc172)
        %qo_offset_y_46 = arith.addi %offset_y_45, %qo_offset_y {async_task_id = array<i32: 2>} : i32 loc(#loc196)
        %offsetkv_y = arith.andi %arg78, %c1_i64_33 {async_task_id = array<i32: 2>} : i64 loc(#loc261)
        %offsetkv_y_47 = arith.trunci %offsetkv_y {async_task_id = array<i32: 2>} : i64 to i1 loc(#loc261)
        %97 = ttg.memdesc_index %arg44[%c0_i32_37] {async_task_id = array<i32: 2>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %q0_48 = arith.xori %offsetkv_y_47, %true_32 {async_task_id = array<i32: 2>} : i1 loc(#loc165)
        %q0_49 = arith.extsi %q0_48 {async_task_id = array<i32: 2>} : i1 to i32 loc(#loc165)
        ttng.wait_barrier %97, %q0_49 {async_task_id = array<i32: 2>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc165)
        %98 = ttg.memdesc_index %arg27[%c0_i32_37] {async_task_id = array<i32: 2>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.barrier_expect %98, 16384 {async_task_id = array<i32: 2>}, %true_32 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %q0_50 = ttg.memdesc_index %q0_22[%c0_i32_37] {async_task_id = array<i32: 2>} : !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc165)
        ttng.async_tma_copy_global_to_local %desc_q_27[%qo_offset_y_46, %c0_i32_37] %q0_50, %98, %true_32 {async_task_id = array<i32: 2>} : !tt.tensordesc<tensor<128x64xbf16, #shared>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc)
        %q1_51 = arith.addi %qo_offset_y_46, %c128_i32_39 {async_task_id = array<i32: 2>} : i32 loc(#loc197)
        %99 = ttg.memdesc_index %arg26[%c0_i32_37] {async_task_id = array<i32: 2>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.barrier_expect %99, 16384 {async_task_id = array<i32: 2>}, %true_32 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %q1_52 = ttg.memdesc_index %q1_25[%c0_i32_37] {async_task_id = array<i32: 2>} : !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc164)
        ttng.async_tma_copy_global_to_local %desc_q_27[%q1_51, %c0_i32_37] %q1_52, %99, %true_32 {async_task_id = array<i32: 2>} : !tt.tensordesc<tensor<128x64xbf16, #shared>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc)
        %offsetkv_y_53:2 = scf.for %offsetkv_y_56 = %c0_i32_37 to %c8192_i32_36 step %c128_i32_39 iter_args(%offset_y_57 = %offset_y_45, %arg82 = %arg79) -> (i32, i64)  : i32 {
          %k = arith.divui %arg82, %c3_i64 {async_task_id = array<i32: 2>} : i64 loc(#loc189)
          %k_58 = arith.muli %k, %c3_i64 {async_task_id = array<i32: 2>} : i64 loc(#loc189)
          %k_59 = arith.subi %arg82, %k_58 {async_task_id = array<i32: 2>} : i64 loc(#loc189)
          %k_60 = arith.trunci %k_59 {async_task_id = array<i32: 2>} : i64 to i32 loc(#loc189)
          %k_61 = arith.andi %k, %c1_i64_33 {async_task_id = array<i32: 2>} : i64 loc(#loc189)
          %k_62 = arith.trunci %k_61 {async_task_id = array<i32: 2>} : i64 to i1 loc(#loc189)
          %100 = ttg.memdesc_index %arg38[%k_60] {async_task_id = array<i32: 2>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %k_63 = arith.xori %k_62, %true_32 {async_task_id = array<i32: 2>} : i1 loc(#loc198)
          %k_64 = arith.extsi %k_63 {async_task_id = array<i32: 2>} : i1 to i32 loc(#loc198)
          ttng.wait_barrier %100, %k_64 {async_task_id = array<i32: 2>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc198)
          %101 = ttg.memdesc_index %arg29[%k_60] {async_task_id = array<i32: 2>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.barrier_expect %101, 16384 {async_task_id = array<i32: 2>}, %true_32 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %k_65 = ttg.memdesc_index %v_20[%k_60] {async_task_id = array<i32: 2>} : !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc189)
          ttng.async_tma_copy_global_to_local %desc_k_28[%offset_y_57, %c0_i32_37] %k_65, %101, %true_32 {async_task_id = array<i32: 2>} : !tt.tensordesc<tensor<128x64xbf16, #shared>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc)
          %v_66 = arith.addi %arg82, %c1_i64_33 {async_task_id = array<i32: 2>} : i64 loc(#loc163)
          %v_67 = arith.divui %v_66, %c3_i64 {async_task_id = array<i32: 2>} : i64 loc(#loc163)
          %v_68 = arith.muli %v_67, %c3_i64 {async_task_id = array<i32: 2>} : i64 loc(#loc163)
          %v_69 = arith.subi %v_66, %v_68 {async_task_id = array<i32: 2>} : i64 loc(#loc163)
          %v_70 = arith.trunci %v_69 {async_task_id = array<i32: 2>} : i64 to i32 loc(#loc163)
          %v_71 = arith.andi %v_67, %c1_i64_33 {async_task_id = array<i32: 2>} : i64 loc(#loc163)
          %v_72 = arith.trunci %v_71 {async_task_id = array<i32: 2>} : i64 to i1 loc(#loc163)
          %102 = ttg.memdesc_index %arg38[%v_70] {async_task_id = array<i32: 2>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %v_73 = arith.xori %v_72, %true_32 {async_task_id = array<i32: 2>} : i1 loc(#loc163)
          %v_74 = arith.extsi %v_73 {async_task_id = array<i32: 2>} : i1 to i32 loc(#loc163)
          ttng.wait_barrier %102, %v_74 {async_task_id = array<i32: 2>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc163)
          %103 = ttg.memdesc_index %arg29[%v_70] {async_task_id = array<i32: 2>} : !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.barrier_expect %103, 16384 {async_task_id = array<i32: 2>}, %true_32 : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %v_75 = ttg.memdesc_index %v_20[%v_70] {async_task_id = array<i32: 2>} : !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc163)
          ttng.async_tma_copy_global_to_local %desc_v_29[%offset_y_57, %c0_i32_37] %v_75, %103, %true_32 {async_task_id = array<i32: 2>} : !tt.tensordesc<tensor<128x64xbf16, #shared>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc)
          %offsetkv_y_76 = arith.addi %arg82, %c2_i64 {async_task_id = array<i32: 2>} : i64 loc(#loc261)
          %offsetkv_y_77 = arith.addi %offset_y_57, %c128_i32_39 {async_task_id = array<i32: 2>} : i32 loc(#loc199)
          scf.yield %offsetkv_y_77, %offsetkv_y_76 : i32, i64 loc(#loc200)
        } {async_task_id = array<i32: 2>, tt.disallow_acc_multi_buffer, tt.warp_specialize} loc(#loc261)
        %tile_idx_54 = arith.addi %prog_id_42, %num_progs {async_task_id = array<i32: 2>} : i32 loc(#loc135)
        %tile_idx_55 = arith.addi %arg78, %c1_i64_33 {async_task_id = array<i32: 2>} : i64 loc(#loc110)
        scf.yield %tile_idx_54, %tile_idx_55, %offsetkv_y_53#1 : i32, i64, i64 loc(#loc51)
      } {async_task_id = array<i32: 2>, tt.warp_specialize, ttg.partition.stages = [0 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], ttg.warp_specialize.tag = 0 : i32} loc(#loc110)
      ttg.warp_return {async_task_id = array<i32: 2>} loc(#loc)
    }
    partition2(%Z_18: i32 loc("Z"(#loc)), %H_19: i32 loc("H"(#loc)), %arg26: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg27: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %v_20: !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc96 at #loc97)), %arg29: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %qk_21: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc100 at #loc167)), %q0_22: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc99 at #loc3)), %arg32: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %acc_23: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc101 at #loc167)), %arg34: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg35: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %qk_24: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc100 at #loc166)), %q1_25: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc98 at #loc3)), %arg38: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg39: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %acc_26: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc101 at #loc166)), %arg41: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg42: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg43: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg44: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %desc_q_27: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_q"(#loc)), %desc_k_28: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_k"(#loc)), %desc_v_29: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_v"(#loc)), %arg48: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc4 at #loc3)), %desc_o_30: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_o"(#loc)), %arg50: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc2 at #loc3)), %sm_scale_31: f32 loc("sm_scale"(#loc)), %arg52: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg53: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg54: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg55: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg56: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg57: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg58: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg59: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg60: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg61: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg62: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg63: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg64: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg65: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg66: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg67: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg68: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg69: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg70: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg71: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg72: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg73: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg74: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg75: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0)) num_warps(1) {
      %c1_i64_32 = arith.constant {async_task_id = array<i32: 3>} 1 : i64 loc(#loc1)
      %c0_i64_33 = arith.constant {async_task_id = array<i32: 3>} 0 : i64 loc(#loc1)
      %n_tile_num = arith.constant {async_task_id = array<i32: 3>} 32 : i32 loc(#loc188)
      %c1_i32_34 = arith.constant {async_task_id = array<i32: 3>} 1 : i32 loc(#loc1)
      %c8192_i32_35 = arith.constant {async_task_id = array<i32: 3>} 8192 : i32 loc(#loc1)
      %c0_i32_36 = arith.constant {async_task_id = array<i32: 3>} 0 : i32 loc(#loc1)
      %c256_i32_37 = arith.constant {async_task_id = array<i32: 3>} 256 : i32 loc(#loc1)
      %c128_i32_38 = arith.constant {async_task_id = array<i32: 3>} 128 : i32 loc(#loc1)
      %prog_id = tt.get_program_id x {async_task_id = array<i32: 3>} : i32 loc(#loc102)
      %num_progs = tt.get_num_programs x {async_task_id = array<i32: 3>} : i32 loc(#loc103)
      %total_tiles = arith.muli %Z_18, %n_tile_num {async_task_id = array<i32: 3>} : i32 loc(#loc104)
      %total_tiles_39 = arith.muli %total_tiles, %H_19 {async_task_id = array<i32: 3>} : i32 loc(#loc105)
      %tiles_per_sm = arith.divsi %total_tiles_39, %num_progs {async_task_id = array<i32: 3>} : i32 loc(#loc168)
      %94 = arith.remsi %total_tiles_39, %num_progs {async_task_id = array<i32: 3>} : i32 loc(#loc18)
      %95 = arith.cmpi slt, %prog_id, %94 {async_task_id = array<i32: 3>} : i32 loc(#loc19)
      %96 = scf.if %95 -> (i32) {
        %tiles_per_sm_40 = arith.addi %tiles_per_sm, %c1_i32_34 {async_task_id = array<i32: 3>} : i32 loc(#loc169)
        scf.yield {async_task_id = array<i32: 3>} %tiles_per_sm_40 : i32 loc(#loc169)
      } else {
        scf.yield {async_task_id = array<i32: 3>} %tiles_per_sm : i32 loc(#loc1)
      } {async_task_id = array<i32: 3>} loc(#loc20)
      %offset_y = arith.muli %H_19, %c8192_i32_35 {async_task_id = array<i32: 3>} : i32 loc(#loc190)
      %tile_idx:2 = scf.for %tile_idx_40 = %c0_i32_36 to %96 step %c1_i32_34 iter_args(%prog_id_41 = %prog_id, %tile_idx_42 = %c0_i64_33) -> (i32, i64)  : i32 {
        %pid = arith.remsi %prog_id_41, %n_tile_num {async_task_id = array<i32: 3>} : i32 loc(#loc111)
        %off_hz = arith.divsi %prog_id_41, %n_tile_num {async_task_id = array<i32: 3>} : i32 loc(#loc112)
        %off_z = arith.divsi %off_hz, %H_19 {async_task_id = array<i32: 3>} : i32 loc(#loc191)
        %off_h = arith.remsi %off_hz, %H_19 {async_task_id = array<i32: 3>} : i32 loc(#loc192)
        %offset_y_43 = arith.muli %off_z, %offset_y {async_task_id = array<i32: 3>} : i32 loc(#loc193)
        %offset_y_44 = arith.muli %off_h, %c8192_i32_35 {async_task_id = array<i32: 3>} : i32 loc(#loc194)
        %offset_y_45 = arith.addi %offset_y_43, %offset_y_44 {async_task_id = array<i32: 3>} : i32 loc(#loc195)
        %qo_offset_y = arith.muli %pid, %c256_i32_37 {async_task_id = array<i32: 3>} : i32 loc(#loc172)
        %qo_offset_y_46 = arith.addi %offset_y_45, %qo_offset_y {async_task_id = array<i32: 3>} : i32 loc(#loc196)
        %q1_47 = arith.addi %qo_offset_y_46, %c128_i32_38 {async_task_id = array<i32: 3>} : i32 loc(#loc197)
        %97 = arith.andi %tile_idx_42, %c1_i64_32 {async_task_id = array<i32: 3>} : i64 loc(#loc95)
        %98 = arith.trunci %97 {async_task_id = array<i32: 3>} : i64 to i1 loc(#loc95)
        %99 = ttg.memdesc_index %arg48[%c0_i32_36] {async_task_id = array<i32: 3>} : !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc95)
        %100 = ttg.memdesc_index %arg54[%c0_i32_36] {async_task_id = array<i32: 3>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %101 = arith.extui %98 : i1 to i32 loc(#loc95)
        ttng.wait_barrier %100, %101 {async_task_id = array<i32: 3>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc95)
        ttng.fence_async_shared {bCluster = false} loc(#loc148)
        ttng.async_tma_copy_local_to_global %desc_o_30[%qo_offset_y_46, %c0_i32_36] %99 : !tt.tensordesc<tensor<128x64xbf16, #shared>>, !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc148)
        ttng.async_tma_store_wait {pendings = 0 : i32} loc(#loc148)
        %102 = ttg.memdesc_index %arg55[%c0_i32_36] {async_task_id = array<i32: 3>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %102, 1 {async_task_id = array<i32: 3>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc148)
        %103 = ttg.memdesc_index %arg50[%c0_i32_36] {async_task_id = array<i32: 3>} : !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc94)
        %104 = ttg.memdesc_index %arg52[%c0_i32_36] {async_task_id = array<i32: 3>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %104, %101 {async_task_id = array<i32: 3>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc94)
        ttng.fence_async_shared {bCluster = false} loc(#loc149)
        ttng.async_tma_copy_local_to_global %desc_o_30[%q1_47, %c0_i32_36] %103 : !tt.tensordesc<tensor<128x64xbf16, #shared>>, !ttg.memdesc<128x64xbf16, #shared, #smem, mutable> loc(#loc149)
        ttng.async_tma_store_wait {pendings = 0 : i32} loc(#loc149)
        %105 = ttg.memdesc_index %arg53[%c0_i32_36] {async_task_id = array<i32: 3>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %105, 1 {async_task_id = array<i32: 3>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc149)
        %tile_idx_48 = arith.addi %prog_id_41, %num_progs {async_task_id = array<i32: 3>} : i32 loc(#loc135)
        %tile_idx_49 = arith.addi %tile_idx_42, %c1_i64_32 {async_task_id = array<i32: 3>} : i64 loc(#loc110)
        scf.yield {async_task_id = array<i32: 3>} %tile_idx_48, %tile_idx_49 : i32, i64 loc(#loc51)
      } {async_task_id = array<i32: 3>, tt.warp_specialize, ttg.partition.stages = [0 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], ttg.warp_specialize.tag = 0 : i32} loc(#loc110)
      ttg.warp_return {async_task_id = array<i32: 3>} loc(#loc)
    }
    partition3(%Z_18: i32 loc("Z"(#loc)), %H_19: i32 loc("H"(#loc)), %arg26: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg27: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %v_20: !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc96 at #loc97)), %arg29: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %qk_21: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc100 at #loc167)), %q0_22: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc99 at #loc3)), %arg32: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %acc_23: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc101 at #loc167)), %arg34: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg35: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %qk_24: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc100 at #loc166)), %q1_25: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc98 at #loc3)), %arg38: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg39: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %acc_26: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc101 at #loc166)), %arg41: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg42: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg43: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg44: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %desc_q_27: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_q"(#loc)), %desc_k_28: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_k"(#loc)), %desc_v_29: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_v"(#loc)), %arg48: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc4 at #loc3)), %desc_o_30: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_o"(#loc)), %arg50: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc2 at #loc3)), %sm_scale_31: f32 loc("sm_scale"(#loc)), %arg52: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg53: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg54: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg55: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg56: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg57: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg58: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg59: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg60: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg61: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg62: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg63: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg64: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg65: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg66: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg67: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg68: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg69: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg70: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg71: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg72: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg73: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg74: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg75: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0)) num_warps(4) {
      %true_32 = arith.constant {async_task_id = array<i32: 4>} true loc(#loc1)
      %c1_i64_33 = arith.constant {async_task_id = array<i32: 4>} 1 : i64 loc(#loc1)
      %c0_i64_34 = arith.constant {async_task_id = array<i32: 4>} 0 : i64 loc(#loc1)
      %n_tile_num = arith.constant {async_task_id = array<i32: 4>} 32 : i32 loc(#loc188)
      %c1_i32_35 = arith.constant {async_task_id = array<i32: 4>} 1 : i32 loc(#loc1)
      %c8192_i32_36 = arith.constant {async_task_id = array<i32: 4>} 8192 : i32 loc(#loc1)
      %c0_i32_37 = arith.constant {async_task_id = array<i32: 4>} 0 : i32 loc(#loc1)
      %cst_38 = arith.constant {async_task_id = array<i32: 4>} 1.44269502 : f32 loc(#loc1)
      %c128_i32_39 = arith.constant {async_task_id = array<i32: 4>} 128 : i32 loc(#loc1)
      %cst_40 = arith.constant {async_task_id = array<i32: 4>} dense<0xFF800000> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc1)
      %cst_41 = arith.constant {async_task_id = array<i32: 4>} dense<1.000000e+00> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc1)
      %prog_id = tt.get_program_id x {async_task_id = array<i32: 4>} : i32 loc(#loc102)
      %num_progs = tt.get_num_programs x {async_task_id = array<i32: 4>} : i32 loc(#loc103)
      %total_tiles = arith.muli %Z_18, %n_tile_num {async_task_id = array<i32: 4>} : i32 loc(#loc104)
      %total_tiles_42 = arith.muli %total_tiles, %H_19 {async_task_id = array<i32: 4>} : i32 loc(#loc105)
      %tiles_per_sm = arith.divsi %total_tiles_42, %num_progs {async_task_id = array<i32: 4>} : i32 loc(#loc168)
      %94 = arith.remsi %total_tiles_42, %num_progs {async_task_id = array<i32: 4>} : i32 loc(#loc18)
      %95 = arith.cmpi slt, %prog_id, %94 {async_task_id = array<i32: 4>} : i32 loc(#loc19)
      %96 = scf.if %95 -> (i32) {
        %tiles_per_sm_44 = arith.addi %tiles_per_sm, %c1_i32_35 {async_task_id = array<i32: 4>} : i32 loc(#loc169)
        scf.yield {async_task_id = array<i32: 4>} %tiles_per_sm_44 : i32 loc(#loc169)
      } else {
        scf.yield {async_task_id = array<i32: 4>} %tiles_per_sm : i32 loc(#loc1)
      } {async_task_id = array<i32: 4>} loc(#loc20)
      %qk_scale = arith.mulf %sm_scale_31, %cst_38 {async_task_id = array<i32: 4>} : f32 loc(#loc201)
      %m_ij = tt.splat %qk_scale {async_task_id = array<i32: 4>} : f32 -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc215)
      %qk_43 = tt.splat %qk_scale {async_task_id = array<i32: 4>} : f32 -> tensor<128x128xf32, #blocked4> loc(#loc216)
      %tile_idx:2 = scf.for %tile_idx_44 = %c0_i32_37 to %96 step %c1_i32_35 iter_args(%arg77 = %c0_i64_34, %arg78 = %c0_i64_34) -> (i64, i64)  : i32 {
        %offsetkv_y:3 = scf.for %offsetkv_y_60 = %c0_i32_37 to %c8192_i32_36 step %c128_i32_39 iter_args(%arg80 = %cst_41, %arg81 = %cst_40, %arg82 = %arg78) -> (tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>, i64)  : i32 {
          %qk_61 = arith.andi %arg82, %c1_i64_33 {async_task_id = array<i32: 4>} : i64 loc(#loc202)
          %qk_62 = arith.trunci %qk_61 {async_task_id = array<i32: 4>} : i64 to i1 loc(#loc202)
          %qk_63 = ttg.memdesc_index %qk_24[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc202)
          %101 = ttg.memdesc_index %arg39[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %qk_64 = arith.extsi %qk_62 {async_task_id = array<i32: 4>} : i1 to i32 loc(#loc202)
          ttng.wait_barrier %101, %qk_64 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc202)
          %102 = ttg.memdesc_index %arg59[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %qk_65 = ttng.tmem_load %qk_63 {async_task_id = array<i32: 4>} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> tensor<128x128xf32, #blocked4> loc(#loc202)
          ttng.arrive_barrier %102, 1 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc202)
          %m_ij_66 = "tt.reduce"(%qk_65) <{axis = 1 : i32}> ({
          ^bb0(%m_ij_89: f32 loc(callsite(#loc1 at #loc217)), %m_ij_90: f32 loc(callsite(#loc1 at #loc217))):
            %m_ij_91 = arith.maxnumf %m_ij_89, %m_ij_90 {async_task_id = array<i32: 4>} : f32 loc(#loc253)
            tt.reduce.return %m_ij_91 {async_task_id = array<i32: 4>} : f32 loc(#loc244)
          }) {async_task_id = array<i32: 4>} : (tensor<128x128xf32, #blocked4>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc244)
          %m_ij_67 = arith.mulf %m_ij_66, %m_ij {async_task_id = array<i32: 4>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc218)
          %m_ij_68 = arith.maxnumf %arg81, %m_ij_67 {async_task_id = array<i32: 4>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc219)
          %qk_69 = arith.mulf %qk_65, %qk_43 {async_task_id = array<i32: 4>} : tensor<128x128xf32, #blocked4> loc(#loc220)
          %qk_70 = tt.expand_dims %m_ij_68 {async_task_id = array<i32: 4>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<128x1xf32, #blocked4> loc(#loc221)
          %qk_71 = tt.broadcast %qk_70 {async_task_id = array<i32: 4>} : tensor<128x1xf32, #blocked4> -> tensor<128x128xf32, #blocked4> loc(#loc222)
          %qk_72 = arith.subf %qk_69, %qk_71 {async_task_id = array<i32: 4>} : tensor<128x128xf32, #blocked4> loc(#loc222)
          %p = math.exp2 %qk_72 {async_task_id = array<i32: 4>} : tensor<128x128xf32, #blocked4> loc(#loc223)
          %alpha = arith.subf %arg81, %m_ij_68 {async_task_id = array<i32: 4>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc224)
          %alpha_73 = math.exp2 %alpha {async_task_id = array<i32: 4>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc211)
          %alpha_74 = ttg.convert_layout %alpha_73 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc211)
          %alpha_75 = tt.expand_dims %alpha_74 {async_task_id = array<i32: 4>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc211)
          %qk_76 = ttng.tmem_subslice %qk_24 {N = 64 : i32, async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc202)
          %qk_77 = ttg.memdesc_reinterpret %qk_76 {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc202)
          %alpha_78 = ttg.memdesc_index %qk_77[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc211)
          %103 = ttg.memdesc_index %arg58[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %alpha_79 = arith.xori %qk_62, %true_32 : i1 loc(#loc211)
          %alpha_80 = arith.extui %alpha_79 : i1 to i32 loc(#loc211)
          ttng.wait_barrier %103, %alpha_80 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc211)
          ttng.tmem_store %alpha_75, %alpha_78, %true_32 {async_task_id = array<i32: 4>} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc211)
          %104 = ttg.memdesc_index %arg57[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.arrive_barrier %104, 1 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc211)
          %l_ij = "tt.reduce"(%p) <{axis = 1 : i32}> ({
          ^bb0(%l_ij_89: f32 loc(callsite(#loc1 at #loc225)), %l_ij_90: f32 loc(callsite(#loc1 at #loc225))):
            %l_ij_91 = arith.addf %l_ij_89, %l_ij_90 {async_task_id = array<i32: 4>} : f32 loc(#loc254)
            tt.reduce.return %l_ij_91 {async_task_id = array<i32: 4>} : f32 loc(#loc246)
          }) {async_task_id = array<i32: 4>} : (tensor<128x128xf32, #blocked4>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc246)
          %p_81 = arith.truncf %p {async_task_id = array<i32: 4>} : tensor<128x128xf32, #blocked4> to tensor<128x128xbf16, #blocked4> loc(#loc226)
          %qk_82 = ttng.tmem_subslice %qk_24 {N = 0 : i32, async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc202)
          %qk_83 = ttg.memdesc_reinterpret %qk_82 {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc202)
          %acc_84 = ttg.memdesc_index %qk_83[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc205)
          %105 = ttg.memdesc_index %arg41[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %acc_85 = arith.xori %qk_62, %true_32 {async_task_id = array<i32: 4>} : i1 loc(#loc205)
          %acc_86 = arith.extsi %acc_85 {async_task_id = array<i32: 4>} : i1 to i32 loc(#loc205)
          ttng.wait_barrier %105, %acc_86 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc205)
          ttng.tmem_store %p_81, %acc_84, %true_32 {async_task_id = array<i32: 4>} : tensor<128x128xbf16, #blocked4> -> !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc205)
          %106 = ttg.memdesc_index %arg56[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.arrive_barrier %106, 1 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc205)
          %l_i0 = arith.mulf %arg80, %alpha_73 {async_task_id = array<i32: 4>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc227)
          %l_i0_87 = arith.addf %l_i0, %l_ij {async_task_id = array<i32: 4>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc228)
          %offsetkv_y_88 = arith.addi %arg82, %c1_i64_33 {async_task_id = array<i32: 4>} : i64 loc(#loc261)
          scf.yield %l_i0_87, %m_ij_68, %offsetkv_y_88 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>, i64 loc(#loc200)
        } {async_task_id = array<i32: 4>, tt.disallow_acc_multi_buffer, tt.warp_specialize} loc(#loc261)
        %offsetkv_y_45 = ttg.convert_layout %offsetkv_y#1 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc261)
        %offsetkv_y_46 = tt.expand_dims %offsetkv_y_45 {async_task_id = array<i32: 4>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc261)
        %qk_47 = ttng.tmem_subslice %qk_24 {N = 65 : i32, async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc202)
        %qk_48 = ttg.memdesc_reinterpret %qk_47 {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc202)
        %offsetkv_y_49 = ttg.memdesc_index %qk_48[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc261)
        %offsetkv_y_50 = arith.andi %arg77, %c1_i64_33 {async_task_id = array<i32: 4>} : i64 loc(#loc261)
        %offsetkv_y_51 = arith.trunci %offsetkv_y_50 {async_task_id = array<i32: 4>} : i64 to i1 loc(#loc261)
        %97 = ttg.memdesc_index %arg65[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %offsetkv_y_52 = arith.xori %offsetkv_y_51, %true_32 : i1 loc(#loc261)
        %offsetkv_y_53 = arith.extui %offsetkv_y_52 : i1 to i32 loc(#loc261)
        ttng.wait_barrier %97, %offsetkv_y_53 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc261)
        ttng.tmem_store %offsetkv_y_46, %offsetkv_y_49, %true_32 {async_task_id = array<i32: 4>} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc261)
        %98 = ttg.memdesc_index %arg64[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %98, 1 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc261)
        %offsetkv_y_54 = ttg.convert_layout %offsetkv_y#0 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc261)
        %offsetkv_y_55 = tt.expand_dims %offsetkv_y_54 {async_task_id = array<i32: 4>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc261)
        %qk_56 = ttng.tmem_subslice %qk_24 {N = 66 : i32, async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc202)
        %qk_57 = ttg.memdesc_reinterpret %qk_56 {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc202)
        %offsetkv_y_58 = ttg.memdesc_index %qk_57[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc261)
        %99 = ttg.memdesc_index %arg69[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %99, %offsetkv_y_53 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc261)
        ttng.tmem_store %offsetkv_y_55, %offsetkv_y_58, %true_32 {async_task_id = array<i32: 4>} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc261)
        %100 = ttg.memdesc_index %arg68[%c0_i32_37] {async_task_id = array<i32: 4>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %100, 1 {async_task_id = array<i32: 4>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc261)
        %tile_idx_59 = arith.addi %arg77, %c1_i64_33 {async_task_id = array<i32: 4>} : i64 loc(#loc110)
        scf.yield %tile_idx_59, %offsetkv_y#2 : i64, i64 loc(#loc51)
      } {async_task_id = array<i32: 4>, tt.warp_specialize, ttg.partition.stages = [0 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], ttg.warp_specialize.tag = 0 : i32} loc(#loc110)
      ttg.warp_return {async_task_id = array<i32: 4>} loc(#loc)
    }
    partition4(%Z_18: i32 loc("Z"(#loc)), %H_19: i32 loc("H"(#loc)), %arg26: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg27: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %v_20: !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc96 at #loc97)), %arg29: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %qk_21: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc100 at #loc167)), %q0_22: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc99 at #loc3)), %arg32: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %acc_23: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc101 at #loc167)), %arg34: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg35: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %qk_24: !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc100 at #loc166)), %q1_25: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc98 at #loc3)), %arg38: !ttg.memdesc<3x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg39: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %acc_26: !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable> loc(callsite(#loc101 at #loc166)), %arg41: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg42: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg43: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg44: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %desc_q_27: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_q"(#loc)), %desc_k_28: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_k"(#loc)), %desc_v_29: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_v"(#loc)), %arg48: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc4 at #loc3)), %desc_o_30: !tt.tensordesc<tensor<128x64xbf16, #shared>> loc("desc_o"(#loc)), %arg50: !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable> loc(callsite(#loc2 at #loc3)), %sm_scale_31: f32 loc("sm_scale"(#loc)), %arg52: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg53: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg54: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg55: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg56: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg57: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg58: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg59: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg60: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg61: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg62: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg63: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg64: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg65: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg66: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg67: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg68: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg69: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg70: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg71: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg72: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg73: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg74: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0), %arg75: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":603:0)) num_warps(4) {
      %true_32 = arith.constant {async_task_id = array<i32: 5>} true loc(#loc1)
      %c1_i64_33 = arith.constant {async_task_id = array<i32: 5>} 1 : i64 loc(#loc1)
      %c0_i64_34 = arith.constant {async_task_id = array<i32: 5>} 0 : i64 loc(#loc1)
      %n_tile_num = arith.constant {async_task_id = array<i32: 5>} 32 : i32 loc(#loc188)
      %c1_i32_35 = arith.constant {async_task_id = array<i32: 5>} 1 : i32 loc(#loc1)
      %c8192_i32_36 = arith.constant {async_task_id = array<i32: 5>} 8192 : i32 loc(#loc1)
      %c0_i32_37 = arith.constant {async_task_id = array<i32: 5>} 0 : i32 loc(#loc1)
      %cst_38 = arith.constant {async_task_id = array<i32: 5>} 1.44269502 : f32 loc(#loc1)
      %c128_i32_39 = arith.constant {async_task_id = array<i32: 5>} 128 : i32 loc(#loc1)
      %cst_40 = arith.constant {async_task_id = array<i32: 5>} dense<0xFF800000> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc1)
      %cst_41 = arith.constant {async_task_id = array<i32: 5>} dense<1.000000e+00> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc1)
      %prog_id = tt.get_program_id x {async_task_id = array<i32: 5>} : i32 loc(#loc102)
      %num_progs = tt.get_num_programs x {async_task_id = array<i32: 5>} : i32 loc(#loc103)
      %total_tiles = arith.muli %Z_18, %n_tile_num {async_task_id = array<i32: 5>} : i32 loc(#loc104)
      %total_tiles_42 = arith.muli %total_tiles, %H_19 {async_task_id = array<i32: 5>} : i32 loc(#loc105)
      %tiles_per_sm = arith.divsi %total_tiles_42, %num_progs {async_task_id = array<i32: 5>} : i32 loc(#loc168)
      %94 = arith.remsi %total_tiles_42, %num_progs {async_task_id = array<i32: 5>} : i32 loc(#loc18)
      %95 = arith.cmpi slt, %prog_id, %94 {async_task_id = array<i32: 5>} : i32 loc(#loc19)
      %96 = scf.if %95 -> (i32) {
        %tiles_per_sm_44 = arith.addi %tiles_per_sm, %c1_i32_35 {async_task_id = array<i32: 5>} : i32 loc(#loc169)
        scf.yield {async_task_id = array<i32: 5>} %tiles_per_sm_44 : i32 loc(#loc169)
      } else {
        scf.yield {async_task_id = array<i32: 5>} %tiles_per_sm : i32 loc(#loc1)
      } {async_task_id = array<i32: 5>} loc(#loc20)
      %qk_scale = arith.mulf %sm_scale_31, %cst_38 {async_task_id = array<i32: 5>} : f32 loc(#loc201)
      %m_ij = tt.splat %qk_scale {async_task_id = array<i32: 5>} : f32 -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc215)
      %qk_43 = tt.splat %qk_scale {async_task_id = array<i32: 5>} : f32 -> tensor<128x128xf32, #blocked4> loc(#loc216)
      %tile_idx:2 = scf.for %tile_idx_44 = %c0_i32_37 to %96 step %c1_i32_35 iter_args(%arg77 = %c0_i64_34, %arg78 = %c0_i64_34) -> (i64, i64)  : i32 {
        %offsetkv_y:3 = scf.for %offsetkv_y_60 = %c0_i32_37 to %c8192_i32_36 step %c128_i32_39 iter_args(%arg80 = %cst_41, %arg81 = %cst_40, %arg82 = %arg78) -> (tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>, i64)  : i32 {
          %qk_61 = arith.andi %arg82, %c1_i64_33 {async_task_id = array<i32: 5>} : i64 loc(#loc203)
          %qk_62 = arith.trunci %qk_61 {async_task_id = array<i32: 5>} : i64 to i1 loc(#loc203)
          %qk_63 = ttg.memdesc_index %qk_21[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc203)
          %101 = ttg.memdesc_index %arg32[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %qk_64 = arith.extsi %qk_62 {async_task_id = array<i32: 5>} : i1 to i32 loc(#loc203)
          ttng.wait_barrier %101, %qk_64 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc203)
          %102 = ttg.memdesc_index %arg63[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %qk_65 = ttng.tmem_load %qk_63 {async_task_id = array<i32: 5>} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> tensor<128x128xf32, #blocked4> loc(#loc203)
          ttng.arrive_barrier %102, 1 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc203)
          %m_ij_66 = "tt.reduce"(%qk_65) <{axis = 1 : i32}> ({
          ^bb0(%m_ij_89: f32 loc(callsite(#loc1 at #loc229)), %m_ij_90: f32 loc(callsite(#loc1 at #loc229))):
            %m_ij_91 = arith.maxnumf %m_ij_89, %m_ij_90 {async_task_id = array<i32: 5>} : f32 loc(#loc255)
            tt.reduce.return %m_ij_91 {async_task_id = array<i32: 5>} : f32 loc(#loc248)
          }) {async_task_id = array<i32: 5>} : (tensor<128x128xf32, #blocked4>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc248)
          %m_ij_67 = arith.mulf %m_ij_66, %m_ij {async_task_id = array<i32: 5>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc215)
          %m_ij_68 = arith.maxnumf %arg81, %m_ij_67 {async_task_id = array<i32: 5>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc230)
          %qk_69 = arith.mulf %qk_65, %qk_43 {async_task_id = array<i32: 5>} : tensor<128x128xf32, #blocked4> loc(#loc216)
          %qk_70 = tt.expand_dims %m_ij_68 {async_task_id = array<i32: 5>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<128x1xf32, #blocked4> loc(#loc231)
          %qk_71 = tt.broadcast %qk_70 {async_task_id = array<i32: 5>} : tensor<128x1xf32, #blocked4> -> tensor<128x128xf32, #blocked4> loc(#loc232)
          %qk_72 = arith.subf %qk_69, %qk_71 {async_task_id = array<i32: 5>} : tensor<128x128xf32, #blocked4> loc(#loc232)
          %p = math.exp2 %qk_72 {async_task_id = array<i32: 5>} : tensor<128x128xf32, #blocked4> loc(#loc233)
          %alpha = arith.subf %arg81, %m_ij_68 {async_task_id = array<i32: 5>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc234)
          %alpha_73 = math.exp2 %alpha {async_task_id = array<i32: 5>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc207)
          %alpha_74 = ttg.convert_layout %alpha_73 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc207)
          %alpha_75 = tt.expand_dims %alpha_74 {async_task_id = array<i32: 5>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc207)
          %qk_76 = ttng.tmem_subslice %qk_21 {N = 64 : i32, async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc203)
          %qk_77 = ttg.memdesc_reinterpret %qk_76 {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc203)
          %alpha_78 = ttg.memdesc_index %qk_77[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc207)
          %103 = ttg.memdesc_index %arg62[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %alpha_79 = arith.xori %qk_62, %true_32 : i1 loc(#loc207)
          %alpha_80 = arith.extui %alpha_79 : i1 to i32 loc(#loc207)
          ttng.wait_barrier %103, %alpha_80 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc207)
          ttng.tmem_store %alpha_75, %alpha_78, %true_32 {async_task_id = array<i32: 5>} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc207)
          %104 = ttg.memdesc_index %arg61[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.arrive_barrier %104, 1 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc207)
          %l_ij = "tt.reduce"(%p) <{axis = 1 : i32}> ({
          ^bb0(%l_ij_89: f32 loc(callsite(#loc1 at #loc235)), %l_ij_90: f32 loc(callsite(#loc1 at #loc235))):
            %l_ij_91 = arith.addf %l_ij_89, %l_ij_90 {async_task_id = array<i32: 5>} : f32 loc(#loc256)
            tt.reduce.return %l_ij_91 {async_task_id = array<i32: 5>} : f32 loc(#loc250)
          }) {async_task_id = array<i32: 5>} : (tensor<128x128xf32, #blocked4>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc250)
          %p_81 = arith.truncf %p {async_task_id = array<i32: 5>} : tensor<128x128xf32, #blocked4> to tensor<128x128xbf16, #blocked4> loc(#loc236)
          %qk_82 = ttng.tmem_subslice %qk_21 {N = 0 : i32, async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc203)
          %qk_83 = ttg.memdesc_reinterpret %qk_82 {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc203)
          %acc_84 = ttg.memdesc_index %qk_83[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc204)
          %105 = ttg.memdesc_index %arg34[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          %acc_85 = arith.xori %qk_62, %true_32 {async_task_id = array<i32: 5>} : i1 loc(#loc204)
          %acc_86 = arith.extsi %acc_85 {async_task_id = array<i32: 5>} : i1 to i32 loc(#loc204)
          ttng.wait_barrier %105, %acc_86 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc204)
          ttng.tmem_store %p_81, %acc_84, %true_32 {async_task_id = array<i32: 5>} : tensor<128x128xbf16, #blocked4> -> !ttg.memdesc<128x128xbf16, #tmem4, #ttng.tensor_memory, mutable> loc(#loc204)
          %106 = ttg.memdesc_index %arg60[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
          ttng.arrive_barrier %106, 1 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc204)
          %l_i0 = arith.mulf %arg80, %alpha_73 {async_task_id = array<i32: 5>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc237)
          %l_i0_87 = arith.addf %l_i0, %l_ij {async_task_id = array<i32: 5>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> loc(#loc238)
          %offsetkv_y_88 = arith.addi %arg82, %c1_i64_33 {async_task_id = array<i32: 5>} : i64 loc(#loc261)
          scf.yield %l_i0_87, %m_ij_68, %offsetkv_y_88 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>, i64 loc(#loc200)
        } {async_task_id = array<i32: 5>, tt.disallow_acc_multi_buffer, tt.warp_specialize} loc(#loc261)
        %offsetkv_y_45 = ttg.convert_layout %offsetkv_y#1 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc261)
        %offsetkv_y_46 = tt.expand_dims %offsetkv_y_45 {async_task_id = array<i32: 5>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc261)
        %qk_47 = ttng.tmem_subslice %qk_21 {N = 65 : i32, async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc203)
        %qk_48 = ttg.memdesc_reinterpret %qk_47 {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc203)
        %offsetkv_y_49 = ttg.memdesc_index %qk_48[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc261)
        %offsetkv_y_50 = arith.andi %arg77, %c1_i64_33 {async_task_id = array<i32: 5>} : i64 loc(#loc261)
        %offsetkv_y_51 = arith.trunci %offsetkv_y_50 {async_task_id = array<i32: 5>} : i64 to i1 loc(#loc261)
        %97 = ttg.memdesc_index %arg67[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        %offsetkv_y_52 = arith.xori %offsetkv_y_51, %true_32 : i1 loc(#loc261)
        %offsetkv_y_53 = arith.extui %offsetkv_y_52 : i1 to i32 loc(#loc261)
        ttng.wait_barrier %97, %offsetkv_y_53 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc261)
        ttng.tmem_store %offsetkv_y_46, %offsetkv_y_49, %true_32 {async_task_id = array<i32: 5>} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc261)
        %98 = ttg.memdesc_index %arg66[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %98, 1 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc261)
        %offsetkv_y_54 = ttg.convert_layout %offsetkv_y#0 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc261)
        %offsetkv_y_55 = tt.expand_dims %offsetkv_y_54 {async_task_id = array<i32: 5>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc261)
        %qk_56 = ttng.tmem_subslice %qk_21 {N = 66 : i32, async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc203)
        %qk_57 = ttg.memdesc_reinterpret %qk_56 {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc203)
        %offsetkv_y_58 = ttg.memdesc_index %qk_57[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x128x1xf32, #tmem3, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc261)
        %99 = ttg.memdesc_index %arg71[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.wait_barrier %99, %offsetkv_y_53 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc261)
        ttng.tmem_store %offsetkv_y_55, %offsetkv_y_58, %true_32 {async_task_id = array<i32: 5>} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem3, #ttng.tensor_memory, mutable> loc(#loc261)
        %100 = ttg.memdesc_index %arg70[%c0_i32_37] {async_task_id = array<i32: 5>} : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc)
        ttng.arrive_barrier %100, 1 {async_task_id = array<i32: 5>} : !ttg.memdesc<1xi64, #shared1, #smem, mutable> loc(#loc261)
        %tile_idx_59 = arith.addi %arg77, %c1_i64_33 {async_task_id = array<i32: 5>} : i64 loc(#loc110)
        scf.yield %tile_idx_59, %offsetkv_y#2 : i64, i64 loc(#loc51)
      } {async_task_id = array<i32: 5>, tt.warp_specialize, ttg.partition.stages = [0 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], ttg.warp_specialize.tag = 0 : i32} loc(#loc110)
      ttg.warp_return {async_task_id = array<i32: 5>} loc(#loc)
    } : (i32, i32, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<3x128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<3x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable>, !ttg.memdesc<3x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x64xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !tt.tensordesc<tensor<128x64xbf16, #shared>>, !tt.tensordesc<tensor<128x64xbf16, #shared>>, !tt.tensordesc<tensor<128x64xbf16, #shared>>, !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable>, !tt.tensordesc<tensor<128x64xbf16, #shared>>, !ttg.memdesc<1x128x64xbf16, #shared, #smem, mutable>, f32, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) -> () loc(#loc1)
    tt.return loc(#loc85)
  } loc(#loc)
} loc(#loc)
#loc13 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":626:28)
#loc14 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":627:32)
#loc15 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:31)
#loc16 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":628:35)
#loc17 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":630:34)
#loc18 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":631:31)
#loc19 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":631:17)
#loc20 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":631:7)
#loc21 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":632:24)
#loc22 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":435:47)
#loc23 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":436:58)
#loc24 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":662:39)
#loc25 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":663:25)
#loc26 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":664:29)
#loc27 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":433:39)
#loc28 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":435:34)
#loc29 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":436:34)
#loc30 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":172:58)
#loc31 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":85:25)
#loc32 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":96:42)
#loc33 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":354:8)
#loc34 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":96:36)
#loc35 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":97:36)
#loc36 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":530:25)
#loc37 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":530:12)
#loc38 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":533:22)
#loc39 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":532:27)
#loc40 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":532:18)
#loc41 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":532:35)
#loc42 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":531:18)
#loc43 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":531:23)
#loc44 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":536:25)
#loc45 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":536:12)
#loc46 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":539:22)
#loc47 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":538:35)
#loc48 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":537:18)
#loc49 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":537:23)
#loc50 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":688:20)
#loc51 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":688:8)
#loc52 = loc("/home/jieeliu/workspace/fb-triton/python/triton/language/standard.py":41:11)
#loc53 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":625:32)
#loc54 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":176:12)
#loc55 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":432:32)
#loc56 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":429:22)
#loc57 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":430:21)
#loc58 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":432:24)
#loc59 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":432:45)
#loc60 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":432:37)
#loc61 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":433:29)
#loc62 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":451:36)
#loc63 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":176:24)
#loc64 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":216:22)
#loc65 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":216:8)
#loc66 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":534:35)
#loc67 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":540:50)
#loc68 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":448:16)
#loc69 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":78:47)
#loc70 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":82:22)
#loc71 = loc("/home/jieeliu/workspace/fb-triton/python/triton/language/standard.py":189:40)
#loc73 = loc("/home/jieeliu/workspace/fb-triton/python/triton/language/standard.py":168:27)
#loc74 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":78:31)
#loc75 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":82:38)
#loc76 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":82:33)
#loc77 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":83:21)
#loc78 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":85:31)
#loc79 = loc("/home/jieeliu/workspace/fb-triton/python/triton/language/standard.py":291:36)
#loc81 = loc("/home/jieeliu/workspace/fb-triton/python/triton/language/standard.py":261:15)
#loc82 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":118:13)
#loc83 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":122:22)
#loc84 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":122:30)
#loc85 = loc("/home/jieeliu/workspace/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention_dp.py":662:4)
#loc102 = loc("prog_id"(#loc13))
#loc103 = loc("num_progs"(#loc14))
#loc104 = loc("total_tiles"(#loc15))
#loc105 = loc("total_tiles"(#loc16))
#loc106 = loc("tiles_per_sm"(#loc17))
#loc107 = loc("tiles_per_sm"(#loc21))
#loc108 = loc("offs_m0"(#loc22))
#loc109 = loc("offs_m1"(#loc23))
#loc110 = loc("tile_idx"(#loc24))
#loc111 = loc("pid"(#loc25))
#loc112 = loc("off_hz"(#loc26))
#loc113 = loc("qo_offset_y"(#loc27))
#loc114 = loc("offs_m0"(#loc28))
#loc115 = loc("offs_m1"(#loc29))
#loc116 = loc("acc0"(#loc30))
#loc117 = loc("alpha"(#loc31))
#loc118 = loc("acc0"(#loc32))
#loc119 = loc("acc0"(#loc34))
#loc120 = loc("acc1"(#loc35))
#loc121 = loc("m_i0"(#loc36))
#loc122 = loc("m_i0"(#loc37))
#loc123 = loc(callsite(#loc38 at #loc3))
#loc124 = loc("m_ptrs0"(#loc39))
#loc125 = loc("m_ptrs0"(#loc40))
#loc126 = loc("m_ptrs0"(#loc41))
#loc127 = loc("acc0"(#loc42))
#loc128 = loc("acc0"(#loc43))
#loc129 = loc("m_i1"(#loc44))
#loc130 = loc("m_i1"(#loc45))
#loc131 = loc(callsite(#loc46 at #loc3))
#loc132 = loc("m_ptrs1"(#loc47))
#loc133 = loc("acc1"(#loc48))
#loc134 = loc("acc1"(#loc49))
#loc135 = loc("tile_idx"(#loc50))
#loc136 = loc("n_tile_num"(#loc53))
#loc137 = loc("k"(#loc54))
#loc138 = loc("offset_y"(#loc55))
#loc139 = loc("off_z"(#loc56))
#loc140 = loc("off_h"(#loc57))
#loc141 = loc("offset_y"(#loc58))
#loc142 = loc("offset_y"(#loc59))
#loc143 = loc("offset_y"(#loc60))
#loc144 = loc("qo_offset_y"(#loc61))
#loc145 = loc("q1"(#loc62))
#loc146 = loc("k"(#loc63))
#loc147 = loc("offsetkv_y"(#loc64))
#loc148 = loc(callsite(#loc66 at #loc3))
#loc149 = loc(callsite(#loc67 at #loc3))
#loc150 = loc("qk_scale"(#loc68))
#loc151 = loc("m_ij"(#loc69))
#loc152 = loc("qk"(#loc70))
#loc154 = loc("m_ij"(#loc74))
#loc155 = loc("qk"(#loc75))
#loc156 = loc("qk"(#loc76))
#loc157 = loc("p"(#loc77))
#loc158 = loc("alpha"(#loc78))
#loc160 = loc("p"(#loc82))
#loc161 = loc("l_i0"(#loc83))
#loc162 = loc("l_i0"(#loc84))
#loc168 = loc("tiles_per_sm"(#loc106))
#loc169 = loc("tiles_per_sm"(#loc107))
#loc170 = loc(callsite(#loc108 at #loc3))
#loc171 = loc(callsite(#loc109 at #loc3))
#loc172 = loc(callsite(#loc113 at #loc3))
#loc173 = loc(callsite(#loc114 at #loc3))
#loc174 = loc(callsite(#loc115 at #loc3))
#loc175 = loc("acc1"(#loc116))
#loc176 = loc(callsite(#loc121 at #loc3))
#loc177 = loc(callsite(#loc122 at #loc3))
#loc178 = loc(callsite(#loc124 at #loc3))
#loc179 = loc(callsite(#loc125 at #loc3))
#loc180 = loc(callsite(#loc126 at #loc3))
#loc181 = loc(callsite(#loc127 at #loc3))
#loc182 = loc(callsite(#loc128 at #loc3))
#loc183 = loc(callsite(#loc129 at #loc3))
#loc184 = loc(callsite(#loc130 at #loc3))
#loc185 = loc(callsite(#loc132 at #loc3))
#loc186 = loc(callsite(#loc133 at #loc3))
#loc187 = loc(callsite(#loc134 at #loc3))
#loc188 = loc(callsite(#loc52 at #loc136))
#loc189 = loc(callsite(#loc137 at #loc97))
#loc190 = loc(callsite(#loc138 at #loc3))
#loc191 = loc(callsite(#loc139 at #loc3))
#loc192 = loc(callsite(#loc140 at #loc3))
#loc193 = loc(callsite(#loc141 at #loc3))
#loc194 = loc(callsite(#loc142 at #loc3))
#loc195 = loc(callsite(#loc143 at #loc3))
#loc196 = loc(callsite(#loc144 at #loc3))
#loc197 = loc(callsite(#loc145 at #loc3))
#loc198 = loc(callsite(#loc146 at #loc97))
#loc199 = loc(callsite(#loc147 at #loc97))
#loc200 = loc(callsite(#loc65 at #loc97))
#loc201 = loc(callsite(#loc150 at #loc3))
#loc206 = loc("l_i0"(#loc175))
#loc207 = loc(callsite(#loc117 at #loc167))
#loc208 = loc(callsite(#loc118 at #loc167))
#loc209 = loc(callsite(#loc119 at #loc167))
#loc210 = loc(callsite(#loc120 at #loc167))
#loc211 = loc(callsite(#loc117 at #loc166))
#loc212 = loc(callsite(#loc118 at #loc166))
#loc213 = loc(callsite(#loc119 at #loc166))
#loc214 = loc(callsite(#loc120 at #loc166))
#loc215 = loc(callsite(#loc151 at #loc167))
#loc216 = loc(callsite(#loc152 at #loc167))
#loc218 = loc(callsite(#loc151 at #loc166))
#loc219 = loc(callsite(#loc154 at #loc166))
#loc220 = loc(callsite(#loc152 at #loc166))
#loc221 = loc(callsite(#loc155 at #loc166))
#loc222 = loc(callsite(#loc156 at #loc166))
#loc223 = loc(callsite(#loc157 at #loc166))
#loc224 = loc(callsite(#loc158 at #loc166))
#loc226 = loc(callsite(#loc160 at #loc166))
#loc227 = loc(callsite(#loc161 at #loc166))
#loc228 = loc(callsite(#loc162 at #loc166))
#loc230 = loc(callsite(#loc154 at #loc167))
#loc231 = loc(callsite(#loc155 at #loc167))
#loc232 = loc(callsite(#loc156 at #loc167))
#loc233 = loc(callsite(#loc157 at #loc167))
#loc234 = loc(callsite(#loc158 at #loc167))
#loc236 = loc(callsite(#loc160 at #loc167))
#loc237 = loc(callsite(#loc161 at #loc167))
#loc238 = loc(callsite(#loc162 at #loc167))
#loc239 = loc("l_i0_1"(#loc206))
#loc240 = loc(callsite(#loc33 at #loc209))
#loc241 = loc(callsite(#loc33 at #loc210))
#loc242 = loc(callsite(#loc33 at #loc213))
#loc243 = loc(callsite(#loc33 at #loc214))
#loc244 = loc(callsite(#loc71 at #loc217))
#loc246 = loc(callsite(#loc79 at #loc225))
#loc248 = loc(callsite(#loc71 at #loc229))
#loc250 = loc(callsite(#loc79 at #loc235))
#loc252 = loc("l_i1"(#loc239))
#loc253 = loc(callsite(#loc73 at #loc244))
#loc254 = loc(callsite(#loc81 at #loc246))
#loc255 = loc(callsite(#loc73 at #loc248))
#loc256 = loc(callsite(#loc81 at #loc250))
#loc257 = loc("l_i1_1"(#loc252))
#loc258 = loc("m_i0"(#loc257))
#loc259 = loc("m_i1"(#loc258))
#loc260 = loc("offsetkv_y"(#loc259))
#loc261 = loc(callsite(#loc260 at #loc97))
